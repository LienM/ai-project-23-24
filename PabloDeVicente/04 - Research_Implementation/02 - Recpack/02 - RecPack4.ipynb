{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the timed scenario runs and we can get back the predictions. \n",
    "It is needed still to analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# from recpack.pipelines import PipelineBuilder\n",
    "from PipelineBuilder_modified import * \n",
    "from recpack.scenarios import WeakGeneralization, Timed\n",
    "from recpack.preprocessing.preprocessors import DataFramePreprocessor\n",
    "from recpack.preprocessing.filters import MinItemsPerUser, MinUsersPerItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_path = '../../00 - Data/transactions/transactions_train.csv'\n",
    "transactions = pd.read_csv(transactions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:  Data collection\n",
    "transactions_path = '../../00 - Data/transactions/transactions_train.csv'\n",
    "transactions = pd.read_csv(transactions_path)\n",
    "print(\"Original data has size of : \" + str(len(transactions)))\n",
    "\n",
    "#transform datetime to unix epoch\n",
    "transactions['timestamp'] = pd.to_datetime(transactions['t_dat']).astype(int) / 10**9\n",
    "transactions.drop(columns=['t_dat'], inplace=True)\n",
    "\n",
    "articles_path = '../../00 - Data/articles/articles.csv'\n",
    "articles_df = pd.read_csv(articles_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.t_dat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Data preprocessing\n",
    "\n",
    "#        item1    item2   item3\n",
    "#usr1      x                x\n",
    "#usr2       x       x\n",
    "proc = DataFramePreprocessor(item_ix='article_id', user_ix='customer_id', timestamp_ix='timestamp')\n",
    "# #every user has at least 2 items bought\n",
    "proc.add_filter(MinUsersPerItem(5, item_ix='article_id', user_ix='customer_id'))\n",
    "# #every item is bought at least twice\n",
    "proc.add_filter(MinItemsPerUser(5, item_ix='article_id', user_ix='customer_id'))\n",
    "\n",
    "interaction_matrix = proc.process(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#As i really dont have validation data (it is hidden by kaggle, i set both values to same date). 'delta_in' before 't' will be used for training and 'delta_out' weeks after 't' will be used for testing\n",
    "t = datetime(2020, 9, 15).timestamp()\n",
    "t_validation = datetime(2020, 9, 14).timestamp()\n",
    "#maybe 9?\n",
    "delta_in = 1 * 604800\n",
    "# 1 semana = 604800\n",
    "delta_out = 604800\n",
    "\n",
    "#3 : Create scenario\n",
    "scenario = Timed(t, t_validation=t_validation, validation=True, delta_in = delta_in, delta_out = delta_out, seed =1)\n",
    "# scenario = Timed() \n",
    "scenario.split(interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 : Create the builder object\n",
    "builder = PipelineBuilder()\n",
    "builder.set_data_from_scenario(scenario)\n",
    "\n",
    "#adds algorithms to use later on. Baseline algorithim, just recommends popular stuff\n",
    "# builder.add_algorithm('Popularity') \n",
    "# builder.add_algorithm('ItemKNN', grid={\n",
    "#     'K': [100, 200, 500],\n",
    "#     'similarity': ['cosine', 'conditional_probability'],\n",
    "# })\n",
    "builder.add_algorithm('KUNN')\n",
    "\n",
    "#Set the metric for optimisation of parameters in algorithms. What is NDCGK ??\n",
    "builder.set_optimisation_metric('NDCGK', K=10)\n",
    "\n",
    "#adds metric for evaluation\n",
    "#NDCGK = Normalized Discounted Cumulative Gain at K\n",
    "builder.add_metric('NDCGK', K=[10, 20, 50])\n",
    "builder.add_metric('CoverageK', K=[10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #5 : Create and run the pipeline\n",
    "# pipeline = builder.build()\n",
    "# pipeline.run()\n",
    "# # x_preds = pipeline.run2()\n",
    "\n",
    "# #6 : Get results\n",
    "# pipeline.get_metrics()\n",
    "# # pipeline.optimisation_results\n",
    "# #pipeline.saveResults()\n",
    "\n",
    "from aux_functions import *\n",
    "from PipelineBuilder_modified import * \n",
    "\n",
    "pipeline = builder.build()\n",
    "csr = pipeline.run2()\n",
    "user_rec = UserRecommendations.fill_user_rec(csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ITEM BEING RECOMMENDED\n",
    "# user_rec.get_rec_user(6)[0][0]\n",
    "article_ids_array = user_rec.get_article_ids()\n",
    "decoded_items = []\n",
    "item_id_mapping = proc.item_id_mapping.set_index(interaction_matrix.ITEM_IX)[proc.item_ix].to_dict()\n",
    "\n",
    "for article_id in article_ids_array:\n",
    "\n",
    "    if article_id in item_id_mapping:\n",
    "        decoded_item = item_id_mapping[article_id]\n",
    "        decoded_items.append(decoded_item)\n",
    "\n",
    "article_counts = pd.Series(decoded_items).value_counts().to_dict()\n",
    "\n",
    "# Filter 'articles_df' based on article IDs present in 'decoded_items'\n",
    "recommended_articles = articles_df[articles_df['article_id'].isin(decoded_items)].copy()\n",
    "\n",
    "# Count occurrences of each article ID in 'decoded_items'\n",
    "article_counts = pd.Series(decoded_items).value_counts().to_dict()\n",
    "\n",
    "# Map counts back to 'filtered_articles_df' using .loc to avoid SettingWithCopyWarning\n",
    "recommended_articles.loc[:, 'number_of_repeats'] = recommended_articles['article_id'].map(article_counts).fillna(0).astype(int)\n",
    "\n",
    "#recommended_articles contains a dataframe with all my new recommendations\n",
    "recommended_articles.to_csv(\"recommended_articles_pop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
