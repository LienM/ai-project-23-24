{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "from model import *\n",
    "os.chdir('../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First week num:  0 \n",
      "Last week num:  104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_pickle('compressed_data/transactions_train.pkl')\n",
    "customers = pd.read_pickle('compressed_data/customers.pkl')\n",
    "articles = pd.read_pickle('compressed_data/articles.pkl')[[\n",
    "        'article_id', \n",
    "        'product_type_no', \n",
    "        'graphical_appearance_no', \n",
    "        'colour_group_code', \n",
    "        'perceived_colour_value_id',\n",
    "        'perceived_colour_master_id', \n",
    "        'department_no', \n",
    "        'index_code',\n",
    "        'index_group_no', \n",
    "        'section_no', \n",
    "        'garment_group_no', \n",
    "    ]]\n",
    "\n",
    "transactions['week'] = 104 - (transactions.t_dat.max() - transactions.t_dat).dt.days // 7\n",
    "\n",
    "print('First week num: ', transactions.week.min(), '\\nLast week num: ', transactions.week.max(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test week is week after last week in train data\n",
    "test_week = transactions.week.max() + 1\n",
    "\n",
    "# Filter transactions to last 10 weeks (most recent data)\n",
    "transactions = transactions[transactions.week > transactions.week.max() - 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id',\n",
       "       'week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load radek's candidates\n",
    "candidates_last_purchase = pd.read_csv('candidates/radek_last_purchase.csv')\n",
    "candidates_bestsellers = pd.read_csv('candidates/radek_bestsellers.csv')\n",
    "bestsellers_previous_week = pd.read_csv('candidates/radek_bestsellers_previous_week.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load my candidates\n",
    "## Seasonal candidates (best k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_seasonal = 20\n",
    "\n",
    "seasonal_bestsellers = pd.read_csv('candidates_200_ranks/seasonal_candidates_2019.csv')\n",
    "\n",
    "seasonal_candidates_filtered = seasonal_bestsellers[seasonal_bestsellers.article_id.isin(transactions.article_id.unique())].head(k_seasonal)\n",
    "\n",
    "seasonal_candidates = transactions[['customer_id', 'week']].drop_duplicates()\n",
    "test_candidates = customers[customers.customer_id.isin(transactions.customer_id.unique())][['customer_id']].drop_duplicates()\n",
    "test_candidates['week'] = test_week\n",
    "seasonal_candidates = pd.concat([seasonal_candidates, test_candidates], ignore_index=True)\n",
    "\n",
    "seasonal_candidates['key'] = 1\n",
    "seasonal_candidates_filtered['key'] = 1\n",
    "\n",
    "seasonal_candidates = seasonal_candidates.merge(seasonal_candidates_filtered, on='key').drop(columns='key')\n",
    "seasonal_candidates['t_dat'] = '2020-07-15'\n",
    "seasonal_candidates['price'] = 0\n",
    "seasonal_candidates['sales_channel_id'] = 2\n",
    "seasonal_candidates.drop(columns=['seasonal_bestseller_rank', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar not bought candidates (best k = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_snb = 120\n",
    "\n",
    "candidates_similar_not_bought = pd.read_csv('candidates_200_ranks/sim_not_bought.csv')\n",
    "\n",
    "top_k_snb_weekly = candidates_similar_not_bought\\\n",
    "    .groupby(['week', 'customer_id']).head(k_snb)\\\n",
    "    .drop(columns=['strategy', 'similarity_score'])\n",
    "\n",
    "del candidates_similar_not_bought\n",
    "gc.collect()\n",
    "\n",
    "top_k_snb_weekly['t_dat'] = '2020-07-15'\n",
    "top_k_snb_weekly['price'] = 0\n",
    "top_k_snb_weekly['sales_channel_id'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not interacted with candidates\n",
    "### Colour group (best k = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_niw_colour = 20\n",
    "\n",
    "candidates_niw_loaded = pd.read_csv('candidates_200_ranks/niw_candidates_colour_group_name.csv')\n",
    "\n",
    "candidates_niw_colour = candidates_niw_loaded.groupby(['week', 'customer_id']).head(k_niw_colour)\\\n",
    "        .drop(columns=['strategy', 'not_interacted_weekly_rank'])\n",
    "\n",
    "niw_ranks_colour = candidates_niw_loaded[['week', 'article_id', 'not_interacted_weekly_rank']].drop_duplicates().rename(columns={'not_interacted_weekly_rank': 'niw_rank_colour'})\n",
    "\n",
    "del candidates_niw_loaded\n",
    "gc.collect()\n",
    "\n",
    "candidates_niw_colour['week'] = candidates_niw_colour['week'] + 1\n",
    "\n",
    "candidates_niw_colour['t_dat'] = '2020-07-15'\n",
    "candidates_niw_colour['price'] = 0\n",
    "candidates_niw_colour['sales_channel_id'] = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garment group (best k = 30/40/50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_niw_garment = 20\n",
    "\n",
    "candidates_niw_loaded = pd.read_csv('candidates_200_ranks/niw_candidates_garment_group_name.csv')\n",
    "\n",
    "candidates_niw_garment = candidates_niw_loaded.groupby(['week', 'customer_id']).head(k_niw_garment)\\\n",
    "        .drop(columns=['strategy', 'not_interacted_weekly_rank'])\n",
    "\n",
    "niw_ranks_garment = candidates_niw_loaded[['week', 'article_id', 'not_interacted_weekly_rank']].drop_duplicates().rename(columns={'not_interacted_weekly_rank': 'niw_rank_garment'})\n",
    "\n",
    "del candidates_niw_loaded\n",
    "gc.collect()\n",
    "\n",
    "candidates_niw_garment['week'] = candidates_niw_garment['week'] + 1\n",
    "\n",
    "candidates_niw_garment['t_dat'] = '2020-07-15'\n",
    "candidates_niw_garment['price'] = 0\n",
    "candidates_niw_garment['sales_channel_id'] = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section (best k = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_niw_section = 20\n",
    "\n",
    "candidates_niw_loaded = pd.read_csv('candidates_200_ranks/niw_candidates_section_name.csv')\n",
    "\n",
    "candidates_niw_section = candidates_niw_loaded.groupby(['week', 'customer_id']).head(k_niw_section)\\\n",
    "        .drop(columns=['strategy', 'not_interacted_weekly_rank'])\n",
    "\n",
    "niw_ranks_section = candidates_niw_loaded[['week', 'article_id', 'not_interacted_weekly_rank']].drop_duplicates().rename(columns={'not_interacted_weekly_rank': 'niw_rank_section'})\n",
    "\n",
    "del candidates_niw_loaded\n",
    "gc.collect()\n",
    "\n",
    "candidates_niw_section['week'] = candidates_niw_section['week'] + 1\n",
    "\n",
    "candidates_niw_section['t_dat'] = '2020-07-15'\n",
    "candidates_niw_section['price'] = 0\n",
    "candidates_niw_section['sales_channel_id'] = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Loop -- check best k for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "    'article_id', \n",
    "    'product_type_no', \n",
    "    'graphical_appearance_no', \n",
    "    'colour_group_code', \n",
    "    'perceived_colour_value_id',\n",
    "    'perceived_colour_master_id', \n",
    "    'department_no', \n",
    "    'index_code',\n",
    "    'index_group_no', \n",
    "    'section_no', \n",
    "    'garment_group_no', \n",
    "    'FN', \n",
    "    'Active',\n",
    "    'club_member_status', \n",
    "    'fashion_news_frequency', \n",
    "    'age', \n",
    "    'postal_code', \n",
    "    'bestseller_rank',\n",
    "    'seasonal_bestseller_rank',\n",
    "    'niw_rank_colour',\n",
    "    'niw_rank_garment',\n",
    "    'niw_rank_section'\n",
    "]\n",
    "\n",
    "model_params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 1,\n",
    "    'importance_type': 'gain'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(t_df, candidates, features, cols_to_use, test_week=105, bestsellers_prev_week=None):\n",
    "    '''\n",
    "    Prepare data for training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_df : pd.DataFrame\n",
    "        DataFrame with transactions.\n",
    "    bestsellers_prev_week : pd.DataFrame\n",
    "        DataFrame with bestsellers for previous week.\n",
    "    candidates : list\n",
    "        List of DataFrames with candidates.\n",
    "    features : list\n",
    "        List of DataFrames with features. DataFrames should have one at least but not all of following columns: week, article_id, customer_id.\n",
    "    cols_to_use : list\n",
    "        List of columns to use for training.\n",
    "    test_week : int, default 105\n",
    "        Week to use as test data. The default is 105.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_X : pd.DataFrame\n",
    "        Training data.\n",
    "    train_y : pd.Series\n",
    "        Training labels.\n",
    "    test_X : pd.DataFrame\n",
    "        Test data features.\n",
    "    test : pd.DataFrame\n",
    "        Test data.\n",
    "    train_baskets : np.array\n",
    "        Number of purchases for each customer week pair.    \n",
    "    '''\n",
    "    t_df['purchased'] = 1\n",
    "    data = pd.concat([t_df] + candidates)\n",
    "    data.purchased.fillna(0, inplace=True)\n",
    "    data.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "\n",
    "    del t_df, candidates\n",
    "    gc.collect()\n",
    "\n",
    "    print('Percentage of real transactions: ', data.purchased.mean())\n",
    "\n",
    "    if bestsellers_prev_week is not None:\n",
    "        model_data = pd.merge(\n",
    "            data,\n",
    "            bestsellers_prev_week[['week', 'article_id', 'bestseller_rank']],\n",
    "            on=['week', 'article_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        del bestsellers_prev_week\n",
    "        gc.collect()\n",
    "    else:\n",
    "        model_data = data.copy()\n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    # Remove first week of data, as we don't have bestseller rank for it\n",
    "    # (week was shifted by one) and fill missing values with 999 -- really bad rank\n",
    "    model_data = model_data[model_data.week != model_data.week.min()]\n",
    "    model_data.fillna({'bestseller_rank':999}, inplace=True)\n",
    "\n",
    "    print('Mergining features...')\n",
    "    for i in range(len(features)):\n",
    "\n",
    "        feature_df = features.pop()\n",
    "\n",
    "        if ('week' in feature_df.columns) and ('article_id' in feature_df.columns):\n",
    "            model_data = pd.merge(\n",
    "                model_data, \n",
    "                feature_df, \n",
    "                on=['week', 'article_id'], \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "        elif ('week' in feature_df.columns) and ('customer_id' in feature_df.columns):\n",
    "            model_data = pd.merge(\n",
    "                model_data, \n",
    "                feature_df, \n",
    "                on=['week', 'customer_id'], \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "        elif ('week' not in feature_df.columns) and ('article_id' in feature_df.columns):\n",
    "            model_data = pd.merge(\n",
    "                model_data, \n",
    "                feature_df, \n",
    "                on='article_id', \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "        elif ('week' not in feature_df.columns) and ('customer_id' in feature_df.columns):\n",
    "            model_data = pd.merge(\n",
    "                model_data, \n",
    "                feature_df, \n",
    "                on='customer_id', \n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Feature DataFrame should have at least one of following columns: week, article_id, customer_id.')\n",
    "        \n",
    "        del feature_df\n",
    "        gc.collect()\n",
    "\n",
    "    del features\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Done.')\n",
    "    print('Sorting data...')\n",
    "    model_data.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    model_data.reset_index(drop=True, inplace=True)\n",
    "    print('Done.')\n",
    "    print('Preparing for training...')\n",
    "    train = model_data[model_data.week != test_week]\n",
    "    test = model_data[model_data.week == test_week]\\\n",
    "        .drop_duplicates(['customer_id', 'article_id', 'sales_channel_id'])\\\n",
    "        .copy()\n",
    "    \n",
    "    del model_data\n",
    "    gc.collect()\n",
    "    \n",
    "    # Basically how many purchased for each customer week pair -- so lgbm knows its one transaction\n",
    "    train_baskets = train.groupby(['week', 'customer_id'])['article_id']\\\n",
    "        .count()\\\n",
    "        .values  \n",
    "    \n",
    "    try:\n",
    "        train_X = train[cols_to_use]\n",
    "    except KeyError:\n",
    "        return train_X\n",
    "    train_y = train['purchased']\n",
    "\n",
    "    test_X = test[cols_to_use]\n",
    "\n",
    "    assert test.purchased.mean() == 0, 'Test data should not contain any actual purchases!'\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    return train_X, train_y, test_X, test, train_baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of real transactions:  0.010623573219747605\n",
      "Mergining features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get bestsellers from previous week\n",
    "bestsellers_last_week = \\\n",
    "    bestsellers_previous_week[bestsellers_previous_week['week'] == bestsellers_previous_week['week'].max()]['article_id'].tolist()\n",
    "\n",
    "# Prepare data for model\n",
    "train_X, train_y, test_X, test, train_baskets = prepare_data(\n",
    "    transactions,\n",
    "    candidates=[candidates_last_purchase, candidates_bestsellers, seasonal_candidates, top_k_snb_weekly, candidates_niw_colour, candidates_niw_garment, candidates_niw_section], \n",
    "    features=[customers, articles, seasonal_bestsellers[['article_id', 'seasonal_bestseller_rank']], niw_ranks_colour, niw_ranks_garment, niw_ranks_section], \n",
    "    cols_to_use=columns_to_use,\n",
    "    bestsellers_prev_week=bestsellers_previous_week\n",
    "    )\n",
    "\n",
    "del candidates_last_purchase, bestsellers_previous_week, candidates_bestsellers, seasonal_candidates, top_k_snb_weekly, candidates_niw_colour, candidates_niw_garment, candidates_niw_section, niw_ranks_colour, niw_ranks_garment, niw_ranks_section\n",
    "gc.collect()\n",
    "\n",
    "# Train model\n",
    "ranker = train_model(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    train_baskets, \n",
    "    model_params, \n",
    "    columns_to_use, \n",
    "    show_importance=10\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Make submission\n",
    "make_submission(customers, test, test_X, ranker, bestsellers_last_week, f'submission_all_best_with_radek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                                  date                 description                        status    publicScore  privateScore  \n",
      "----------------------------------------  -------------------  ---------------------------------  --------  -----------  ------------  \n",
      "submission_all_best_with_radek.csv.gz     2023-12-16 20:56:36  submission_all_best_with_radek     complete  0.00974      0.0097        \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c h-and-m-personalized-fashion-recommendations | head -n 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
