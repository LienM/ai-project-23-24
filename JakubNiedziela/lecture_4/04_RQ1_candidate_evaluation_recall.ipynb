{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "from eval_helpers import recall_at_k\n",
    "os.chdir('lecture_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First week num:  0 \n",
      "Last week num:  104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_pickle('../../../data/compressed_data/transactions_train.pkl')\n",
    "customers = pd.read_pickle('../../../data/compressed_data/customers.pkl')\n",
    "articles = pd.read_pickle('../../../data/compressed_data/articles.pkl')\n",
    "\n",
    "transactions['week'] = 104 - (transactions.t_dat.max() - transactions.t_dat).dt.days // 7\n",
    "\n",
    "print('First week num: ', transactions.week.min(), '\\nLast week num: ', transactions.week.max(), '\\n')\n",
    "\n",
    "# Test week is week after last week in train data\n",
    "test_week = transactions.week.max()\n",
    "train_weeks = range(test_week - 10, test_week)\n",
    "\n",
    "# Filter transactions to last 10 weeks (most recent data)\n",
    "transactions_train = transactions[transactions.week.isin(train_weeks)]\n",
    "transaction_test = transactions[transactions.week == test_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_score(test_week_transactions, predictions_df, k=100):\n",
    "#     y_true = test_week_transactions.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "#     y_true.columns = ['customer_id', 'y_true']\n",
    "#     predictions_df.columns = ['customer_id', 'y_pred']\n",
    "#     eval_df = pd.merge(y_true, predictions_df, on='customer_id')\n",
    "#     return recall_at_k(eval_df['y_true'], eval_df['y_pred'], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(test_week_transactions, predictions_df, k=100):\n",
    "    y_true = test_week_transactions.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "    y_true.columns = ['customer_id', 'y_true']\n",
    "    predictions_df.columns = ['customer_id', 'y_pred']\n",
    "    eval_df = pd.merge(y_true, predictions_df, on='customer_id')\n",
    "    return recall_at_k(eval_df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_week_transactions = transaction_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Radek's candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_last_purchase_test = pd.read_csv('../../../data/candidates/radek_last_purchase.csv')\n",
    "bestsellers_previous_week = pd.read_csv('../../../data/candidates_100/radek_bestsellers_previous_week.csv')\n",
    "candidates_bestsellers_test_week = pd.read_csv('../../../data/candidates_100/radek_bestsellers.csv')\n",
    "\n",
    "bestsellers_last_week = bestsellers_previous_week[bestsellers_previous_week['week'] == bestsellers_previous_week['week'].max()]['article_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11905203020399299"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = candidates_bestsellers_test_week\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04666869968353053"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = candidates_last_purchase_test\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate my candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal candidates\n",
    "#### Seasonal previous baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_candidates = pd.read_csv('../../../data/candidates_100/baskets_seasonal.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005885792960200041"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2018 candidates\n",
    "predictions_df = seasonal_candidates[seasonal_candidates.year == 2018]\\\n",
    "    .drop(columns=['year'])\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003252009003428331"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2019 candidates\n",
    "predictions_df = seasonal_candidates[seasonal_candidates.year == 2019]\\\n",
    "    .drop(columns=['year'])\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best seasonal candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seasonal_items = pd.read_csv('../../../data/candidates/best_seasonal.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009176737674634968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(\n",
    "    test_week_transactions.customer_id.unique(),\n",
    "    columns=['customer_id']\n",
    "    )\n",
    "predictions_df['year'] = 2018\n",
    "predictions_df = pd.merge(predictions_df, best_seasonal_items, on='year')\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027426012836377116"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(\n",
    "    test_week_transactions.customer_id.unique(),\n",
    "    columns=['customer_id']\n",
    "    )\n",
    "predictions_df['year'] = 2019\n",
    "predictions_df = pd.merge(predictions_df, best_seasonal_items, on='year')\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar not bought candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01041905066245714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_not_bought_candidates = pd.read_csv('../../../data/candidates_100/similar_not_bought.csv', index_col=0)\n",
    "\n",
    "# add bestsellers for sake of completeness\n",
    "predictions_df = similar_not_bought_candidates\\\n",
    "    .groupby('customer_id')['sim_not_bought']\\\n",
    "    .apply(lambda x: list(x))\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items from categories user not interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_interacted_with_candidates_v2(t, a, articles_col, k=10):\n",
    "    \n",
    "    # Get unique values of given category\n",
    "    group_unique_values = a[articles_col].unique()\n",
    "    group_df = pd.merge(t, a[['article_id', articles_col]])\n",
    "\n",
    "    # Not interacted category for each customer\n",
    "    not_interacted_with = group_df\\\n",
    "        .groupby('customer_id')[articles_col]\\\n",
    "        .apply(lambda x: np.array(list(set(x))))\\\n",
    "        .apply(lambda x: np.setdiff1d(group_unique_values, x))\n",
    "    \n",
    "    # Get k most popular articles in given category\n",
    "    items_popularity = group_df\\\n",
    "        .groupby(articles_col)['article_id']\\\n",
    "        .value_counts()\\\n",
    "        .groupby(articles_col)\\\n",
    "        .head(k)\\\n",
    "        .reset_index()\n",
    "\n",
    "    # Rank items by popularity (number of purchases)\n",
    "    items_popularity['not_interacted_rank'] = items_popularity['count']\\\n",
    "        .rank(method='dense', ascending=False)\\\n",
    "        .astype('int16')\n",
    "    \n",
    "    items_popularity = items_popularity\\\n",
    "        .filter(items=['article_id', articles_col, 'not_interacted_rank'])\\\n",
    "        .sort_values(by=['not_interacted_rank'])\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # For each customer get k most popular articles in categories that customer did not interact with\n",
    "    for cid in tqdm(not_interacted_with.index.values):\n",
    "        groups = not_interacted_with.loc[cid]\n",
    "\n",
    "        cid_candidates = items_popularity\\\n",
    "            [items_popularity[articles_col].isin(groups)]\\\n",
    "            .head(k)\\\n",
    "            .drop(columns=[articles_col])\n",
    "        \n",
    "        cid_candidates['customer_id'] = cid\n",
    "\n",
    "        candidates.append(cid_candidates)\n",
    "\n",
    "    return pd.concat(candidates)[['customer_id', 'article_id', 'not_interacted_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:29<00:00, 2935.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column product_group_name: \n",
      "\t Score: 0.03020.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:33<00:00, 2855.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column graphical_appearance_name: \n",
      "\t Score: 0.03476.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:42<00:00, 2709.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column colour_group_name: \n",
      "\t Score: 0.04318.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:05<00:00, 3512.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column perceived_colour_value_name: \n",
      "\t Score: 0.03111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:12<00:00, 3319.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column perceived_colour_master_name: \n",
      "\t Score: 0.03950.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:05<00:00, 3492.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column index_name: \n",
      "\t Score: 0.03206.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:03<00:00, 3555.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column index_group_name: \n",
      "\t Score: 0.02880.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:54<00:00, 2517.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column section_name: \n",
      "\t Score: 0.04312.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:14<00:00, 3256.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column garment_group_name: \n",
      "\t Score: 0.04813.\n"
     ]
    }
   ],
   "source": [
    "article_groups_cols = [\n",
    "    'product_group_name',\n",
    "    'graphical_appearance_name',\n",
    "    'colour_group_name',\n",
    "    'perceived_colour_value_name',\n",
    "    'perceived_colour_master_name',\n",
    "    'index_name',\n",
    "    'index_group_name',\n",
    "    'section_name',\n",
    "    'garment_group_name'\n",
    "]\n",
    "k = 100\n",
    "for col_name in article_groups_cols:\n",
    "    candidates = not_interacted_with_candidates_v2(transactions_train, articles, col_name, k)\n",
    "\n",
    "    candidates.to_csv(f'../../../data/candidates_100/not_interacted_with_{col_name}_k{k}.csv', index=False)\n",
    "\n",
    "    predictions_df = candidates\\\n",
    "        .groupby('customer_id')['article_id']\\\n",
    "        .apply(lambda x: list(x))\\\n",
    "        .reset_index()\n",
    "\n",
    "    score = calculate_score(test_week_transactions, predictions_df)\n",
    "\n",
    "    print(f'\\nPrediction using column {col_name}: \\n\\t Score: {score:.5f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:06<00:00, 3475.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column product_group_name: \n",
      "\t Score: 0.01907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:09<00:00, 3388.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column graphical_appearance_name: \n",
      "\t Score: 0.02319.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [13:35<00:00, 538.45it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column colour_group_name: \n",
      "\t Score: 0.02694.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:01<00:00, 3627.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column perceived_colour_value_name: \n",
      "\t Score: 0.01942.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:03<00:00, 3549.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column perceived_colour_master_name: \n",
      "\t Score: 0.02419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:00<00:00, 3654.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column index_name: \n",
      "\t Score: 0.02106.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [01:55<00:00, 3813.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column index_group_name: \n",
      "\t Score: 0.01933.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:17<00:00, 3190.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column section_name: \n",
      "\t Score: 0.02731.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439368/439368 [02:01<00:00, 3615.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column garment_group_name: \n",
      "\t Score: 0.02949.\n"
     ]
    }
   ],
   "source": [
    "article_groups_cols = [\n",
    "    'product_group_name',\n",
    "    'graphical_appearance_name',\n",
    "    'colour_group_name',\n",
    "    'perceived_colour_value_name',\n",
    "    'perceived_colour_master_name',\n",
    "    'index_name',\n",
    "    'index_group_name',\n",
    "    'section_name',\n",
    "    'garment_group_name'\n",
    "]\n",
    "k = 50\n",
    "for col_name in article_groups_cols:\n",
    "    candidates = not_interacted_with_candidates_v2(transactions_train, articles, col_name, k)\n",
    "\n",
    "    candidates.to_csv(f'../../../data/candidates_50/not_interacted_with_{col_name}_k{k}.csv', index=False)\n",
    "\n",
    "    predictions_df = candidates\\\n",
    "        .groupby('customer_id')['article_id']\\\n",
    "        .apply(lambda x: list(x))\\\n",
    "        .reset_index()\n",
    "\n",
    "    score = calculate_score(test_week_transactions, predictions_df)\n",
    "\n",
    "    print(f'\\nPrediction using column {col_name}: \\n\\t Score: {score:.5f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
