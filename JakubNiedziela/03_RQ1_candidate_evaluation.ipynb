{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from eval_helpers import mean_average_precision\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First week num:  0 \n",
      "Last week num:  104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_pickle('../../data/compressed_data/transactions_train.pkl')\n",
    "customers = pd.read_pickle('../../data/compressed_data/customers.pkl')\n",
    "articles = pd.read_pickle('../../data/compressed_data/articles.pkl')\n",
    "\n",
    "transactions['week'] = 104 - (transactions.t_dat.max() - transactions.t_dat).dt.days // 7\n",
    "\n",
    "print('First week num: ', transactions.week.min(), '\\nLast week num: ', transactions.week.max(), '\\n')\n",
    "\n",
    "# Test week is week after last week in train data\n",
    "test_week = transactions.week.max()\n",
    "train_weeks = range(test_week - 10, test_week)\n",
    "\n",
    "# Filter transactions to last 10 weeks (most recent data)\n",
    "transactions_train = transactions[transactions.week.isin(train_weeks)]\n",
    "transaction_test = transactions[transactions.week == test_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(test_week_transactions, predictions_df, k=12):\n",
    "    y_true = test_week_transactions.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "    y_true.columns = ['customer_id', 'y_true']\n",
    "    predictions_df.columns = ['customer_id', 'y_pred']\n",
    "    eval_df = pd.merge(y_true, predictions_df, on='customer_id')\n",
    "    return mean_average_precision(eval_df['y_true'], eval_df['y_pred'], k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and evaluate Radek's candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2weeks = transactions_train.groupby('customer_id')['week'].unique()\n",
    "\n",
    "c2weeks2shifted_weeks = {}\n",
    "\n",
    "for c_id, weeks in c2weeks.items():\n",
    "    c2weeks2shifted_weeks[c_id] = {}\n",
    "    for i in range(weeks.shape[0]-1):\n",
    "        c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "    c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "\n",
    "candidates_last_purchase = transactions_train.copy()\n",
    "\n",
    "weeks = []\n",
    "for i, (c_id, week) in enumerate(zip(transactions_train['customer_id'], transactions_train['week'])):\n",
    "    weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    \n",
    "candidates_last_purchase.week=weeks\n",
    "\n",
    "# bestseller\n",
    "mean_price = transactions_train.groupby(['week', 'article_id'])['price'].mean()\n",
    "sales = transactions_train \\\n",
    "    .groupby('week')['article_id'].value_counts() \\\n",
    "    .groupby('week').rank(method='dense', ascending=False) \\\n",
    "    .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "bestsellers_previous_week.week += 1\n",
    "\n",
    "unique_transactions = transactions_train \\\n",
    "    .groupby(['week', 'customer_id']) \\\n",
    "    .head(1) \\\n",
    "    .drop(columns=['article_id', 'price']) \\\n",
    "    .copy()\n",
    "\n",
    "candidates_bestsellers = pd.merge(\n",
    "    unique_transactions,\n",
    "    bestsellers_previous_week,\n",
    "    on='week',\n",
    ")\n",
    "\n",
    "test_set_transactions = unique_transactions\\\n",
    "    .drop_duplicates('customer_id')\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "test_set_transactions.week = test_week\n",
    "\n",
    "candidates_bestsellers_test_week = pd.merge(\n",
    "    test_set_transactions,\n",
    "    bestsellers_previous_week,\n",
    "    on='week'\n",
    ")\n",
    "\n",
    "# # Concatenate data with test data\n",
    "# candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "# candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_last_purchase_test = candidates_last_purchase[candidates_last_purchase.week == test_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestsellers_last_week = bestsellers_previous_week[bestsellers_previous_week['week'] == bestsellers_previous_week['week'].max()]['article_id'].tolist()\n",
    "test_week_transactions = transaction_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00852672068485865"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = candidates_bestsellers_test_week\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03255156197770692"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = candidates_last_purchase_test\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate my candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal candidates\n",
    "#### Seasonal previous baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_candidates = pd.read_csv('../../data/candidates/baskets_seasonal.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005022726347485874"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2018 candidates\n",
    "predictions_df = seasonal_candidates[seasonal_candidates.year == 2018]\\\n",
    "    .drop(columns=['year'])\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0054099095872320885"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2019 candidates\n",
    "predictions_df = seasonal_candidates[seasonal_candidates.year == 2019]\\\n",
    "    .drop(columns=['year'])\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best seasonal candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seasonal_items = pd.read_csv('../../data/candidates/best_seasonal.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.460632124859302e-05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(\n",
    "    test_week_transactions.customer_id.unique(),\n",
    "    columns=['customer_id']\n",
    "    )\n",
    "predictions_df['year'] = 2018\n",
    "predictions_df = pd.merge(predictions_df, best_seasonal_items, on='year')\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002140628852461899"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(\n",
    "    test_week_transactions.customer_id.unique(),\n",
    "    columns=['customer_id']\n",
    "    )\n",
    "predictions_df['year'] = 2019\n",
    "predictions_df = pd.merge(predictions_df, best_seasonal_items, on='year')\\\n",
    "    .groupby('customer_id')['article_id']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar not bought candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001300274631099207"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_not_bought_candidates = pd.read_csv('../../data/candidates/similar_not_bought.csv', index_col=0)\n",
    "\n",
    "# add bestsellers for sake of completeness\n",
    "predictions_df = similar_not_bought_candidates\\\n",
    "    .groupby('customer_id')['sim_not_bought']\\\n",
    "    .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "    .apply(lambda x: x[:12])\\\n",
    "    .reset_index()\n",
    "\n",
    "calculate_score(test_week_transactions, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items from categories user not interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_interacted_with_candidates(t, a, articles_col, k=2):\n",
    "\n",
    "    # Get unique values of given category\n",
    "    group_unique_values = a[articles_col].unique()\n",
    "    group_df = pd.merge(t, a[['article_id', articles_col]])\n",
    "\n",
    "    # Get k most popular articles in given category\n",
    "    popular_by_group = group_df.groupby(articles_col)['article_id'].value_counts()\\\n",
    "        .groupby(articles_col).head(k).reset_index()\n",
    "    popular_by_group = popular_by_group[['article_id', articles_col]]\n",
    "\n",
    "    # Not interacted category for each customer\n",
    "    not_interacted_with = group_df.groupby('customer_id')[articles_col].unique()\\\n",
    "        .apply(lambda x: np.setdiff1d(group_unique_values, x))\\\n",
    "        .explode().reset_index()\n",
    "    \n",
    "    # Join to create recommendation based on lack of interaction\n",
    "    candidates = pd.merge(not_interacted_with, popular_by_group, on=articles_col)\n",
    "\n",
    "    return candidates[['customer_id', 'article_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column product_group_name: \n",
      "\t Score: 0.00098.\n",
      "\n",
      "Prediction using column graphical_appearance_name: \n",
      "\t Score: 0.00061.\n",
      "\n",
      "Prediction using column colour_group_name: \n",
      "\t Score: 0.00162.\n",
      "\n",
      "Prediction using column perceived_colour_value_name: \n",
      "\t Score: 0.00089.\n",
      "\n",
      "Prediction using column perceived_colour_master_name: \n",
      "\t Score: 0.00180.\n",
      "\n",
      "Prediction using column index_name: \n",
      "\t Score: 0.00027.\n",
      "\n",
      "Prediction using column index_group_name: \n",
      "\t Score: 0.00084.\n",
      "\n",
      "Prediction using column section_name: \n",
      "\t Score: 0.00016.\n",
      "\n",
      "Prediction using column garment_group_name: \n",
      "\t Score: 0.00165.\n"
     ]
    }
   ],
   "source": [
    "article_groups_cols = [\n",
    "    'product_group_name',\n",
    "    'graphical_appearance_name',\n",
    "    'colour_group_name',\n",
    "    'perceived_colour_value_name',\n",
    "    'perceived_colour_master_name',\n",
    "    'index_name',\n",
    "    'index_group_name',\n",
    "    'section_name',\n",
    "    'garment_group_name'\n",
    "]\n",
    "for col_name in article_groups_cols:\n",
    "    candidates = not_interacted_with_candidates(transactions_train, articles, col_name, 5)\n",
    "\n",
    "    candidates.to_csv(f'../../data/candidates/not_interacted_with_{col_name}.csv', index=False)\n",
    "\n",
    "    predictions_df = candidates\\\n",
    "        .groupby('customer_id')['article_id']\\\n",
    "        .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "        .apply(lambda x: x[:12])\\\n",
    "        .reset_index()\n",
    "\n",
    "    score = calculate_score(test_week_transactions, predictions_df)\n",
    "\n",
    "    print(f'\\nPrediction using column {col_name}: \\n\\t Score: {score:.5f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction using column product_group_name: \n",
      "\t Score: 0.00091.\n",
      "\n",
      "Prediction using column graphical_appearance_name: \n",
      "\t Score: 0.00074.\n",
      "\n",
      "Prediction using column colour_group_name: \n",
      "\t Score: 0.00149.\n",
      "\n",
      "Prediction using column perceived_colour_value_name: \n",
      "\t Score: 0.00175.\n",
      "\n",
      "Prediction using column perceived_colour_master_name: \n",
      "\t Score: 0.00152.\n",
      "\n",
      "Prediction using column index_name: \n",
      "\t Score: 0.00048.\n",
      "\n",
      "Prediction using column index_group_name: \n",
      "\t Score: 0.00255.\n",
      "\n",
      "Prediction using column section_name: \n",
      "\t Score: 0.00011.\n",
      "\n",
      "Prediction using column garment_group_name: \n",
      "\t Score: 0.00176.\n"
     ]
    }
   ],
   "source": [
    "article_groups_cols = [\n",
    "    'product_group_name',\n",
    "    'graphical_appearance_name',\n",
    "    'colour_group_name',\n",
    "    'perceived_colour_value_name',\n",
    "    'perceived_colour_master_name',\n",
    "    'index_name',\n",
    "    'index_group_name',\n",
    "    'section_name',\n",
    "    'garment_group_name'\n",
    "]\n",
    "k = 2\n",
    "for col_name in article_groups_cols:\n",
    "    candidates = not_interacted_with_candidates(transactions_train, articles, col_name, k)\n",
    "\n",
    "    candidates.to_csv(f'../../data/candidates/not_interacted_{k}/not_interacted_with_{col_name}.csv', index=False)\n",
    "\n",
    "    predictions_df = candidates\\\n",
    "        .groupby('customer_id')['article_id']\\\n",
    "        .apply(lambda x: list(x) + bestsellers_last_week)\\\n",
    "        .apply(lambda x: x[:12])\\\n",
    "        .reset_index()\n",
    "\n",
    "    score = calculate_score(test_week_transactions, predictions_df)\n",
    "\n",
    "    print(f'\\nPrediction using column {col_name}: \\n\\t Score: {score:.5f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
