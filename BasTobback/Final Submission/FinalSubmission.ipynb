{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 31254,
     "databundleVersionId": 3103714,
     "sourceType": "competition"
    },
    {
     "sourceId": 93163345,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30178,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Radek posted about this [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/309220), and linked to a GitHub repo with the code.\n",
    "\n",
    "I just transferred that code here to Kaggle notebooks, that's all."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:36.258435Z",
     "iopub.execute_input": "2023-12-21T16:22:36.258819Z",
     "iopub.status.idle": "2023-12-21T16:22:36.269728Z",
     "shell.execute_reply.started": "2023-12-21T16:22:36.258781Z",
     "shell.execute_reply": "2023-12-21T16:22:36.269033Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\n",
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)\n",
    "\n",
    "def article_id_str_to_int(series):\n",
    "    return series.astype('int32')\n",
    "\n",
    "def article_id_int_to_str(series):\n",
    "    return '0' + series.astype('str')\n",
    "\n",
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "def calculate_apk(list_of_preds, list_of_gts):\n",
    "    # for fast validation this can be changed to operate on dicts of {'cust_id_int': [art_id_int, ...]}\n",
    "    # using 'data/val_week_purchases_by_cust.pkl'\n",
    "    apks = []\n",
    "    for preds, gt in zip(list_of_preds, list_of_gts):\n",
    "        apks.append(apk(gt, preds, k=12))\n",
    "    return np.mean(apks)\n",
    "\n",
    "def eval_sub(sub_csv, skip_cust_with_no_purchases=True):\n",
    "    sub=pd.read_csv(sub_csv)\n",
    "    validation_set=pd.read_parquet('data/validation_ground_truth.parquet')\n",
    "\n",
    "    apks = []\n",
    "\n",
    "    no_purchases_pattern = []\n",
    "    for pred, gt in zip(sub.prediction.str.split(), validation_set.prediction.str.split()):\n",
    "        if skip_cust_with_no_purchases and (gt == no_purchases_pattern): continue\n",
    "        apks.append(apk(gt, pred, k=12))\n",
    "    return np.mean(apks)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:36.271793Z",
     "iopub.execute_input": "2023-12-21T16:22:36.272574Z",
     "iopub.status.idle": "2023-12-21T16:22:36.293192Z",
     "shell.execute_reply.started": "2023-12-21T16:22:36.272535Z",
     "shell.execute_reply": "2023-12-21T16:22:36.292240Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:36.294547Z",
     "iopub.execute_input": "2023-12-21T16:22:36.294998Z",
     "iopub.status.idle": "2023-12-21T16:22:36.308674Z",
     "shell.execute_reply.started": "2023-12-21T16:22:36.294952Z",
     "shell.execute_reply": "2023-12-21T16:22:36.307216Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "transactions = pd.read_parquet('../input/warmup/transactions_train.parquet')\n",
    "customers = pd.read_parquet('../input/warmup/customers.parquet')\n",
    "articles = pd.read_parquet('../input/warmup/articles.parquet')\n",
    "sample_submission = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "# sample = 0.05\n",
    "# transactions = pd.read_parquet(f'data/transactions_train_sample_{sample}.parquet')\n",
    "# customers = pd.read_parquet(f'data/customers_sample_{sample}.parquet')\n",
    "# articles = pd.read_parquet(f'data/articles_train_sample_{sample}.parquet')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:36.310471Z",
     "iopub.execute_input": "2023-12-21T16:22:36.310736Z",
     "iopub.status.idle": "2023-12-21T16:22:50.157333Z",
     "shell.execute_reply.started": "2023-12-21T16:22:36.310706Z",
     "shell.execute_reply": "2023-12-21T16:22:50.156237Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 6.2 s, sys: 4.2 s, total: 10.4 s\nWall time: 13.8 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering\n",
    "We want to add some features or change some values, therefore we engineer some features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# define age groups\n",
    "def get_age_group(age):\n",
    "    if age < 15:\n",
    "        return 0\n",
    "    elif age >= 18 and age < 25:\n",
    "        return 1\n",
    "    elif age >= 25 and age < 35:\n",
    "        return 2\n",
    "    elif age >= 35 and age < 45:\n",
    "        return 3\n",
    "    elif age >= 45 and age < 55:\n",
    "        return 4\n",
    "    elif age >= 55 and age < 65:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:50.160039Z",
     "iopub.execute_input": "2023-12-21T16:22:50.160880Z",
     "iopub.status.idle": "2023-12-21T16:22:50.169629Z",
     "shell.execute_reply.started": "2023-12-21T16:22:50.160823Z",
     "shell.execute_reply": "2023-12-21T16:22:50.168541Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transactions"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:50.171112Z",
     "iopub.execute_input": "2023-12-21T16:22:50.171481Z",
     "iopub.status.idle": "2023-12-21T16:22:50.202356Z",
     "shell.execute_reply.started": "2023-12-21T16:22:50.171433Z",
     "shell.execute_reply": "2023-12-21T16:22:50.200986Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": [
    {
     "execution_count": 36,
     "output_type": "execute_result",
     "data": {
      "text/plain": "              t_dat           customer_id  article_id     price  \\\n25784    2018-09-20      1728846800780188   519773001  0.028458   \n25785    2018-09-20      1728846800780188   578472001  0.032525   \n5389     2018-09-20      2076973761519164   661795002  0.167797   \n5390     2018-09-20      2076973761519164   684080003  0.101678   \n47429    2018-09-20      2918879973994241   662980001  0.033881   \n...             ...                   ...         ...       ...   \n31774722 2020-09-22  18439937050817258297   891591003  0.084729   \n31774723 2020-09-22  18439937050817258297   869706005  0.084729   \n31779097 2020-09-22  18440902715633436014   918894002  0.016932   \n31779098 2020-09-22  18440902715633436014   761269001  0.016932   \n31780475 2020-09-22  18443633011701112574   914868002  0.033881   \n\n          sales_channel_id  week  \n25784                    2     0  \n25785                    2     0  \n5389                     2     0  \n5390                     2     0  \n47429                    1     0  \n...                    ...   ...  \n31774722                 2   104  \n31774723                 2   104  \n31779097                 1   104  \n31779098                 1   104  \n31780475                 1   104  \n\n[31788324 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25784</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>519773001</td>\n      <td>0.028458</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25785</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>578472001</td>\n      <td>0.032525</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5389</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>661795002</td>\n      <td>0.167797</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>684080003</td>\n      <td>0.101678</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47429</th>\n      <td>2018-09-20</td>\n      <td>2918879973994241</td>\n      <td>662980001</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31774722</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>891591003</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31774723</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>869706005</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779097</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>918894002</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779098</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>761269001</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31780475</th>\n      <td>2020-09-22</td>\n      <td>18443633011701112574</td>\n      <td>914868002</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n  </tbody>\n</table>\n<p>31788324 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making a recall evaluation function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# return the average recall of generated candidates versus the actual bought items\n",
    "def average_recall(purchases, candidates):\n",
    "    joined = pd.merge(purchases, candidates, how='inner').drop_duplicates()\n",
    "    true_positives = joined.groupby('customer_id').count()\n",
    "    total_positives = purchases.groupby('customer_id').count()\n",
    "    recall = true_positives.divide(total_positives, fill_value=0)\n",
    "    return recall.mean().values[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:50.204174Z",
     "iopub.execute_input": "2023-12-21T16:22:50.204811Z",
     "iopub.status.idle": "2023-12-21T16:22:50.213306Z",
     "shell.execute_reply.started": "2023-12-21T16:22:50.204481Z",
     "shell.execute_reply": "2023-12-21T16:22:50.211722Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating candidates\n",
    "TODO: ensure the bestsellers of last week can be save to add to empty users"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def candidate_generation(data, test_week):\n",
    "    \"\"\"\n",
    "    Return the candidates for test_week based on data given. The candidates that are generated are the repurchase candidates, the bestsellers and the bestsellers based on an age group.\n",
    "    :param data: a pandas dataframe with the transactions\n",
    "    :param test_week: a value that indicates the week that is supposed to be taken, advised is using either 104 as testing or 105 as final target week\n",
    "    :return: pandas dataframe containing the candidates for all customers available in data\n",
    "    \"\"\"\n",
    "    ################\n",
    "    ## Repurchase ##\n",
    "    ################\n",
    "    \"\"\"\n",
    "    The repurchases as they were generated in the Radek baseline. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    c2weeks = transactions.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = transactions.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(transactions['customer_id'], transactions['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "    \n",
    "    ################\n",
    "    ## bestseller ##\n",
    "    ################\n",
    "    \"\"\"\n",
    "    The bestsellers as they were generated in the Radek baseline. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    mean_price = transactions \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = transactions \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Filer is replaced later on\n",
    "    \"\"\"\n",
    "\n",
    "    unique_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "    \n",
    "    # testing the recall of the bestseller candidates which are generated\n",
    "    # done by taking the subsets of the actual transactions and the generated candidates ONLY WHEN the test week is not bigger than the absolute max_week which is the highest week available in the dataset\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        t_candidates = candidates_bestsellers[candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        print(f\"Average recall of bestsellers : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    ###################################\n",
    "    ## Bestseller based on age group ##\n",
    "    ###################################\n",
    "    \"\"\"\n",
    "    The bestsellers as they were generated in the previous file. All pieces of code are kept intact. \n",
    "    \"\"\"\n",
    "    # Group the mean_price not per week/article but by week/article/age_group\n",
    "    # this is so we know\n",
    "    mean_price_age_group = transactions \\\n",
    "        .groupby(['week', 'age_group', 'article_id'])['price'].mean()\n",
    "\n",
    "    # group the sales by week AND the age group and so find the most popular article for each age group in each week\n",
    "    sales_age_group = transactions \\\n",
    "        .groupby(['week', 'age_group'])['article_id'].value_counts() \\\n",
    "        .groupby(['week', 'age_group']).rank(method='dense', ascending=False) \\\n",
    "        .groupby(['week', 'age_group']).head(12).rename('age_group_bestseller_rank').astype('int8')\n",
    "\n",
    "    \n",
    "    # now calculate the bestsellers for these week - age_group combos\n",
    "    bestsellers_previous_week_age_group = pd.merge(sales_age_group, mean_price_age_group, on=['week', 'age_group', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week_age_group.week += 1\n",
    "\n",
    "    \"\"\"\n",
    "    This part is making sure I have a filler. The filler is used a a list of predictions used to fill the list of customers that were not considered in the candidate generation process.\n",
    "    This filler is a dataframe consisting of the bestsellers per age group\n",
    "    \"\"\"\n",
    "    # fillers from age groups\n",
    "    filler = bestsellers_previous_week_age_group[bestsellers_previous_week_age_group.week == bestsellers_previous_week_age_group.week.max()].groupby('age_group').article_id.apply(list)\n",
    "#     print(\"=== FILLER ===\")\n",
    "#     print(filler)\n",
    "    \n",
    "    unique_age_group_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "\n",
    "    age_group_candidates_bestsellers = pd.merge(\n",
    "        unique_age_group_transactions,\n",
    "        bestsellers_previous_week_age_group,\n",
    "        on=['week', 'age_group'],\n",
    "    )\n",
    "    test_set_age_group_transactions = unique_age_group_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_age_group_transactions.week = test_week\n",
    "\n",
    "    age_group_candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_age_group_transactions,\n",
    "        bestsellers_previous_week_age_group,\n",
    "        on=['week', 'age_group'],\n",
    "    )\n",
    "    age_group_candidates_bestsellers = pd.concat([age_group_candidates_bestsellers, age_group_candidates_bestsellers_test_week])\n",
    "    age_group_candidates_bestsellers.drop(columns='age_group_bestseller_rank', inplace=True)\n",
    "    \n",
    "    # testing the recall of the age group bestseller candidates which are generated\n",
    "    # done by taking the subsets of the actual transactions and the generated candidates ONLY WHEN the test week is not bigger than the absolute max_week which is the highest week available in the dataset\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        t_candidates = age_group_candidates_bestsellers[age_group_candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        print(f\"Average recall of age group bestsellers : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    \n",
    "    ###################################################\n",
    "    # Combine the transactions and negative examples ##\n",
    "    ###################################################\n",
    "    \"\"\"\n",
    "    Here the code which I have written in one of the previous steps is omitted. This is the part of the sin values of age etc. They did not seem pertinent to the objective of this step of my research. The rest is still the same as the code written in the baseline file with age groups.  The initial assumption was that the results would be similar, but they aren't.\n",
    "    NOTE: After doing this I realized some mistakes in usage of variables, the variable just below is not used.\n",
    "    \"\"\"\n",
    "    \n",
    "    # adding a feature for each method of candidate generation as its origin feature\n",
    "    purchased_transactions = data.copy()\n",
    "    purchased_transactions['purchased'] = 1\n",
    "    candidates_last_purchase['repurchase_candidate'] = 1\n",
    "    age_group_candidates_bestsellers['age_popularity_candidate'] = 1\n",
    "    candidates_bestsellers['popularity_candidate'] = 1\n",
    "    \n",
    "    result = pd.concat([\n",
    "        purchased_transactions, candidates_last_purchase, age_group_candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    # some printings of the average recall of combined candidates\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        together = pd.concat([age_group_candidates_bestsellers, candidates_bestsellers])\n",
    "        t_candidates = together[together.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "\n",
    "        print(f\"Average recall of total : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    ######################################################\n",
    "    # Fill in with zeroes for which columns is included ##\n",
    "    ###################################################### \n",
    "    result.repurchase_candidate.fillna(0, inplace=True)\n",
    "    result.age_popularity_candidate.fillna(0, inplace=True)\n",
    "#     result.popularity_candidate.fillna(0, inplace=True)\n",
    "\n",
    "    # by first taking the id week purchased, repurchase etc we have a collection of all origin features\n",
    "    # and of all necessary discriminatory features. Then dropping them from result ensures the ability te merge after\n",
    "    # grouping the items by week article is and customer gives us the ability to use .any() to ensure we keep all origins from where the item came in one entry\n",
    "    temp = result[[\"customer_id\", \"week\", \"article_id\", \"purchased\",\"repurchase_candidate\", \"age_popularity_candidate\"]]\n",
    "    result.drop(columns=[\"purchased\",\"repurchase_candidate\", \"age_popularity_candidate\"], inplace=True)\n",
    "    temp = temp.groupby([\"customer_id\", \"week\", \"article_id\"], as_index=False).any()\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        temp,\n",
    "        on=[\"customer_id\", \"week\", \"article_id\"]\n",
    "    )\n",
    "    \n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    # merge the data with the bestsellers information from the age_group popularity study\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week_age_group[['week', 'age_group', 'article_id', 'age_group_bestseller_rank']],\n",
    "        on=['week', 'age_group', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "    result.age_group_bestseller_rank.fillna(999, inplace=True)\n",
    "    \n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result, filler"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:50.216049Z",
     "iopub.execute_input": "2023-12-21T16:22:50.217019Z",
     "iopub.status.idle": "2023-12-21T16:22:52.011388Z",
     "shell.execute_reply.started": "2023-12-21T16:22:50.216918Z",
     "shell.execute_reply": "2023-12-21T16:22:52.010519Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def add_features(data):\n",
    "    columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n",
    "    'perceived_colour_master_id', 'department_no', 'index_code',\n",
    "    'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n",
    "    'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', 'bestseller_rank', 'age_group_bestseller_rank', 'age_group',\n",
    "    'repurchase_candidate', 'age_popularity_candidate'\n",
    "#     , 'popularity_candidate'\n",
    "    ]\n",
    "    \n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on=['customer_id', 'age_group'])\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "    \n",
    "    # features from assignment 2 can go here\n",
    "    \n",
    "    return result[columns_to_use]\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:52.013074Z",
     "iopub.execute_input": "2023-12-21T16:22:52.013433Z",
     "iopub.status.idle": "2023-12-21T16:22:52.026736Z",
     "shell.execute_reply.started": "2023-12-21T16:22:52.013393Z",
     "shell.execute_reply": "2023-12-21T16:22:52.025782Z"
    },
    "trusted": true
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# use the generation for training and testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# define the test week and limit the data to a set of previous weeks\n",
    "\"\"\"\n",
    "The same code as the previous step. Choosing a week, using it to set boundaries on used data\n",
    "\"\"\"\n",
    "test_week = 105\n",
    "num_training_weeks = 10\n",
    "absolute_max_week = transactions.week.max()\n",
    "print(test_week)\n",
    "test_week_transactions = transactions[transactions.week == test_week]\n",
    "transactions = transactions[(transactions.week > test_week - num_training_weeks - 1) & (transactions.week < test_week)].reset_index(drop=True)\n",
    "\n",
    "customers[\"age_group\"] = customers[\"age\"].apply(get_age_group)\n",
    "# firstly take the age_groups and the cutomer ids\n",
    "age_groups_customers = customers[['customer_id', 'age_group']].drop_duplicates()\n",
    "\n",
    "# now join them into the transactions to create a new transactions set to work with\n",
    "transactions = pd.merge(transactions, age_groups_customers)\n",
    "# now the age_group is included, we will have to change some values and names to ensure this is used\n",
    "\n",
    "# assemble training data by using positive and negative samples\n",
    "examples, filler = candidate_generation(transactions, test_week)\n",
    "print(examples)\n",
    "train_examples = examples[examples.week != test_week]\n",
    "train_x = add_features(train_examples)\n",
    "train_y = train_examples['purchased']\n",
    "print(train_y.value_counts())\n",
    "\n",
    "# make the ranker, make the train_groups\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "# sort the training_examples\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(train_x, train_y, group=train_groups)\n",
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    print(train_x.columns[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())\n",
    "    \n",
    "\"\"\"\n",
    "predict the scores similarly to Radek's baseline and then create submissions in case the week is correct\n",
    "\"\"\" \n",
    "\n",
    "# testing\n",
    "test_examples_not_copy = examples[examples.week == test_week]\n",
    "print(test_examples_not_copy)\n",
    "test_examples = test_examples_not_copy.copy()\n",
    "test_x = add_features(test_examples)\n",
    "\n",
    "test_examples[\"score\"] = ranker.predict(test_x)\n",
    "\n",
    "test_examples = test_examples[['customer_id', 'article_id', 'score']]\n",
    "print(test_examples)\n",
    "predictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n",
    "    .groupby(\"customer_id\")\\\n",
    "    .head(12)\\\n",
    "    .groupby(\"customer_id\")\\\n",
    "    .article_id.apply(list).reset_index()\\\n",
    "    .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n",
    "print(predictions)\n",
    "\n",
    "\"\"\"\n",
    "Here the week is checked and the submission made if the week is correct. Otherwise there is room for manual checking and evaluating\n",
    "\"\"\"\n",
    "### evaluate\n",
    "# if the test week is a week of which the data is in fact known\n",
    "if test_week < absolute_max_week:\n",
    "    print(\"In a previous week right now\")\n",
    "    pass\n",
    "\n",
    "# if the week is our target week\n",
    "else:\n",
    "    \"\"\"\n",
    "    make the missing customers as a series\n",
    "    then merge them with the filler and check some contents. \n",
    "    finally make the submission\n",
    "    \"\"\"\n",
    "    # take the list of missing customers\n",
    "    missing_customers_list = list(set(age_groups_customers.customer_id) - set(predictions.customer_id))\n",
    "    \n",
    "    # take the dataframe of customer information where the customers are part of the missing customers\n",
    "    missing_customers = age_groups_customers[age_groups_customers.customer_id.isin(missing_customers_list)]\n",
    "    # merge it with the filler which is now a dataframe based on age groups\n",
    "    missing_predictions = pd.merge(\n",
    "        missing_customers, filler, on=\"age_group\"\n",
    "    )\n",
    "    missing_predictions.drop(columns='age_group', inplace = True)\n",
    "    missing_predictions.rename(columns={'article_id' : 'prediction'}, inplace=True)\n",
    "    print(\"====================\")\n",
    "    print(missing_customers)\n",
    "    print(\"====================\")\n",
    "    print(filler)\n",
    "    print(\"====================\")\n",
    "    print(missing_predictions)\n",
    "    predictions = pd.concat((predictions, missing_predictions))\n",
    "    \n",
    "    # create a submission\n",
    "    predictions = predictions.set_index(\"customer_id\").prediction.to_dict()\n",
    "    preds = []\n",
    "    sub = sample_submission.copy()\n",
    "    for customer_id in customer_hex_id_to_int(sub.customer_id):\n",
    "        preds.append(\" \".join(f\"0{x}\" for x in predictions[customer_id]))\n",
    "    sub.prediction = preds\n",
    "    \n",
    "    # to csv\n",
    "    sub_name = 'testing_submission_with_bestsellers'\n",
    "    sub.to_csv(f'{sub_name}.csv.gz', index=False)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T16:22:52.028340Z",
     "iopub.execute_input": "2023-12-21T16:22:52.028866Z",
     "iopub.status.idle": "2023-12-21T16:24:13.239077Z",
     "shell.execute_reply.started": "2023-12-21T16:22:52.028807Z",
     "shell.execute_reply": "2023-12-21T16:24:13.237876Z"
    },
    "trusted": true
   },
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "text": "100\nAverage recall of bestsellers : 0.01539928540078478\nAverage recall of age group bestsellers : 0.016811439857364108\nAverage recall of total : 0.022375378969914882\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_17/3013030120.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;31m# assemble training data by using positive and negative samples\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0mexamples\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiller\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcandidate_generation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransactions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_week\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexamples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0mtrain_examples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexamples\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mexamples\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweek\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mtest_week\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_17/392334190.py\u001B[0m in \u001B[0;36mcandidate_generation\u001B[0;34m(data, test_week)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mtemp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"customer_id\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"week\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"article_id\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"purchased\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"repurchase_candidate\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"age_popularity_candidate\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"purchased\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"repurchase_candidate\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"age_popularity_candidate\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m     \u001B[0mtemp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtemp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"customer_id\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"week\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"article_id\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     result = pd.merge(\n\u001B[1;32m    152\u001B[0m         \u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001B[0m in \u001B[0;36many\u001B[0;34m(self, skipna)\u001B[0m\n\u001B[1;32m   1596\u001B[0m             \u001B[0;32mis\u001B[0m \u001B[0;32mTrue\u001B[0m \u001B[0mwithin\u001B[0m \u001B[0mits\u001B[0m \u001B[0mrespective\u001B[0m \u001B[0mgroup\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m \u001B[0motherwise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1597\u001B[0m         \"\"\"\n\u001B[0;32m-> 1598\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bool_agg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"any\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1599\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1600\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfinal\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001B[0m in \u001B[0;36m_bool_agg\u001B[0;34m(self, val_test, skipna)\u001B[0m\n\u001B[1;32m   1575\u001B[0m             \u001B[0mpost_processing\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresult_to_bool\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1576\u001B[0m             \u001B[0mval_test\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_test\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1577\u001B[0;31m             \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1578\u001B[0m         )\n\u001B[1;32m   1579\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001B[0m in \u001B[0;36m_get_cythonized_result\u001B[0;34m(self, how, cython_dtype, aggregate, numeric_only, needs_counts, needs_values, needs_2d, needs_nullable, min_count, needs_mask, needs_ngroups, result_is_index, pre_processing, post_processing, **kwargs)\u001B[0m\n\u001B[1;32m   2878\u001B[0m         \u001B[0mgrouper\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrouper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2879\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2880\u001B[0;31m         \u001B[0mids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mngroups\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgrouper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroup_info\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2881\u001B[0m         \u001B[0moutput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbase\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOutputKey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2882\u001B[0m         \u001B[0mbase_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlibgroupby\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/properties.pyx\u001B[0m in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001B[0m in \u001B[0;36mgroup_info\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    909\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mcache_readonly\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    910\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgroup_info\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 911\u001B[0;31m         \u001B[0mcomp_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobs_group_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_compressed_codes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    912\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    913\u001B[0m         \u001B[0mngroups\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs_group_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001B[0m in \u001B[0;36m_get_compressed_codes\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    930\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupings\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    931\u001B[0m             \u001B[0mgroup_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_group_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcodes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msort\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxnull\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 932\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcompress_group_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgroup_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msort\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sort\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    933\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    934\u001B[0m         \u001B[0mping\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupings\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/sorting.py\u001B[0m in \u001B[0;36mcompress_group_index\u001B[0;34m(group_index, sort)\u001B[0m\n\u001B[1;32m    659\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    660\u001B[0m     \u001B[0;31m# note, group labels come out ascending (ie, 1,2,3 etc)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 661\u001B[0;31m     \u001B[0mcomp_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobs_group_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtable\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_labels_groupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgroup_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    662\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    663\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msort\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs_group_ids\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submission"
   ],
   "metadata": {}
  }
 ]
}
