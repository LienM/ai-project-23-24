{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":93163345,"sourceType":"kernelVersion"}],"dockerImageVersionId":30178,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Radek posted about this [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/309220), and linked to a GitHub repo with the code.\n\nI just transferred that code here to Kaggle notebooks, that's all.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom lightgbm.sklearn import LGBMRanker\n\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n\n    This function computes the average prescision at k between two lists of\n    items.\n\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n\n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n\n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n\n    \"\"\"\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:03.301253Z","iopub.execute_input":"2023-12-18T10:53:03.302331Z","iopub.status.idle":"2023-12-18T10:53:05.924016Z","shell.execute_reply.started":"2023-12-18T10:53:03.302218Z","shell.execute_reply":"2023-12-18T10:53:05.922980Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n.datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\n# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\ndef customer_hex_id_to_int(series):\n    return series.str[-16:].apply(hex_id_to_int)\n\ndef hex_id_to_int(str):\n    return int(str[-16:], 16)\n\ndef article_id_str_to_int(series):\n    return series.astype('int32')\n\ndef article_id_int_to_str(series):\n    return '0' + series.astype('str')\n\nclass Categorize(BaseEstimator, TransformerMixin):\n    def __init__(self, min_examples=0):\n        self.min_examples = min_examples\n        self.categories = []\n        \n    def fit(self, X):\n        for i in range(X.shape[1]):\n            vc = X.iloc[:, i].value_counts()\n            self.categories.append(vc[vc > self.min_examples].index.tolist())\n        return self\n\n    def transform(self, X):\n        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n        return pd.DataFrame(data=data)\n\n\ndef calculate_apk(list_of_preds, list_of_gts):\n    # for fast validation this can be changed to operate on dicts of {'cust_id_int': [art_id_int, ...]}\n    # using 'data/val_week_purchases_by_cust.pkl'\n    apks = []\n    for preds, gt in zip(list_of_preds, list_of_gts):\n        apks.append(apk(gt, preds, k=12))\n    return np.mean(apks)\n\ndef eval_sub(sub_csv, skip_cust_with_no_purchases=True):\n    sub=pd.read_csv(sub_csv)\n    validation_set=pd.read_parquet('data/validation_ground_truth.parquet')\n\n    apks = []\n\n    no_purchases_pattern = []\n    for pred, gt in zip(sub.prediction.str.split(), validation_set.prediction.str.split()):\n        if skip_cust_with_no_purchases and (gt == no_purchases_pattern): continue\n        apks.append(apk(gt, pred, k=12))\n    return np.mean(apks)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:05.925843Z","iopub.execute_input":"2023-12-18T10:53:05.926159Z","iopub.status.idle":"2023-12-18T10:53:05.945698Z","shell.execute_reply.started":"2023-12-18T10:53:05.926116Z","shell.execute_reply":"2023-12-18T10:53:05.944705Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:05.947061Z","iopub.execute_input":"2023-12-18T10:53:05.947903Z","iopub.status.idle":"2023-12-18T10:53:05.964812Z","shell.execute_reply.started":"2023-12-18T10:53:05.947857Z","shell.execute_reply":"2023-12-18T10:53:05.964082Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntransactions = pd.read_parquet('../input/warmup/transactions_train.parquet')\ncustomers = pd.read_parquet('../input/warmup/customers.parquet')\narticles = pd.read_parquet('../input/warmup/articles.parquet')\nsample_submission = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n# sample = 0.05\n# transactions = pd.read_parquet(f'data/transactions_train_sample_{sample}.parquet')\n# customers = pd.read_parquet(f'data/customers_sample_{sample}.parquet')\n# articles = pd.read_parquet(f'data/articles_train_sample_{sample}.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:05.966428Z","iopub.execute_input":"2023-12-18T10:53:05.967085Z","iopub.status.idle":"2023-12-18T10:53:23.885056Z","shell.execute_reply.started":"2023-12-18T10:53:05.967041Z","shell.execute_reply":"2023-12-18T10:53:23.884361Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CPU times: user 5.3 s, sys: 3.38 s, total: 8.68 s\nWall time: 17.9 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature engineering\nWe want to add some features or change some values, therefore we engineer some features","metadata":{}},{"cell_type":"code","source":"# define age groups\ndef get_age_group(age):\n    if age < 18:\n        return 0\n    elif age >= 18 and age < 25:\n        return 1\n    elif age >= 25 and age < 35:\n        return 2\n    elif age >= 35 and age < 45:\n        return 3\n    elif age >= 45 and age < 55:\n        return 4\n    elif age >= 55 and age < 65:\n        return 5\n    else:\n        return 6","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:23.886289Z","iopub.execute_input":"2023-12-18T10:53:23.886558Z","iopub.status.idle":"2023-12-18T10:53:23.891735Z","shell.execute_reply.started":"2023-12-18T10:53:23.886528Z","shell.execute_reply":"2023-12-18T10:53:23.891129Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:23.892836Z","iopub.execute_input":"2023-12-18T10:53:23.893394Z","iopub.status.idle":"2023-12-18T10:53:23.922196Z","shell.execute_reply.started":"2023-12-18T10:53:23.893360Z","shell.execute_reply":"2023-12-18T10:53:23.921170Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"              t_dat           customer_id  article_id     price  \\\n25784    2018-09-20      1728846800780188   519773001  0.028458   \n25785    2018-09-20      1728846800780188   578472001  0.032525   \n5389     2018-09-20      2076973761519164   661795002  0.167797   \n5390     2018-09-20      2076973761519164   684080003  0.101678   \n47429    2018-09-20      2918879973994241   662980001  0.033881   \n...             ...                   ...         ...       ...   \n31774722 2020-09-22  18439937050817258297   891591003  0.084729   \n31774723 2020-09-22  18439937050817258297   869706005  0.084729   \n31779097 2020-09-22  18440902715633436014   918894002  0.016932   \n31779098 2020-09-22  18440902715633436014   761269001  0.016932   \n31780475 2020-09-22  18443633011701112574   914868002  0.033881   \n\n          sales_channel_id  week  \n25784                    2     0  \n25785                    2     0  \n5389                     2     0  \n5390                     2     0  \n47429                    1     0  \n...                    ...   ...  \n31774722                 2   104  \n31774723                 2   104  \n31779097                 1   104  \n31779098                 1   104  \n31780475                 1   104  \n\n[31788324 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25784</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>519773001</td>\n      <td>0.028458</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25785</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>578472001</td>\n      <td>0.032525</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5389</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>661795002</td>\n      <td>0.167797</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>684080003</td>\n      <td>0.101678</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47429</th>\n      <td>2018-09-20</td>\n      <td>2918879973994241</td>\n      <td>662980001</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31774722</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>891591003</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31774723</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>869706005</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779097</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>918894002</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779098</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>761269001</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31780475</th>\n      <td>2020-09-22</td>\n      <td>18443633011701112574</td>\n      <td>914868002</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n  </tbody>\n</table>\n<p>31788324 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Making a recall evaluation function","metadata":{}},{"cell_type":"code","source":"# return the average recall of generated candidates versus the actual bought items\ndef average_recall(purchases, candidates):\n    joined = pd.merge(purchases, candidates, how='inner').drop_duplicates()\n    true_positives = joined.groupby('customer_id').count()\n    total_positives = purchases.groupby('customer_id').count()\n    recall = true_positives.divide(total_positives, fill_value=0)\n    return recall.mean().values[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:23.923849Z","iopub.execute_input":"2023-12-18T10:53:23.924545Z","iopub.status.idle":"2023-12-18T10:53:23.930479Z","shell.execute_reply.started":"2023-12-18T10:53:23.924492Z","shell.execute_reply":"2023-12-18T10:53:23.929739Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Generating candidates\nTODO: ensure the bestsellers of last week can be save to add to empty users","metadata":{}},{"cell_type":"code","source":"def candidate_generation(data, test_week):\n    ################\n    ## Repurchase ##\n    ################\n    c2weeks = transactions.groupby('customer_id')['week'].unique()\n    c2weeks2shifted_weeks = {}\n    for c_id, weeks in c2weeks.items():\n        c2weeks2shifted_weeks[c_id] = {}\n        for i in range(weeks.shape[0]-1):\n            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n    candidates_last_purchase = transactions.copy()\n    weeks = []\n    for i, (c_id, week) in enumerate(zip(transactions['customer_id'], transactions['week'])):\n        weeks.append(c2weeks2shifted_weeks[c_id][week])\n    candidates_last_purchase.week=weeks\n    \n    ################\n    ## bestseller ##\n    ################\n    mean_price = transactions \\\n        .groupby(['week', 'article_id'])['price'].mean()\n    sales = transactions \\\n        .groupby('week')['article_id'].value_counts() \\\n        .groupby('week').rank(method='dense', ascending=False) \\\n        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n    bestsellers_previous_week.week += 1\n    \n    filler = bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()].article_id.value_counts().head(12).index.values\n    print(\"=========================================\")\n    print(\"The content supposed to be in filler\")\n    print(bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()].article_id.value_counts().head(12).index.values)\n    print(\"=========================================\")\n    print(\"The content in filler\")\n    print(filler)\n\n    unique_transactions = transactions \\\n        .groupby(['week', 'customer_id']) \\\n        .head(1) \\\n        .drop(columns=['article_id', 'price']) \\\n        .copy()\n    candidates_bestsellers = pd.merge(\n        unique_transactions,\n        bestsellers_previous_week,\n        on='week',\n    )\n    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n    test_set_transactions.week = test_week\n    candidates_bestsellers_test_week = pd.merge(\n        test_set_transactions,\n        bestsellers_previous_week,\n        on='week'\n    )\n    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n    \n    if not test_week > absolute_max_week:\n        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        t_candidates = candidates_bestsellers[candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        print(f\"Average recall of bestsellers : {average_recall(t_purchases, t_candidates)}\")\n    \n    ###################################\n    ## Bestseller based on age group ##\n    ###################################\n    # Group the mean_price not per week/article but by week/article/age_group\n    # this is so we know\n    mean_price_age_group = transactions \\\n        .groupby(['week', 'age_group', 'article_id'])['price'].mean()\n\n    # group the sales by week AND the age group and so find the most popular article for each age group in each week\n    sales_age_group = transactions \\\n        .groupby(['week', 'age_group'])['article_id'].value_counts() \\\n        .groupby(['week', 'age_group']).rank(method='dense', ascending=False) \\\n        .groupby(['week', 'age_group']).head(12).rename('age_group_bestseller_rank').astype('int8')\n\n    # now calculate the bestsellers for these week - age_group combos\n    bestsellers_previous_week_age_group = pd.merge(sales_age_group, mean_price_age_group, on=['week', 'age_group', 'article_id']).reset_index()\n    bestsellers_previous_week_age_group.week += 1\n\n    # fillers from age groups\n    \n    \n    unique_age_group_transactions = transactions \\\n        .groupby(['week', 'customer_id']) \\\n        .head(1) \\\n        .drop(columns=['article_id', 'price']) \\\n        .copy()\n\n    age_group_candidates_bestsellers = pd.merge(\n        unique_age_group_transactions,\n        bestsellers_previous_week_age_group,\n        on=['week', 'age_group'],\n    )\n    test_set_age_group_transactions = unique_age_group_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n    test_set_age_group_transactions.week = test_week\n\n    age_group_candidates_bestsellers_test_week = pd.merge(\n        test_set_age_group_transactions,\n        bestsellers_previous_week_age_group,\n        on=['week', 'age_group'],\n    )\n    age_group_candidates_bestsellers = pd.concat([age_group_candidates_bestsellers, age_group_candidates_bestsellers_test_week])\n    age_group_candidates_bestsellers.drop(columns='age_group_bestseller_rank', inplace=True)\n    \n    if not test_week > absolute_max_week:\n        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        t_candidates = age_group_candidates_bestsellers[age_group_candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        print(f\"Average recall of age group bestsellers : {average_recall(t_purchases, t_candidates)}\")\n    \n    \n    ###################################################\n    # Combine the transactions and negative examples ##\n    ###################################################\n    purchased_transactions = data.copy()\n    purchased_transactions['purchased'] = 1\n    candidates_last_purchase['repurchase_candidate'] = 1\n    age_group_candidates_bestsellers['age_popularity_candidate'] = 1\n    candidates_bestsellers['popularity_candidate'] = 1\n    \n    result = pd.concat([\n        purchased_transactions, candidates_last_purchase, age_group_candidates_bestsellers\n    ])\n    result.purchased.fillna(0, inplace=True)\n    \n    ######################################################\n    # Fill in with zeroes for which columns is included ##\n    ###################################################### \n    result.repurchase_candidate.fillna(0, inplace=True)\n    result.age_popularity_candidate.fillna(0, inplace=True)\n#     result.popularity_candidate.fillna(0, inplace=True)\n\n    temp = result[[\"customer_id\", \"week\", \"article_id\", \"purchased\",\"repurchase_candidate\", \"age_popularity_candidate\"]]\n    result.drop(columns=[\"purchased\",\"repurchase_candidate\", \"age_popularity_candidate\"], inplace=True)\n    temp = temp.groupby([\"customer_id\", \"week\", \"article_id\"], as_index=False).any()\n    result = pd.merge(\n        result,\n        temp,\n        on=[\"customer_id\", \"week\", \"article_id\"]\n    )\n    \n    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n    result = pd.merge(\n        result,\n        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n        on=['week', 'article_id'],\n        how='left'\n    )\n    # merge the data with the bestsellers information from the age_group popularity study\n    result = pd.merge(\n        result,\n        bestsellers_previous_week_age_group[['week', 'age_group', 'article_id', 'age_group_bestseller_rank']],\n        on=['week', 'age_group', 'article_id'],\n        how='left'\n    )\n    result = result[result.week != result.week.min()]\n    result.bestseller_rank.fillna(999, inplace=True)\n    result.age_group_bestseller_rank.fillna(999, inplace=True)\n    \n    result.sort_values(['week', 'customer_id'], inplace=True)\n    result.reset_index(drop=True, inplace=True)\n    return result, filler","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:23.931952Z","iopub.execute_input":"2023-12-18T10:53:23.932473Z","iopub.status.idle":"2023-12-18T10:53:23.964435Z","shell.execute_reply.started":"2023-12-18T10:53:23.932407Z","shell.execute_reply":"2023-12-18T10:53:23.963070Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def add_features(data):\n    columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n    'perceived_colour_master_id', 'department_no', 'index_code',\n    'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n    'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', 'bestseller_rank', 'age_group_bestseller_rank', 'age_group',\n    'repurchase_candidate', 'age_popularity_candidate'\n#     , 'popularity_candidate'\n    ]\n    \n    result = data\n    result = pd.merge(result, customers, how='left', on=['customer_id', 'age_group'])\n    result = pd.merge(result, articles, how='left', on='article_id')\n    \n    # features from assignment 2 can go here\n    \n    return result[columns_to_use]\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:23.966175Z","iopub.execute_input":"2023-12-18T10:53:23.966482Z","iopub.status.idle":"2023-12-18T10:53:23.981131Z","shell.execute_reply.started":"2023-12-18T10:53:23.966447Z","shell.execute_reply":"2023-12-18T10:53:23.979967Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# use the generation for training and testing","metadata":{}},{"cell_type":"code","source":"# define the test week and limit the data to a set of previous weeks\ntest_week = 105\nnum_training_weeks = 10\nabsolute_max_week = transactions.week.max()\nprint(test_week)\ntest_week_transactions = transactions[transactions.week == test_week]\ntransactions = transactions[(transactions.week > test_week - num_training_weeks - 1) & (transactions.week < test_week)].reset_index(drop=True)\n\ncustomers[\"age_group\"] = customers[\"age\"].apply(get_age_group)\n# firstly take the age_groups and the cutomer ids\nage_groups_customers = customers[['customer_id', 'age_group']].drop_duplicates()\n\n# now join them into the transactions to create a new transactions set to work with\ntransactions = pd.merge(transactions, age_groups_customers)\n# now the age_group is included, we will have to change some values and names to ensure this is used\n\n# assemble training data by using positive and negative samples\nexamples, filler = candidate_generation(transactions, test_week)\nprint(examples)\ntrain_examples = examples[examples.week != test_week]\ntrain_x = add_features(train_examples)\ntrain_y = train_examples['purchased']\nprint(train_y.value_counts())\n\n# make the ranker, make the train_groups\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    n_estimators=1,\n    importance_type='gain',\n    verbose=10\n)\n# sort the training_examples\ntrain_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\nranker.fit(train_x, train_y, group=train_groups)\nfor i in ranker.feature_importances_.argsort()[::-1]:\n    print(train_x.columns[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())\n    \n\n# testing\ntest_examples_not_copy = examples[examples.week == test_week]\nprint(test_examples_not_copy)\ntest_examples = test_examples_not_copy.copy()\ntest_x = add_features(test_examples)\n\ntest_examples[\"score\"] = ranker.predict(test_x)\n\ntest_examples = test_examples[['customer_id', 'article_id', 'score']]\nprint(test_examples)\npredictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n    .groupby(\"customer_id\")\\\n    .head(12)\\\n    .groupby(\"customer_id\")\\\n    .article_id.apply(list).reset_index()\\\n    .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\nprint(predictions)\n\n# make the predictions of the customers\n# predictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n#     .groupby(\"customer_id\")\\\n#     .head(12)\\\n#     .groupby(\"customer_id\", as_index = False)\\\n#     .article_id.apply(list)\\\n#     .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n\n# scored_candidates.sort_values([\"customer_id\", \"score\"], ascending=False)\n#         .groupby(\"customer_id\")\n#         .head(k)\n#         .groupby(\"customer_id\", as_index=False)\n#         .article_id.apply(list)\n#         .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n\n\n### evaluate\n# if the test week is a week of which the data is in fact known\nif test_week < absolute_max_week:\n    print(\"In a previous week right now\")\n    pass\n\n# if the week is our target week\nelse:\n    missing_customers = pd.Series(\n        list(set(age_groups_customers.customer_id) - set(predictions.customer_id)),\n        name=\"customer_id\",\n    )\n    missing_predictions = pd.merge(\n        missing_customers, pd.Series([filler], name=\"prediction\"), how=\"cross\"\n    )\n    print(\"====================\")\n    print(missing_customers)\n    print(\"====================\")\n    print(filler)\n    print(\"====================\")\n    print(missing_predictions)\n    predictions = pd.concat((predictions, missing_predictions))\n    \n    # create a submission\n    predictions = predictions.set_index(\"customer_id\").prediction.to_dict()\n    preds = []\n    sub = sample_submission.copy()\n    for customer_id in customer_hex_id_to_int(sub.customer_id):\n        preds.append(\" \".join(f\"0{x}\" for x in predictions[customer_id]))\n    sub.prediction = preds\n    \n    # to csv\n    sub_name = 'testing_submission_with_bestsellers'\n    sub.to_csv(f'{sub_name}.csv.gz', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:53:23.983679Z","iopub.execute_input":"2023-12-18T10:53:23.984293Z","iopub.status.idle":"2023-12-18T10:57:04.077794Z","shell.execute_reply.started":"2023-12-18T10:53:23.984248Z","shell.execute_reply":"2023-12-18T10:57:04.076433Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"105\n=========================================\nThe content supposed to be in filler\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n=========================================\nThe content in filler\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n              t_dat           customer_id  article_id     price  \\\n0        2020-07-26        28847241659200   887770001  0.016932   \n1        2020-07-18        28847241659200   762846001  0.025407   \n2        2020-07-18        28847241659200   829308001  0.033881   \n3        2020-07-26        28847241659200   760084003  0.025180   \n4        2020-07-26        28847241659200   706016001  0.033148   \n...             ...                   ...         ...       ...   \n17982673 2020-09-21  18446737527580148316   918522001  0.041050   \n17982674 2020-09-21  18446737527580148316   923758001  0.033550   \n17982675 2020-09-21  18446737527580148316   910601002  0.040806   \n17982676 2020-09-21  18446737527580148316   896169002  0.049836   \n17982677 2020-09-21  18446737527580148316   929275001  0.058531   \n\n          sales_channel_id  week  age_group  purchased  repurchase_candidate  \\\n0                        1    96          1       True                 False   \n1                        1    96          1      False                  True   \n2                        1    96          1      False                  True   \n3                        1    96          1      False                 False   \n4                        1    96          1      False                 False   \n...                    ...   ...        ...        ...                   ...   \n17982673                 2   105          5      False                 False   \n17982674                 2   105          5      False                 False   \n17982675                 2   105          5      False                 False   \n17982676                 2   105          5      False                 False   \n17982677                 2   105          5      False                 False   \n\n          age_popularity_candidate  bestseller_rank  age_group_bestseller_rank  \n0                            False            999.0                      999.0  \n1                            False            999.0                      999.0  \n2                            False            999.0                      999.0  \n3                             True              1.0                        1.0  \n4                             True              4.0                        2.0  \n...                            ...              ...                        ...  \n17982673                      True              3.0                        7.0  \n17982674                      True              4.0                        8.0  \n17982675                      True            999.0                        9.0  \n17982676                      True            999.0                       10.0  \n17982677                      True            999.0                       11.0  \n\n[17982678 rows x 12 columns]\nFalse    9151784\nTrue     2221907\nName: purchased, dtype: int64\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.868237\n[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.166601\n[LightGBM] [Debug] init for col-wise cost 0.285746 seconds, init for row-wise cost 1.578234 seconds\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.926237 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Debug] Using Dense Multi-Val Bin\n[LightGBM] [Info] Total Bins 1103\n[LightGBM] [Info] Number of data points in the train set: 11373691, number of used features: 22\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\nage_group_bestseller_rank 0.8793894846557108\nrepurchase_candidate 0.1189775196509987\nproduct_type_no 0.0005532047992596363\ngarment_group_no 0.00035432567106007714\narticle_id 0.00025125329836431535\ndepartment_no 0.00017868172459221498\nsection_no 0.0001605829307429539\nage 5.5684544484041795e-05\nclub_member_status 3.550662038016785e-05\ncolour_group_code 2.5471497673729986e-05\nindex_group_no 1.8284606733392792e-05\nage_group 0.0\ngraphical_appearance_no 0.0\nperceived_colour_value_id 0.0\nperceived_colour_master_id 0.0\npostal_code 0.0\nindex_code 0.0\nbestseller_rank 0.0\nFN 0.0\nActive 0.0\nfashion_news_frequency 0.0\nage_popularity_candidate 0.0\n              t_dat           customer_id  article_id     price  \\\n11373691 2020-09-03        28847241659200   925246001  0.128797   \n11373692 2020-07-18        28847241659200   918522001  0.041459   \n11373693 2020-07-18        28847241659200   924243001  0.041515   \n11373694 2020-07-18        28847241659200   448509014  0.041836   \n11373695 2020-07-18        28847241659200   915529005  0.033490   \n...             ...                   ...         ...       ...   \n17982673 2020-09-21  18446737527580148316   918522001  0.041050   \n17982674 2020-09-21  18446737527580148316   923758001  0.033550   \n17982675 2020-09-21  18446737527580148316   910601002  0.040806   \n17982676 2020-09-21  18446737527580148316   896169002  0.049836   \n17982677 2020-09-21  18446737527580148316   929275001  0.058531   \n\n          sales_channel_id  week  age_group  purchased  repurchase_candidate  \\\n11373691                 2   105          1      False                  True   \n11373692                 1   105          1      False                 False   \n11373693                 1   105          1      False                 False   \n11373694                 1   105          1      False                 False   \n11373695                 1   105          1      False                 False   \n...                    ...   ...        ...        ...                   ...   \n17982673                 2   105          5      False                 False   \n17982674                 2   105          5      False                 False   \n17982675                 2   105          5      False                 False   \n17982676                 2   105          5      False                 False   \n17982677                 2   105          5      False                 False   \n\n          age_popularity_candidate  bestseller_rank  age_group_bestseller_rank  \n11373691                     False            999.0                      999.0  \n11373692                      True              3.0                        1.0  \n11373693                      True              1.0                        2.0  \n11373694                      True             10.0                        3.0  \n11373695                      True              9.0                        4.0  \n...                            ...              ...                        ...  \n17982673                      True              3.0                        7.0  \n17982674                      True              4.0                        8.0  \n17982675                      True            999.0                        9.0  \n17982676                      True            999.0                       10.0  \n17982677                      True            999.0                       11.0  \n\n[6608987 rows x 12 columns]\n                   customer_id  article_id     score\n11373691        28847241659200   925246001  0.034598\n11373692        28847241659200   918522001 -0.165273\n11373693        28847241659200   924243001 -0.165273\n11373694        28847241659200   448509014 -0.190245\n11373695        28847241659200   915529005 -0.191590\n...                        ...         ...       ...\n17982673  18446737527580148316   918522001 -0.191590\n17982674  18446737527580148316   923758001 -0.191590\n17982675  18446737527580148316   910601002 -0.191590\n17982676  18446737527580148316   896169002 -0.191590\n17982677  18446737527580148316   929275001 -0.191590\n\n[6608987 rows x 3 columns]\n                 customer_id  \\\n0             28847241659200   \n1             41318098387474   \n2            116809474287335   \n3            200292573348128   \n4            248294615847351   \n...                      ...   \n437360  18446624797007271432   \n437361  18446630855572834764   \n437362  18446662237889060501   \n437363  18446705133201055310   \n437364  18446737527580148316   \n\n                                               prediction  \n0       [925246001, 918522001, 924243001, 448509014, 9...  \n1       [868879003, 924243001, 930380001, 865799006, 7...  \n2       [906305002, 918522001, 924243001, 448509014, 9...  \n3       [903861001, 924243001, 866731001, 909370001, 9...  \n4       [720504008, 337991001, 471714002, 878987003, 9...  \n...                                                   ...  \n437360  [832482004, 805308003, 832482001, 556539027, 9...  \n437361  [568601045, 886966002, 898713001, 924243001, 8...  \n437362  [845790006, 915526002, 924243001, 930380001, 8...  \n437363  [875784002, 924243001, 924243002, 930380001, 9...  \n437364  [547780001, 763988001, 763988003, 547780040, 9...  \n\n[437365 rows x 2 columns]\n====================\n0          8458218447050899455\n1           542790048565690371\n2          1802629795750608900\n3          4887099159844225027\n4         16144079219589119995\n                  ...         \n934610    15944707497669951467\n934611     4086072098351480818\n934612    15284459503020408814\n934613     8719979846679134195\n934614    12233031500311822330\nName: customer_id, Length: 934615, dtype: uint64\n====================\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n====================\n                 customer_id  \\\n0        8458218447050899455   \n1         542790048565690371   \n2        1802629795750608900   \n3        4887099159844225027   \n4       16144079219589119995   \n...                      ...   \n934610  15944707497669951467   \n934611   4086072098351480818   \n934612  15284459503020408814   \n934613   8719979846679134195   \n934614  12233031500311822330   \n\n                                               prediction  \n0       [924243001, 924243002, 918522001, 923758001, 8...  \n1       [924243001, 924243002, 918522001, 923758001, 8...  \n2       [924243001, 924243002, 918522001, 923758001, 8...  \n3       [924243001, 924243002, 918522001, 923758001, 8...  \n4       [924243001, 924243002, 918522001, 923758001, 8...  \n...                                                   ...  \n934610  [924243001, 924243002, 918522001, 923758001, 8...  \n934611  [924243001, 924243002, 918522001, 923758001, 8...  \n934612  [924243001, 924243002, 918522001, 923758001, 8...  \n934613  [924243001, 924243002, 918522001, 923758001, 8...  \n934614  [924243001, 924243002, 918522001, 923758001, 8...  \n\n[934615 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculate predictions","metadata":{}},{"cell_type":"markdown","source":"# Create submission","metadata":{}}]}