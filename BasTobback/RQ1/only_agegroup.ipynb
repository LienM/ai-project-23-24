{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":93163345,"sourceType":"kernelVersion"}],"dockerImageVersionId":30178,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Radek posted about this [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/309220), and linked to a GitHub repo with the code.\n\nI just transferred that code here to Kaggle notebooks, that's all.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom lightgbm.sklearn import LGBMRanker\n\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n\n    This function computes the average prescision at k between two lists of\n    items.\n\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n\n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n\n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n\n    \"\"\"\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:21.780596Z","iopub.execute_input":"2023-12-17T16:02:21.780955Z","iopub.status.idle":"2023-12-17T16:02:24.456185Z","shell.execute_reply.started":"2023-12-17T16:02:21.780860Z","shell.execute_reply":"2023-12-17T16:02:24.454375Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n.datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAYAAADuMJi0AAAGR0lEQVR42rVZ21IbRxBtCbQrkIR2dQVjsLmDLBsET3nTQ8ouYRkQVf6e/E9+Im958qMfkgoXAaKSSj6C9Jnd2R2NeiRSRaZqitVOT5+Z6dNnWoKGlN94JFp8Ipofkb/7SOXjGyp8wF+z35K3f0uUp/GW4XfLQ8v2gefj3ZCCzojoNfue+43o1Q3l3xB/yA3JO7jnF2pCLnI+pNyx/qw7L+SQ7T2N9p2f8c60QcfcK6KGXsAd+ZvA4LlZYuSSAoOhMs5vwJkEGDlbPMaJoA+FcQ0IH38QLWkbAFLkOOhoMF5tU6/eBRhNjro0ZgKiPRAt3FLhCO/vqdgmNTm32LkmKpvBmQY4q5uAaAgbwDBG2BVv3bfI8KKAMWj2kfw9+pkZREIbEql4ST1x7hgHIANkbJ//MF8mAH/ilTCJ2tIi4ASr1IC3VNqXHKOxjy34mgoImnOQtx1g81fkqTiMOBVGcTogNhiT5iBHET8R8C+iApJUmgim3SQAXhsLQz7ee2G8gOAQNtJckBEplADiAxtX+G9NmhDl0qJKnTvyWlAMPYZnvIviGXRg6/Dh824DBXhP/tbfREXJEIvQ+aaPGjG7pvw6r3xdx+9hqb4dgZaP2XmdHO2K/B0c1+oUph6k8kShBryl/Ft0DYgjTlOieOACHFFpVyUl72T9V3cM1jUoYvxIC2vpCSys/ck70mDYuYvdvKjlMdKAUThneWVU1aAsyjv6PURDiwNsHGBZzY+JtAAgE2TFxdRHJdyIp/f+zqu09M5cDP2F08Ukkpj4YNSdX950HY2pNCCUK/Hhx5ZMBfjNSEzdsIihVzzAMdn9dz4eDYhnyQb9SSCiAryiJcQk82LiTbJ4x2FZJaUenpKnzP95WyDf4Y+QN9EFHHSeDLGdBjjKNQ5vKHf4XMA7KrY0y0GEObBOO/8e1ywuQExOHXktuQyJALEBpcEqhwtHqgiDuCK5b6i0p2MQpcckIIoh+6hYgTZtO8xlMi6O4tKCF/kOGHEg/W0UUpHW0ZoGNZ1ExZWcn7EErgwt4uj50E/sFBjXXIayWvh7WryjasxarZKssXon0zxvvkc32Q0bqbBCuZiKt9dWFysfQefeL29JYFaeztX6tePaZdz5mYx8+6Zq3Mk0wXECQxlhdzgS2wjBHju3j1RIgKyOMdNUE8X0+RAdbSapS11MRCv1SzUXmO6wGZe2SQYrv2MvCSWEv2VODE6DN7bz8ufypgQKW7uQskFTQHULLKyaEyrnlZbgOGLrV5qrn9U79jjm2HJmgkaVN98AfBub91lGPLZBqdroN5LYgjSu4zYZDDHXZOIPC691HqrWI1900I8qLzgKP4ft8DxEWigprPfrO+KcXno9gZz4jjGewWdUcpGCj0qVFuGPYbl2VturndZ2qRvlL8acDO6lF/DY/VjsFesiUK+ypJ+r/ep+cJkSQxEK4PG4WozgA75TYrDDqStE69K8/mzGEM+JXTeqvmedEElMmwCMm2SLd6bNNF9su02zEtoW6nAQtpMj5Gd7fKa//wqonF7UdtHFsVn+6hf1o7AfriPH7M6EeIUEF5zKVxXbYo7kS/OEtOqDYZKPoBsETIixn0uYrasThmzDkhdKPkz2EnaX0HdQbIgr59vAdGYDqjHrxkjS7WOxkTD8sqEqhiwcJETgBYigrBqF08KyDaje9SZ/I1A7MzaTzMGDEulPtZUkuKcyIRAjxEJPVrnVlb/9wkfij31D/pQt1IN+iL8bGJcstBIO7Y5VI/cwDqURbXhMuJxBqD0KLoK3esWFs0Jz5i5ZvJUAfFJMFb9XmGIOnzGpijpcWYCaMqXSQWp8EnCABepQ0Elyi4wfKfsw78ikIqif1pe1AGPlLmojl1SKxHHXp1L+Ut7AmDQHvhI5xHGi4EooO2BR7k78PEkJOdL7cAxQUZ/Tyclu9gnfwGgOmm2lNHGNmZXsq4Pqgc1EG1ATrvKl8s4R9ywwnqulGUnaRLVhxy8v3ieUwy2hbooT68uscW++DCDH0WSzuoyN2D4LUJ/tLECbcSKznwMIFs0ChF4mRTCnQbIIfk4SHJo6A9BMuTnXTs3Ku/KxsgZWqzuSe+Os8cEUfnMBY6UF5gi3SUbd5K7vDjq5WW0UENJlRsWn4sy21Er/E/AvPQSFHy1p4fgAAAAASUVORK5CYII=');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\n# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\ndef customer_hex_id_to_int(series):\n    return series.str[-16:].apply(hex_id_to_int)\n\ndef hex_id_to_int(str):\n    return int(str[-16:], 16)\n\ndef article_id_str_to_int(series):\n    return series.astype('int32')\n\ndef article_id_int_to_str(series):\n    return '0' + series.astype('str')\n\nclass Categorize(BaseEstimator, TransformerMixin):\n    def __init__(self, min_examples=0):\n        self.min_examples = min_examples\n        self.categories = []\n        \n    def fit(self, X):\n        for i in range(X.shape[1]):\n            vc = X.iloc[:, i].value_counts()\n            self.categories.append(vc[vc > self.min_examples].index.tolist())\n        return self\n\n    def transform(self, X):\n        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n        return pd.DataFrame(data=data)\n\n\ndef calculate_apk(list_of_preds, list_of_gts):\n    # for fast validation this can be changed to operate on dicts of {'cust_id_int': [art_id_int, ...]}\n    # using 'data/val_week_purchases_by_cust.pkl'\n    apks = []\n    for preds, gt in zip(list_of_preds, list_of_gts):\n        apks.append(apk(gt, preds, k=12))\n    return np.mean(apks)\n\ndef eval_sub(sub_csv, skip_cust_with_no_purchases=True):\n    sub=pd.read_csv(sub_csv)\n    validation_set=pd.read_parquet('data/validation_ground_truth.parquet')\n\n    apks = []\n\n    no_purchases_pattern = []\n    for pred, gt in zip(sub.prediction.str.split(), validation_set.prediction.str.split()):\n        if skip_cust_with_no_purchases and (gt == no_purchases_pattern): continue\n        apks.append(apk(gt, pred, k=12))\n    return np.mean(apks)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:24.459777Z","iopub.execute_input":"2023-12-17T16:02:24.460287Z","iopub.status.idle":"2023-12-17T16:02:24.514781Z","shell.execute_reply.started":"2023-12-17T16:02:24.460228Z","shell.execute_reply":"2023-12-17T16:02:24.513634Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:24.533279Z","iopub.execute_input":"2023-12-17T16:02:24.533714Z","iopub.status.idle":"2023-12-17T16:02:24.542772Z","shell.execute_reply.started":"2023-12-17T16:02:24.533663Z","shell.execute_reply":"2023-12-17T16:02:24.541244Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntransactions = pd.read_parquet('../input/warmup/transactions_train.parquet')\ncustomers = pd.read_parquet('../input/warmup/customers.parquet')\narticles = pd.read_parquet('../input/warmup/articles.parquet')\nsample_submission = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n# sample = 0.05\n# transactions = pd.read_parquet(f'data/transactions_train_sample_{sample}.parquet')\n# customers = pd.read_parquet(f'data/customers_sample_{sample}.parquet')\n# articles = pd.read_parquet(f'data/articles_train_sample_{sample}.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:24.545489Z","iopub.execute_input":"2023-12-17T16:02:24.546639Z","iopub.status.idle":"2023-12-17T16:02:37.275027Z","shell.execute_reply.started":"2023-12-17T16:02:24.546592Z","shell.execute_reply":"2023-12-17T16:02:37.274249Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CPU times: user 5.57 s, sys: 3.27 s, total: 8.85 s\nWall time: 12.7 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature engineering\nWe want to add some features or change some values, therefore we engineer some features","metadata":{}},{"cell_type":"code","source":"# define age groups\ndef get_age_group(age):\n    if age < 18:\n        return 0\n    elif age >= 18 and age < 25:\n        return 1\n    elif age >= 25 and age < 35:\n        return 2\n    elif age >= 35 and age < 45:\n        return 3\n    elif age >= 45 and age < 55:\n        return 4\n    elif age >= 55 and age < 65:\n        return 5\n    else:\n        return 6","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:37.276342Z","iopub.execute_input":"2023-12-17T16:02:37.276888Z","iopub.status.idle":"2023-12-17T16:02:37.284132Z","shell.execute_reply.started":"2023-12-17T16:02:37.276848Z","shell.execute_reply":"2023-12-17T16:02:37.283158Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:37.286281Z","iopub.execute_input":"2023-12-17T16:02:37.287080Z","iopub.status.idle":"2023-12-17T16:02:37.324761Z","shell.execute_reply.started":"2023-12-17T16:02:37.287013Z","shell.execute_reply":"2023-12-17T16:02:37.323838Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"              t_dat           customer_id  article_id     price  \\\n25784    2018-09-20      1728846800780188   519773001  0.028458   \n25785    2018-09-20      1728846800780188   578472001  0.032525   \n5389     2018-09-20      2076973761519164   661795002  0.167797   \n5390     2018-09-20      2076973761519164   684080003  0.101678   \n47429    2018-09-20      2918879973994241   662980001  0.033881   \n...             ...                   ...         ...       ...   \n31774722 2020-09-22  18439937050817258297   891591003  0.084729   \n31774723 2020-09-22  18439937050817258297   869706005  0.084729   \n31779097 2020-09-22  18440902715633436014   918894002  0.016932   \n31779098 2020-09-22  18440902715633436014   761269001  0.016932   \n31780475 2020-09-22  18443633011701112574   914868002  0.033881   \n\n          sales_channel_id  week  \n25784                    2     0  \n25785                    2     0  \n5389                     2     0  \n5390                     2     0  \n47429                    1     0  \n...                    ...   ...  \n31774722                 2   104  \n31774723                 2   104  \n31779097                 1   104  \n31779098                 1   104  \n31780475                 1   104  \n\n[31788324 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25784</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>519773001</td>\n      <td>0.028458</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25785</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>578472001</td>\n      <td>0.032525</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5389</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>661795002</td>\n      <td>0.167797</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>684080003</td>\n      <td>0.101678</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47429</th>\n      <td>2018-09-20</td>\n      <td>2918879973994241</td>\n      <td>662980001</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31774722</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>891591003</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31774723</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>869706005</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779097</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>918894002</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779098</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>761269001</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31780475</th>\n      <td>2020-09-22</td>\n      <td>18443633011701112574</td>\n      <td>914868002</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n  </tbody>\n</table>\n<p>31788324 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Making a recall evaluation function","metadata":{}},{"cell_type":"code","source":"# return the average recall of generated candidates versus the actual bought items\ndef average_recall(purchases, candidates):\n    joined = pd.merge(purchases, candidates, how='inner').drop_duplicates()\n    true_positives = joined.groupby('customer_id').count()\n    total_positives = purchases.groupby('customer_id').count()\n    recall = true_positives.divide(total_positives, fill_value=0)\n    return recall.mean().values[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:37.326548Z","iopub.execute_input":"2023-12-17T16:02:37.326846Z","iopub.status.idle":"2023-12-17T16:02:37.333349Z","shell.execute_reply.started":"2023-12-17T16:02:37.326808Z","shell.execute_reply":"2023-12-17T16:02:37.332423Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Generating candidates\nTODO: ensure the bestsellers of last week can be save to add to empty users","metadata":{}},{"cell_type":"code","source":"def candidate_generation(data, test_week):\n    ################\n    ## Repurchase ##\n    ################\n    c2weeks = transactions.groupby('customer_id')['week'].unique()\n    c2weeks2shifted_weeks = {}\n    for c_id, weeks in c2weeks.items():\n        c2weeks2shifted_weeks[c_id] = {}\n        for i in range(weeks.shape[0]-1):\n            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n    candidates_last_purchase = transactions.copy()\n    weeks = []\n    for i, (c_id, week) in enumerate(zip(transactions['customer_id'], transactions['week'])):\n        weeks.append(c2weeks2shifted_weeks[c_id][week])\n    candidates_last_purchase.week=weeks\n    \n    ################\n    ## bestseller ##\n    ################\n    mean_price = transactions \\\n        .groupby(['week', 'article_id'])['price'].mean()\n    sales = transactions \\\n        .groupby('week')['article_id'].value_counts() \\\n        .groupby('week').rank(method='dense', ascending=False) \\\n        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n    bestsellers_previous_week.week += 1\n    \n    filler = bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()].article_id.value_counts().head(12).index.values\n    print(\"=========================================\")\n    print(\"The content supposed to be in filler\")\n    print(bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()].article_id.value_counts().head(12).index.values)\n    print(\"=========================================\")\n    print(\"The content in filler\")\n    print(filler)\n\n    unique_transactions = transactions \\\n        .groupby(['week', 'customer_id']) \\\n        .head(1) \\\n        .drop(columns=['article_id', 'price']) \\\n        .copy()\n    candidates_bestsellers = pd.merge(\n        unique_transactions,\n        bestsellers_previous_week,\n        on='week',\n    )\n    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n    test_set_transactions.week = test_week\n    candidates_bestsellers_test_week = pd.merge(\n        test_set_transactions,\n        bestsellers_previous_week,\n        on='week'\n    )\n    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n    \n    if not test_week > absolute_max_week:\n        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        t_candidates = candidates_bestsellers[candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        print(f\"Average recall of bestsellers : {average_recall(t_purchases, t_candidates)}\")\n    \n    ###################################\n    ## Bestseller based on age group ##\n    ###################################\n    # Group the mean_price not per week/article but by week/article/age_group\n    # this is so we know\n    mean_price_age_group = transactions \\\n        .groupby(['week', 'age_group', 'article_id'])['price'].mean()\n\n    # group the sales by week AND the age group and so find the most popular article for each age group in each week\n    sales_age_group = transactions \\\n        .groupby(['week', 'age_group'])['article_id'].value_counts() \\\n        .groupby(['week', 'age_group']).rank(method='dense', ascending=False) \\\n        .groupby(['week', 'age_group']).head(12).rename('age_group_bestseller_rank').astype('int8')\n\n    # now calculate the bestsellers for these week - age_group combos\n    bestsellers_previous_week_age_group = pd.merge(sales_age_group, mean_price_age_group, on=['week', 'age_group', 'article_id']).reset_index()\n    bestsellers_previous_week_age_group.week += 1\n\n    unique_age_group_transactions = transactions \\\n        .groupby(['week', 'customer_id']) \\\n        .head(1) \\\n        .drop(columns=['article_id', 'price']) \\\n        .copy()\n\n    age_group_candidates_bestsellers = pd.merge(\n        unique_age_group_transactions,\n        bestsellers_previous_week_age_group,\n        on=['week', 'age_group'],\n    )\n    test_set_age_group_transactions = unique_age_group_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n    test_set_age_group_transactions.week = test_week\n\n    age_group_candidates_bestsellers_test_week = pd.merge(\n        test_set_age_group_transactions,\n        bestsellers_previous_week_age_group,\n        on=['week', 'age_group'],\n    )\n    age_group_candidates_bestsellers = pd.concat([age_group_candidates_bestsellers, age_group_candidates_bestsellers_test_week])\n    age_group_candidates_bestsellers.drop(columns='age_group_bestseller_rank', inplace=True)\n    \n    if not test_week > absolute_max_week:\n        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        t_candidates = age_group_candidates_bestsellers[age_group_candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n        print(f\"Average recall of age group bestsellers : {average_recall(t_purchases, t_candidates)}\")\n    \n    \n    ###################################################\n    # Combine the transactions and negative examples ##\n    ###################################################\n    purchased_transactions = data.copy()\n    transactions['purchased'] = 1\n    result = pd.concat([\n        data, candidates_last_purchase, age_group_candidates_bestsellers\n    ])\n    result.purchased.fillna(0, inplace=True)\n    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n    result = pd.merge(\n        result,\n        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n        on=['week', 'article_id'],\n        how='left'\n    )\n    # merge the data with the bestsellers information from the age_group popularity study\n    result = pd.merge(\n        result,\n        bestsellers_previous_week_age_group[['week', 'age_group', 'article_id', 'age_group_bestseller_rank']],\n        on=['week', 'age_group', 'article_id'],\n        how='left'\n    )\n    result = result[result.week != result.week.min()]\n    result.bestseller_rank.fillna(999, inplace=True)\n    result.age_group_bestseller_rank.fillna(999, inplace=True)\n    \n    result.sort_values(['week', 'customer_id'], inplace=True)\n    result.reset_index(drop=True, inplace=True)\n    return result, filler","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:37.334711Z","iopub.execute_input":"2023-12-17T16:02:37.335343Z","iopub.status.idle":"2023-12-17T16:02:37.375390Z","shell.execute_reply.started":"2023-12-17T16:02:37.335292Z","shell.execute_reply":"2023-12-17T16:02:37.374571Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def add_features(data):\n    columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n    'perceived_colour_master_id', 'department_no', 'index_code',\n    'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n    'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', 'bestseller_rank', 'age_group_bestseller_rank', 'age_group']\n    \n    result = data\n    result = pd.merge(result, customers, how='left', on=['customer_id', 'age_group'])\n    result = pd.merge(result, articles, how='left', on='article_id')\n    \n    # features from assignment 2 can go here\n    \n    return result[columns_to_use]\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:37.377735Z","iopub.execute_input":"2023-12-17T16:02:37.378470Z","iopub.status.idle":"2023-12-17T16:02:37.392130Z","shell.execute_reply.started":"2023-12-17T16:02:37.378429Z","shell.execute_reply":"2023-12-17T16:02:37.391127Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# use the generation for training and testing","metadata":{}},{"cell_type":"code","source":"# define the test week and limit the data to a set of previous weeks\ntest_week = 105\nnum_training_weeks = 10\nabsolute_max_week = transactions.week.max()\nprint(test_week)\ntest_week_transactions = transactions[transactions.week == test_week]\ntransactions = transactions[(transactions.week > test_week - num_training_weeks - 1) & (transactions.week < test_week)].reset_index(drop=True)\n\ncustomers[\"age_group\"] = customers[\"age\"].apply(get_age_group)\n# firstly take the age_groups and the cutomer ids\nage_groups_customers = customers[['customer_id', 'age_group']].drop_duplicates()\n\n# now join them into the transactions to create a new transactions set to work with\ntransactions = pd.merge(transactions, age_groups_customers)\n# now the age_group is included, we will have to change some values and names to ensure this is used\n\n# assemble training data by using positive and negative samples\nexamples, filler = candidate_generation(transactions, test_week)\nprint(examples)\ntrain_examples = examples[examples.week != test_week]\ntrain_x = add_features(train_examples)\ntrain_y = train_examples['purchased']\n\n# make the ranker, make the train_groups\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    n_estimators=1,\n    importance_type='gain',\n    verbose=10\n)\n# sort the training_examples\ntrain_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\nranker.fit(train_x, train_y, group=train_groups)\nfor i in ranker.feature_importances_.argsort()[::-1]:\n    print(train_x.columns[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())\n    \n\n# testing\ntest_examples_not_copy = examples[examples.week == test_week]\nprint(test_examples_not_copy)\ntest_examples = test_examples_not_copy.copy()\ntest_x = add_features(test_examples)\n\ntest_examples[\"score\"] = ranker.predict(test_x)\n\ntest_examples = test_examples[['customer_id', 'article_id', 'score']]\nprint(test_examples)\npredictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n    .groupby(\"customer_id\")\\\n    .head(12)\\\n    .groupby(\"customer_id\")\\\n    .article_id.apply(list).reset_index()\\\n    .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\nprint(predictions)\n\n# make the predictions of the customers\n# predictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n#     .groupby(\"customer_id\")\\\n#     .head(12)\\\n#     .groupby(\"customer_id\", as_index = False)\\\n#     .article_id.apply(list)\\\n#     .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n\n# scored_candidates.sort_values([\"customer_id\", \"score\"], ascending=False)\n#         .groupby(\"customer_id\")\n#         .head(k)\n#         .groupby(\"customer_id\", as_index=False)\n#         .article_id.apply(list)\n#         .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n\n\n### evaluate\n# if the test week is a week of which the data is in fact known\nif test_week < absolute_max_week:\n    print(\"In a previous week right now\")\n    pass\n\n# if the week is our target week\nelse:\n    missing_customers = pd.Series(\n        list(set(age_groups_customers.customer_id) - set(predictions.customer_id)),\n        name=\"customer_id\",\n    )\n    missing_predictions = pd.merge(\n        missing_customers, pd.Series([filler], name=\"prediction\"), how=\"cross\"\n    )\n    print(\"====================\")\n    print(missing_customers)\n    print(\"====================\")\n    print(filler)\n    print(\"====================\")\n    print(missing_predictions)\n    predictions = pd.concat((predictions, missing_predictions))\n    \n    # create a submission\n    predictions = predictions.set_index(\"customer_id\").prediction.to_dict()\n    preds = []\n    sub = sample_submission.copy()\n    for customer_id in customer_hex_id_to_int(sub.customer_id):\n        preds.append(\" \".join(f\"0{x}\" for x in predictions[customer_id]))\n    sub.prediction = preds\n    \n    # to csv\n    sub_name = 'testing_submission'\n    sub.to_csv(f'{sub_name}.csv.gz', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T16:02:37.394095Z","iopub.execute_input":"2023-12-17T16:02:37.394881Z","iopub.status.idle":"2023-12-17T16:05:41.369101Z","shell.execute_reply.started":"2023-12-17T16:02:37.394808Z","shell.execute_reply":"2023-12-17T16:05:41.368234Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"105\n=========================================\nThe content supposed to be in filler\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n=========================================\nThe content in filler\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n              t_dat           customer_id  article_id     price  \\\n0        2020-07-26        28847241659200   887770001  0.016932   \n1        2020-07-18        28847241659200   762846001  0.025407   \n2        2020-07-18        28847241659200   829308001  0.033881   \n3        2020-07-26        28847241659200   760084003  0.025180   \n4        2020-07-26        28847241659200   706016001  0.033148   \n...             ...                   ...         ...       ...   \n17982673 2020-09-21  18446737527580148316   918522001  0.041050   \n17982674 2020-09-21  18446737527580148316   923758001  0.033550   \n17982675 2020-09-21  18446737527580148316   910601002  0.040806   \n17982676 2020-09-21  18446737527580148316   896169002  0.049836   \n17982677 2020-09-21  18446737527580148316   929275001  0.058531   \n\n          sales_channel_id  week  age_group  purchased  bestseller_rank  \\\n0                        1    96          1        1.0            999.0   \n1                        1    96          1        0.0            999.0   \n2                        1    96          1        0.0            999.0   \n3                        1    96          1        0.0              1.0   \n4                        1    96          1        0.0              4.0   \n...                    ...   ...        ...        ...              ...   \n17982673                 2   105          5        0.0              3.0   \n17982674                 2   105          5        0.0              4.0   \n17982675                 2   105          5        0.0            999.0   \n17982676                 2   105          5        0.0            999.0   \n17982677                 2   105          5        0.0            999.0   \n\n          age_group_bestseller_rank  \n0                             999.0  \n1                             999.0  \n2                             999.0  \n3                               1.0  \n4                               2.0  \n...                             ...  \n17982673                        7.0  \n17982674                        8.0  \n17982675                        9.0  \n17982676                       10.0  \n17982677                       11.0  \n\n[17982678 rows x 10 columns]\n[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.102383\n[LightGBM] [Debug] init for col-wise cost 0.002507 seconds, init for row-wise cost 1.846995 seconds\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.637628 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Debug] Using Dense Multi-Val Bin\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 11373691, number of used features: 20\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\nage_group_bestseller_rank 0.9989235255453759\nage 0.00023177743573232561\ngarment_group_no 0.00022266642713777244\narticle_id 0.00022051945021253223\nproduct_type_no 0.00013215411057960482\npostal_code 6.768542774227429e-05\nclub_member_status 6.475560524098679e-05\ncolour_group_code 5.415297999395995e-05\ndepartment_no 3.761268413280684e-05\nsection_no 2.520426467904366e-05\ngraphical_appearance_no 1.994606917284504e-05\nFN 0.0\nindex_group_no 0.0\nindex_code 0.0\nActive 0.0\nperceived_colour_master_id 0.0\nperceived_colour_value_id 0.0\nfashion_news_frequency 0.0\nbestseller_rank 0.0\nage_group 0.0\n              t_dat           customer_id  article_id     price  \\\n11373691 2020-09-03        28847241659200   925246001  0.128797   \n11373692 2020-07-18        28847241659200   918522001  0.041459   \n11373693 2020-07-18        28847241659200   924243001  0.041515   \n11373694 2020-07-18        28847241659200   448509014  0.041836   \n11373695 2020-07-18        28847241659200   915529005  0.033490   \n...             ...                   ...         ...       ...   \n17982673 2020-09-21  18446737527580148316   918522001  0.041050   \n17982674 2020-09-21  18446737527580148316   923758001  0.033550   \n17982675 2020-09-21  18446737527580148316   910601002  0.040806   \n17982676 2020-09-21  18446737527580148316   896169002  0.049836   \n17982677 2020-09-21  18446737527580148316   929275001  0.058531   \n\n          sales_channel_id  week  age_group  purchased  bestseller_rank  \\\n11373691                 2   105          1        0.0            999.0   \n11373692                 1   105          1        0.0              3.0   \n11373693                 1   105          1        0.0              1.0   \n11373694                 1   105          1        0.0             10.0   \n11373695                 1   105          1        0.0              9.0   \n...                    ...   ...        ...        ...              ...   \n17982673                 2   105          5        0.0              3.0   \n17982674                 2   105          5        0.0              4.0   \n17982675                 2   105          5        0.0            999.0   \n17982676                 2   105          5        0.0            999.0   \n17982677                 2   105          5        0.0            999.0   \n\n          age_group_bestseller_rank  \n11373691                      999.0  \n11373692                        1.0  \n11373693                        2.0  \n11373694                        3.0  \n11373695                        4.0  \n...                             ...  \n17982673                        7.0  \n17982674                        8.0  \n17982675                        9.0  \n17982676                       10.0  \n17982677                       11.0  \n\n[6608987 rows x 10 columns]\n                   customer_id  article_id     score\n11373691        28847241659200   925246001  0.177487\n11373692        28847241659200   918522001 -0.164432\n11373693        28847241659200   924243001 -0.164432\n11373694        28847241659200   448509014 -0.191404\n11373695        28847241659200   915529005 -0.188362\n...                        ...         ...       ...\n17982673  18446737527580148316   918522001 -0.188362\n17982674  18446737527580148316   923758001 -0.192367\n17982675  18446737527580148316   910601002 -0.192367\n17982676  18446737527580148316   896169002 -0.188362\n17982677  18446737527580148316   929275001 -0.192367\n\n[6608987 rows x 3 columns]\n                 customer_id  \\\n0             28847241659200   \n1             41318098387474   \n2            116809474287335   \n3            200292573348128   \n4            248294615847351   \n...                      ...   \n437360  18446624797007271432   \n437361  18446630855572834764   \n437362  18446662237889060501   \n437363  18446705133201055310   \n437364  18446737527580148316   \n\n                                               prediction  \n0       [925246001, 918522001, 924243001, 915529005, 9...  \n1       [868879003, 924243001, 751471043, 918522001, 9...  \n2       [906305002, 918522001, 924243001, 915529005, 9...  \n3       [903861001, 909370001, 924243001, 924243002, 8...  \n4       [720504008, 337991001, 878987003, 471714002, 9...  \n...                                                   ...  \n437360  [805308003, 832482001, 556539027, 832482004, 9...  \n437361  [898713001, 568601045, 886966002, 909370001, 9...  \n437362  [915526002, 845790006, 924243001, 751471043, 9...  \n437363  [875784002, 924243001, 924243002, 918522001, 8...  \n437364  [547780001, 763988001, 763988003, 547780040, 9...  \n\n[437365 rows x 2 columns]\n====================\n0          8458218447050899455\n1           542790048565690371\n2          1802629795750608900\n3          4887099159844225027\n4         16144079219589119995\n                  ...         \n934610    15944707497669951467\n934611     4086072098351480818\n934612    15284459503020408814\n934613     8719979846679134195\n934614    12233031500311822330\nName: customer_id, Length: 934615, dtype: uint64\n====================\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n====================\n                 customer_id  \\\n0        8458218447050899455   \n1         542790048565690371   \n2        1802629795750608900   \n3        4887099159844225027   \n4       16144079219589119995   \n...                      ...   \n934610  15944707497669951467   \n934611   4086072098351480818   \n934612  15284459503020408814   \n934613   8719979846679134195   \n934614  12233031500311822330   \n\n                                               prediction  \n0       [924243001, 924243002, 918522001, 923758001, 8...  \n1       [924243001, 924243002, 918522001, 923758001, 8...  \n2       [924243001, 924243002, 918522001, 923758001, 8...  \n3       [924243001, 924243002, 918522001, 923758001, 8...  \n4       [924243001, 924243002, 918522001, 923758001, 8...  \n...                                                   ...  \n934610  [924243001, 924243002, 918522001, 923758001, 8...  \n934611  [924243001, 924243002, 918522001, 923758001, 8...  \n934612  [924243001, 924243002, 918522001, 923758001, 8...  \n934613  [924243001, 924243002, 918522001, 923758001, 8...  \n934614  [924243001, 924243002, 918522001, 923758001, 8...  \n\n[934615 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculate predictions","metadata":{}},{"cell_type":"markdown","source":"# Create submission","metadata":{}}]}