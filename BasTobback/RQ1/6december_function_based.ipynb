{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 31254,
     "databundleVersionId": 3103714,
     "sourceType": "competition"
    },
    {
     "sourceId": 93163345,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30178,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Radek posted about this [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/309220), and linked to a GitHub repo with the code.\n",
    "\n",
    "I just transferred that code here to Kaggle notebooks, that's all."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:43.477377Z",
     "iopub.execute_input": "2023-12-05T22:42:43.477786Z",
     "iopub.status.idle": "2023-12-05T22:42:43.490419Z",
     "shell.execute_reply.started": "2023-12-05T22:42:43.477744Z",
     "shell.execute_reply": "2023-12-05T22:42:43.489621Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\n",
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)\n",
    "\n",
    "def article_id_str_to_int(series):\n",
    "    return series.astype('int32')\n",
    "\n",
    "def article_id_int_to_str(series):\n",
    "    return '0' + series.astype('str')\n",
    "\n",
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "def calculate_apk(list_of_preds, list_of_gts):\n",
    "    # for fast validation this can be changed to operate on dicts of {'cust_id_int': [art_id_int, ...]}\n",
    "    # using 'data/val_week_purchases_by_cust.pkl'\n",
    "    apks = []\n",
    "    for preds, gt in zip(list_of_preds, list_of_gts):\n",
    "        apks.append(apk(gt, preds, k=12))\n",
    "    return np.mean(apks)\n",
    "\n",
    "def eval_sub(sub_csv, skip_cust_with_no_purchases=True):\n",
    "    sub=pd.read_csv(sub_csv)\n",
    "    validation_set=pd.read_parquet('data/validation_ground_truth.parquet')\n",
    "\n",
    "    apks = []\n",
    "\n",
    "    no_purchases_pattern = []\n",
    "    for pred, gt in zip(sub.prediction.str.split(), validation_set.prediction.str.split()):\n",
    "        if skip_cust_with_no_purchases and (gt == no_purchases_pattern): continue\n",
    "        apks.append(apk(gt, pred, k=12))\n",
    "    return np.mean(apks)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:43.495811Z",
     "iopub.execute_input": "2023-12-05T22:42:43.496659Z",
     "iopub.status.idle": "2023-12-05T22:42:43.688144Z",
     "shell.execute_reply.started": "2023-12-05T22:42:43.496605Z",
     "shell.execute_reply": "2023-12-05T22:42:43.686594Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:43.689682Z",
     "iopub.execute_input": "2023-12-05T22:42:43.690061Z",
     "iopub.status.idle": "2023-12-05T22:42:43.706336Z",
     "shell.execute_reply.started": "2023-12-05T22:42:43.690022Z",
     "shell.execute_reply": "2023-12-05T22:42:43.704927Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "transactions = pd.read_parquet('../input/warmup/transactions_train.parquet')\n",
    "customers = pd.read_parquet('../input/warmup/customers.parquet')\n",
    "articles = pd.read_parquet('../input/warmup/articles.parquet')\n",
    "sample_submission = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "# sample = 0.05\n",
    "# transactions = pd.read_parquet(f'data/transactions_train_sample_{sample}.parquet')\n",
    "# customers = pd.read_parquet(f'data/customers_sample_{sample}.parquet')\n",
    "# articles = pd.read_parquet(f'data/articles_train_sample_{sample}.parquet')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:43.708625Z",
     "iopub.execute_input": "2023-12-05T22:42:43.708948Z",
     "iopub.status.idle": "2023-12-05T22:42:52.882024Z",
     "shell.execute_reply.started": "2023-12-05T22:42:43.708909Z",
     "shell.execute_reply": "2023-12-05T22:42:52.880702Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 6.19 s, sys: 3.71 s, total: 9.9 s\nWall time: 9.16 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering\n",
    "We want to add some features or change some values, therefore we engineer some features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# define age groups\n",
    "def get_age_group(age):\n",
    "    if age < 18:\n",
    "        return 0\n",
    "    elif age >= 18 and age < 25:\n",
    "        return 1\n",
    "    elif age >= 25 and age < 35:\n",
    "        return 2\n",
    "    elif age >= 35 and age < 45:\n",
    "        return 3\n",
    "    elif age >= 45 and age < 55:\n",
    "        return 4\n",
    "    elif age >= 55 and age < 65:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:52.883784Z",
     "iopub.execute_input": "2023-12-05T22:42:52.884182Z",
     "iopub.status.idle": "2023-12-05T22:42:52.892329Z",
     "shell.execute_reply.started": "2023-12-05T22:42:52.884131Z",
     "shell.execute_reply": "2023-12-05T22:42:52.891242Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transactions"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:52.894221Z",
     "iopub.execute_input": "2023-12-05T22:42:52.894578Z",
     "iopub.status.idle": "2023-12-05T22:42:52.924345Z",
     "shell.execute_reply.started": "2023-12-05T22:42:52.894532Z",
     "shell.execute_reply": "2023-12-05T22:42:52.923381Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "              t_dat           customer_id  article_id     price  \\\n25784    2018-09-20      1728846800780188   519773001  0.028458   \n25785    2018-09-20      1728846800780188   578472001  0.032525   \n5389     2018-09-20      2076973761519164   661795002  0.167797   \n5390     2018-09-20      2076973761519164   684080003  0.101678   \n47429    2018-09-20      2918879973994241   662980001  0.033881   \n...             ...                   ...         ...       ...   \n31774722 2020-09-22  18439937050817258297   891591003  0.084729   \n31774723 2020-09-22  18439937050817258297   869706005  0.084729   \n31779097 2020-09-22  18440902715633436014   918894002  0.016932   \n31779098 2020-09-22  18440902715633436014   761269001  0.016932   \n31780475 2020-09-22  18443633011701112574   914868002  0.033881   \n\n          sales_channel_id  week  \n25784                    2     0  \n25785                    2     0  \n5389                     2     0  \n5390                     2     0  \n47429                    1     0  \n...                    ...   ...  \n31774722                 2   104  \n31774723                 2   104  \n31779097                 1   104  \n31779098                 1   104  \n31780475                 1   104  \n\n[31788324 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25784</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>519773001</td>\n      <td>0.028458</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25785</th>\n      <td>2018-09-20</td>\n      <td>1728846800780188</td>\n      <td>578472001</td>\n      <td>0.032525</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5389</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>661795002</td>\n      <td>0.167797</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>2018-09-20</td>\n      <td>2076973761519164</td>\n      <td>684080003</td>\n      <td>0.101678</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47429</th>\n      <td>2018-09-20</td>\n      <td>2918879973994241</td>\n      <td>662980001</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31774722</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>891591003</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31774723</th>\n      <td>2020-09-22</td>\n      <td>18439937050817258297</td>\n      <td>869706005</td>\n      <td>0.084729</td>\n      <td>2</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779097</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>918894002</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31779098</th>\n      <td>2020-09-22</td>\n      <td>18440902715633436014</td>\n      <td>761269001</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>31780475</th>\n      <td>2020-09-22</td>\n      <td>18443633011701112574</td>\n      <td>914868002</td>\n      <td>0.033881</td>\n      <td>1</td>\n      <td>104</td>\n    </tr>\n  </tbody>\n</table>\n<p>31788324 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making a recall evaluation function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# return the average recall of generated candidates versus the actual bought items\n",
    "def average_recall(purchases, candidates):\n",
    "    joined = pd.merge(purchases, candidates, how='inner').drop_duplicates()\n",
    "    true_positives = joined.groupby('customer_id').count()\n",
    "    total_positives = purchases.groupby('customer_id').count()\n",
    "    recall = true_positives.divide(total_positives, fill_value=0)\n",
    "    return recall.mean().values[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:52.925788Z",
     "iopub.execute_input": "2023-12-05T22:42:52.926472Z",
     "iopub.status.idle": "2023-12-05T22:42:52.933627Z",
     "shell.execute_reply.started": "2023-12-05T22:42:52.926430Z",
     "shell.execute_reply": "2023-12-05T22:42:52.932193Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating candidates\n",
    "TODO: ensure the bestsellers of last week can be save to add to empty users"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def candidate_generation(data, test_week):\n",
    "    \"\"\"\n",
    "    Return the candidates for test_week based on data given. The candidates that are generated are the repurchase candidates, the bestsellers and the bestsellers based on an age group.\n",
    "    :param data: a pandas dataframe with the transactions\n",
    "    :param test_week: a value that indicates the week that is supposed to be taken, advised is using either 104 as testing or 105 as final target week\n",
    "    :return: pandas dataframe containing the candidates for all customers available in data\n",
    "    \"\"\"\n",
    "    ################\n",
    "    ## Repurchase ##\n",
    "    ################\n",
    "    \"\"\"\n",
    "    The repurchases as they were generated in the Radek baseline. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    c2weeks = transactions.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = transactions.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(transactions['customer_id'], transactions['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "    \n",
    "    ################\n",
    "    ## bestseller ##\n",
    "    ################\n",
    "    \"\"\"\n",
    "    The bestsellers as they were generated in the Radek baseline. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    mean_price = transactions \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = transactions \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    This part is making sure I have a filler. The filler is used a a list of predictions used to fill the list of customers that were not considered in the candidate generation process.\n",
    "    This filler, as is seen in one of the last paragraphs of previous steps where no function replacement has taken place, is made out of the bestsellers.\n",
    "    \"\"\"\n",
    "    filler = bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()].article_id.value_counts().head(12).index.values\n",
    "    print(\"=========================================\")\n",
    "    print(\"The content supposed to be in filler\")\n",
    "    print(bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()].article_id.value_counts().head(12).index.values)\n",
    "    print(\"=========================================\")\n",
    "    print(\"The content in filler\")\n",
    "    print(filler)\n",
    "    \"\"\"\n",
    "    Some checkup printings to make sure the exact contents are correct are done. these can be found in the output.\n",
    "    this is one small correction in response to the realization that I only recommended for the users present in the used data and did not consider all users in the entire dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    unique_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "    \n",
    "    # testing the recall of the bestseller candidates which are generated\n",
    "    # done by taking the subsets of the actual transactions and the generated candidates ONLY WHEN the test week is not bigger than the absolute max_week which is the highest week available in the dataset\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        t_candidates = candidates_bestsellers[candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        print(f\"Average recall of bestsellers : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    ###################################\n",
    "    ## Bestseller based on age group ##\n",
    "    ###################################\n",
    "    \"\"\"\n",
    "    The bestsellers as they were generated in the previous file. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    # Group the mean_price not per week/article but by week/article/age_group\n",
    "    # this is so we know\n",
    "    mean_price_age_group = transactions \\\n",
    "        .groupby(['week', 'age_group', 'article_id'])['price'].mean()\n",
    "\n",
    "    # group the sales by week AND the age group and so find the most popular article for each age group in each week\n",
    "    sales_age_group = transactions \\\n",
    "        .groupby(['week', 'age_group'])['article_id'].value_counts() \\\n",
    "        .groupby(['week', 'age_group']).rank(method='dense', ascending=False) \\\n",
    "        .groupby(['week', 'age_group']).head(12).rename('age_group_bestseller_rank').astype('int8')\n",
    "\n",
    "    # now calculate the bestsellers for these week - age_group combos\n",
    "    bestsellers_previous_week_age_group = pd.merge(sales_age_group, mean_price_age_group, on=['week', 'age_group', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week_age_group.week += 1\n",
    "\n",
    "    unique_age_group_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "\n",
    "    age_group_candidates_bestsellers = pd.merge(\n",
    "        unique_age_group_transactions,\n",
    "        bestsellers_previous_week_age_group,\n",
    "        on=['week', 'age_group'],\n",
    "    )\n",
    "    test_set_age_group_transactions = unique_age_group_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_age_group_transactions.week = test_week\n",
    "\n",
    "    age_group_candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_age_group_transactions,\n",
    "        bestsellers_previous_week_age_group,\n",
    "        on=['week', 'age_group'],\n",
    "    )\n",
    "    age_group_candidates_bestsellers = pd.concat([age_group_candidates_bestsellers, age_group_candidates_bestsellers_test_week])\n",
    "    age_group_candidates_bestsellers.drop(columns='age_group_bestseller_rank', inplace=True)\n",
    "    \n",
    "    # testing the recall of the age group bestseller candidates which are generated\n",
    "    # done by taking the subsets of the actual transactions and the generated candidates ONLY WHEN the test week is not bigger than the absolute max_week which is the highest week available in the dataset\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        t_candidates = age_group_candidates_bestsellers[age_group_candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        print(f\"Average recall of age group bestsellers : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    \n",
    "    ###################################################\n",
    "    # Combine the transactions and negative examples ##\n",
    "    ###################################################\n",
    "    \"\"\"\n",
    "    Here the code which I have written in one of the previous steps is omitted. This is the part of the sin values of age etc. They did not seem pertinent to the objective of this step of my research. The rest is still the same as the code written in the baseline file with age groups. No further changes are made. The initial assumption was that the results would be similar, but they aren't.\n",
    "    NOTE: After doing this I realized some mistakes in usage of variables, the variable just below is not used.\n",
    "    \"\"\"\n",
    "    purchased_transactions = data.copy()\n",
    "    transactions['purchased'] = 1\n",
    "    result = pd.concat([\n",
    "        \n",
    "        # here the age group bestsellers are concatenated after normal bestsellers\n",
    "        data, candidates_last_purchase, candidates_bestsellers, age_group_candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(0, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    # merge the data with the bestsellers information from the age_group popularity study\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week_age_group[['week', 'age_group', 'article_id', 'age_group_bestseller_rank']],\n",
    "        on=['week', 'age_group', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "    result.age_group_bestseller_rank.fillna(999, inplace=True)\n",
    "    \n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result, filler"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:52.936169Z",
     "iopub.execute_input": "2023-12-05T22:42:52.936497Z",
     "iopub.status.idle": "2023-12-05T22:42:53.075594Z",
     "shell.execute_reply.started": "2023-12-05T22:42:52.936458Z",
     "shell.execute_reply": "2023-12-05T22:42:53.073970Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A watchful eye may have already noticed that the code in the previous file (previous version) was the same, as in exactly the same. Then why did the outcome change?\n",
    "This is because of the data parameter. The candidates are generated on the same dataset, not one on a dataset with the age group feature and the other on a dataset without the age group feature. This means that the age group has no NAN values anymore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_features(data):\n",
    "    columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n",
    "    'perceived_colour_master_id', 'department_no', 'index_code',\n",
    "    'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n",
    "    'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', 'bestseller_rank', 'age_group_bestseller_rank', 'age_group']\n",
    "    \n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on=['customer_id', 'age_group'])\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "    \n",
    "    # features from assignment 2 can go here\n",
    "    \n",
    "    return result[columns_to_use]\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:53.077209Z",
     "iopub.execute_input": "2023-12-05T22:42:53.077534Z",
     "iopub.status.idle": "2023-12-05T22:42:53.095903Z",
     "shell.execute_reply.started": "2023-12-05T22:42:53.077495Z",
     "shell.execute_reply": "2023-12-05T22:42:53.094700Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# use the generation for training and testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# define the test week and limit the data to a set of previous weeks\n",
    "\"\"\"\n",
    "The same code as the previous step. Choosing a week, using it to set boundaries on used data\n",
    "\"\"\"\n",
    "test_week = 105\n",
    "num_training_weeks = 10\n",
    "absolute_max_week = transactions.week.max()\n",
    "print(test_week)\n",
    "test_week_transactions = transactions[transactions.week == test_week]\n",
    "transactions = transactions[(transactions.week > test_week - num_training_weeks - 1) & (transactions.week < test_week)].reset_index(drop=True)\n",
    "\n",
    "customers[\"age_group\"] = customers[\"age\"].apply(get_age_group)\n",
    "# firstly take the age_groups and the customer ids\n",
    "age_groups_customers = customers[['customer_id', 'age_group']].drop_duplicates()\n",
    "\n",
    "# now join them into the transactions to create a new transactions set to work with\n",
    "transactions = pd.merge(transactions, age_groups_customers)\n",
    "# now the age_group is included, we will have to change some values and names to ensure this is used\n",
    "\n",
    "# assemble training data by using positive and negative samples\n",
    "# NOTE: filler is also saved to fill the missing customers' predictions\n",
    "examples, filler = candidate_generation(transactions, test_week)\n",
    "print(examples)\n",
    "train_examples = examples[examples.week != test_week]\n",
    "train_x = add_features(train_examples)\n",
    "train_y = train_examples['purchased']\n",
    "\n",
    "# make the ranker, make the train_groups\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "# sort the training_examples\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(train_x, train_y, group=train_groups)\n",
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    print(train_x.columns[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "predict the scores similarly to Radek's baseline and then create submissions in case the week is correct\n",
    "\"\"\"\n",
    "\n",
    "# testing\n",
    "test_examples_not_copy = examples[examples.week == test_week]\n",
    "print(test_examples_not_copy)\n",
    "test_examples = test_examples_not_copy.copy()\n",
    "test_x = add_features(test_examples)\n",
    "\n",
    "# predict the scores\n",
    "test_examples[\"score\"] = ranker.predict(test_x)\n",
    "\n",
    "test_examples = test_examples[['customer_id', 'article_id', 'score']]\n",
    "print(test_examples)\n",
    "predictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n",
    "    .groupby(\"customer_id\")\\\n",
    "    .head(12)\\\n",
    "    .groupby(\"customer_id\")\\\n",
    "    .article_id.apply(list).reset_index()\\\n",
    "    .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n",
    "print(predictions)\n",
    "\n",
    "# make the predictions of the customers\n",
    "# predictions = test_examples.sort_values([\"customer_id\", \"score\"], ascending=False)\\\n",
    "#     .groupby(\"customer_id\")\\\n",
    "#     .head(12)\\\n",
    "#     .groupby(\"customer_id\", as_index = False)\\\n",
    "#     .article_id.apply(list)\\\n",
    "#     .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n",
    "\n",
    "# scored_candidates.sort_values([\"customer_id\", \"score\"], ascending=False)\n",
    "#         .groupby(\"customer_id\")\n",
    "#         .head(k)\n",
    "#         .groupby(\"customer_id\", as_index=False)\n",
    "#         .article_id.apply(list)\n",
    "#         .rename(columns={\"article_id\": \"prediction\"})[[\"customer_id\", \"prediction\"]]\n",
    "\n",
    "\"\"\"\n",
    "Here the week is checked and the submission made if the week is correct. Otherwise there is room for manual checking and evaluating\n",
    "\"\"\"\n",
    "\n",
    "### evaluate\n",
    "# if the test week is a week of which the data is in fact known\n",
    "if test_week < absolute_max_week:\n",
    "    print(\"In a previous week right now\")\n",
    "    pass\n",
    "\n",
    "# if the week is our target week\n",
    "else:\n",
    "    \"\"\"\n",
    "    make the missing customers as a series\n",
    "    then merge them with the filler and check some contents. \n",
    "    finally make the submission\n",
    "    \"\"\"\n",
    "    missing_customers = pd.Series(\n",
    "        list(set(age_groups_customers.customer_id) - set(predictions.customer_id)),\n",
    "        name=\"customer_id\",\n",
    "    )\n",
    "    # the filler is used to fill the predictions for the missing customers\n",
    "    missing_predictions = pd.merge(\n",
    "        missing_customers, pd.Series([filler], name=\"prediction\"), how=\"cross\"\n",
    "    )\n",
    "    \n",
    "    # some checking prints\n",
    "    print(\"====================\")\n",
    "    print(missing_customers)\n",
    "    print(\"====================\")\n",
    "    print(filler)\n",
    "    print(\"====================\")\n",
    "    print(missing_predictions)\n",
    "    predictions = pd.concat((predictions, missing_predictions))\n",
    "    \n",
    "    # create a submission, just like Radek did\n",
    "    predictions = predictions.set_index(\"customer_id\").prediction.to_dict()\n",
    "    preds = []\n",
    "    sub = sample_submission.copy()\n",
    "    for customer_id in customer_hex_id_to_int(sub.customer_id):\n",
    "        preds.append(\" \".join(f\"0{x}\" for x in predictions[customer_id]))\n",
    "    sub.prediction = preds\n",
    "    \n",
    "    # to csv\n",
    "    sub_name = 'age_group_first_features_model_submission'\n",
    "    sub.to_csv(f'{sub_name}.csv.gz', index=False)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T22:42:53.098638Z",
     "iopub.execute_input": "2023-12-05T22:42:53.099264Z",
     "iopub.status.idle": "2023-12-05T22:47:02.118419Z",
     "shell.execute_reply.started": "2023-12-05T22:42:53.099220Z",
     "shell.execute_reply": "2023-12-05T22:47:02.116691Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "105\n=========================================\nThe content supposed to be in filler\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n=========================================\nThe content in filler\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n              t_dat           customer_id  article_id     price  \\\n0        2020-07-26        28847241659200   887770001  0.016932   \n1        2020-07-18        28847241659200   762846001  0.025407   \n2        2020-07-18        28847241659200   829308001  0.033881   \n3        2020-07-26        28847241659200   760084003  0.025094   \n4        2020-07-26        28847241659200   866731001  0.024919   \n...             ...                   ...         ...       ...   \n24067766 2020-09-21  18446737527580148316   928206001  0.033260   \n24067767 2020-09-21  18446737527580148316   751471043  0.033019   \n24067768 2020-09-21  18446737527580148316   910601002  0.040806   \n24067769 2020-09-21  18446737527580148316   896169002  0.049836   \n24067770 2020-09-21  18446737527580148316   929275001  0.058531   \n\n          sales_channel_id  week  age_group  purchased  bestseller_rank  \\\n0                        1    96          1        1.0            999.0   \n1                        1    96          1        0.0            999.0   \n2                        1    96          1        0.0            999.0   \n3                        1    96          1        0.0              1.0   \n4                        1    96          1        0.0              2.0   \n...                    ...   ...        ...        ...              ...   \n24067766                 2   105          5        0.0            999.0   \n24067767                 2   105          5        0.0            999.0   \n24067768                 2   105          5        0.0            999.0   \n24067769                 2   105          5        0.0            999.0   \n24067770                 2   105          5        0.0            999.0   \n\n          age_group_bestseller_rank  \n0                             999.0  \n1                             999.0  \n2                             999.0  \n3                               1.0  \n4                               8.0  \n...                             ...  \n24067766                        4.0  \n24067767                        6.0  \n24067768                        9.0  \n24067769                       10.0  \n24067770                       11.0  \n\n[24067771 rows x 10 columns]\n[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.103207\n[LightGBM] [Debug] init for col-wise cost 0.000112 seconds, init for row-wise cost 2.538182 seconds\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.819104 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Debug] Using Dense Multi-Val Bin\n[LightGBM] [Info] Total Bins 1166\n[LightGBM] [Info] Number of data points in the train set: 14992830, number of used features: 20\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\nage_group_bestseller_rank 0.5391450300551438\nbestseller_rank 0.4599967773679569\nage 0.0002829210162747213\nproduct_type_no 8.530934030416766e-05\ncolour_group_code 7.89020918780184e-05\ngarment_group_no 7.867067289923367e-05\ndepartment_no 7.860098006229983e-05\nperceived_colour_value_id 7.626709823365781e-05\narticle_id 6.5635588196715e-05\nsection_no 4.989994149193562e-05\npostal_code 3.1512540040359314e-05\nclub_member_status 3.0473307518179505e-05\nFN 0.0\nindex_group_no 0.0\nindex_code 0.0\nActive 0.0\nperceived_colour_master_id 0.0\nfashion_news_frequency 0.0\ngraphical_appearance_no 0.0\nage_group 0.0\n              t_dat           customer_id  article_id     price  \\\n14992830 2020-09-03        28847241659200   925246001  0.128797   \n14992831 2020-07-18        28847241659200   924243001  0.041535   \n14992832 2020-07-18        28847241659200   924243002  0.041877   \n14992833 2020-07-18        28847241659200   918522001  0.041435   \n14992834 2020-07-18        28847241659200   923758001  0.033462   \n...             ...                   ...         ...       ...   \n24067766 2020-09-21  18446737527580148316   928206001  0.033260   \n24067767 2020-09-21  18446737527580148316   751471043  0.033019   \n24067768 2020-09-21  18446737527580148316   910601002  0.040806   \n24067769 2020-09-21  18446737527580148316   896169002  0.049836   \n24067770 2020-09-21  18446737527580148316   929275001  0.058531   \n\n          sales_channel_id  week  age_group  purchased  bestseller_rank  \\\n14992830                 2   105          1        0.0            999.0   \n14992831                 1   105          1        0.0              1.0   \n14992832                 1   105          1        0.0              2.0   \n14992833                 1   105          1        0.0              3.0   \n14992834                 1   105          1        0.0              4.0   \n...                    ...   ...        ...        ...              ...   \n24067766                 2   105          5        0.0            999.0   \n24067767                 2   105          5        0.0            999.0   \n24067768                 2   105          5        0.0            999.0   \n24067769                 2   105          5        0.0            999.0   \n24067770                 2   105          5        0.0            999.0   \n\n          age_group_bestseller_rank  \n14992830                      999.0  \n14992831                        2.0  \n14992832                        6.0  \n14992833                        1.0  \n14992834                       11.0  \n...                             ...  \n24067766                        4.0  \n24067767                        6.0  \n24067768                        9.0  \n24067769                       10.0  \n24067770                       11.0  \n\n[9074941 rows x 10 columns]\n                   customer_id  article_id     score\n14992830        28847241659200   925246001  0.183955\n14992831        28847241659200   924243001 -0.152278\n14992832        28847241659200   924243002 -0.183640\n14992833        28847241659200   918522001 -0.162738\n14992834        28847241659200   923758001 -0.188911\n...                        ...         ...       ...\n24067766  18446737527580148316   928206001 -0.188911\n24067767  18446737527580148316   751471043 -0.188911\n24067768  18446737527580148316   910601002 -0.188911\n24067769  18446737527580148316   896169002 -0.183640\n24067770  18446737527580148316   929275001 -0.188911\n\n[9074941 rows x 3 columns]\n                 customer_id  \\\n0             28847241659200   \n1             41318098387474   \n2            116809474287335   \n3            200292573348128   \n4            248294615847351   \n...                      ...   \n437360  18446624797007271432   \n437361  18446630855572834764   \n437362  18446662237889060501   \n437363  18446705133201055310   \n437364  18446737527580148316   \n\n                                               prediction  \n0       [925246001, 924243001, 918522001, 924243002, 9...  \n1       [868879003, 924243001, 930380001, 924243002, 9...  \n2       [906305002, 924243001, 918522001, 924243002, 9...  \n3       [903861001, 924243001, 909370001, 924243002, 8...  \n4       [720504008, 878987003, 337991001, 471714002, 9...  \n...                                                   ...  \n437360  [556539027, 805308003, 832482001, 832482004, 9...  \n437361  [568601045, 886966002, 898713001, 924243001, 9...  \n437362  [845790006, 915526002, 924243001, 930380001, 9...  \n437363  [875784002, 924243002, 924243001, 930380001, 9...  \n437364  [547780001, 763988001, 763988003, 547780040, 9...  \n\n[437365 rows x 2 columns]\n====================\n0          8458218447050899455\n1           542790048565690371\n2          1802629795750608900\n3          4887099159844225027\n4         16144079219589119995\n                  ...         \n934610    15944707497669951467\n934611     4086072098351480818\n934612    15284459503020408814\n934613     8719979846679134195\n934614    12233031500311822330\nName: customer_id, Length: 934615, dtype: uint64\n====================\n[924243001 924243002 918522001 923758001 866731001 909370001 751471001\n 915529003 915529005 448509014 762846027 714790020]\n====================\n                 customer_id  \\\n0        8458218447050899455   \n1         542790048565690371   \n2        1802629795750608900   \n3        4887099159844225027   \n4       16144079219589119995   \n...                      ...   \n934610  15944707497669951467   \n934611   4086072098351480818   \n934612  15284459503020408814   \n934613   8719979846679134195   \n934614  12233031500311822330   \n\n                                               prediction  \n0       [924243001, 924243002, 918522001, 923758001, 8...  \n1       [924243001, 924243002, 918522001, 923758001, 8...  \n2       [924243001, 924243002, 918522001, 923758001, 8...  \n3       [924243001, 924243002, 918522001, 923758001, 8...  \n4       [924243001, 924243002, 918522001, 923758001, 8...  \n...                                                   ...  \n934610  [924243001, 924243002, 918522001, 923758001, 8...  \n934611  [924243001, 924243002, 918522001, 923758001, 8...  \n934612  [924243001, 924243002, 918522001, 923758001, 8...  \n934613  [924243001, 924243002, 918522001, 923758001, 8...  \n934614  [924243001, 924243002, 918522001, 923758001, 8...  \n\n[934615 rows x 2 columns]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submission"
   ],
   "metadata": {}
  }
 ]
}
