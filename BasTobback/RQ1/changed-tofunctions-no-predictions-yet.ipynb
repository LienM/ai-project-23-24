{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 31254,
     "databundleVersionId": 3103714,
     "sourceType": "competition"
    },
    {
     "sourceId": 93163345,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30178,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Radek posted about this [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/309220), and linked to a GitHub repo with the code.\n",
    "\n",
    "I just transferred that code here to Kaggle notebooks, that's all."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:06.306770Z",
     "iopub.execute_input": "2023-11-21T08:38:06.307294Z",
     "iopub.status.idle": "2023-11-21T08:38:06.322319Z",
     "shell.execute_reply.started": "2023-11-21T08:38:06.307249Z",
     "shell.execute_reply": "2023-11-21T08:38:06.321309Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\n",
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)\n",
    "\n",
    "def article_id_str_to_int(series):\n",
    "    return series.astype('int32')\n",
    "\n",
    "def article_id_int_to_str(series):\n",
    "    return '0' + series.astype('str')\n",
    "\n",
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "def calculate_apk(list_of_preds, list_of_gts):\n",
    "    # for fast validation this can be changed to operate on dicts of {'cust_id_int': [art_id_int, ...]}\n",
    "    # using 'data/val_week_purchases_by_cust.pkl'\n",
    "    apks = []\n",
    "    for preds, gt in zip(list_of_preds, list_of_gts):\n",
    "        apks.append(apk(gt, preds, k=12))\n",
    "    return np.mean(apks)\n",
    "\n",
    "def eval_sub(sub_csv, skip_cust_with_no_purchases=True):\n",
    "    sub=pd.read_csv(sub_csv)\n",
    "    validation_set=pd.read_parquet('data/validation_ground_truth.parquet')\n",
    "\n",
    "    apks = []\n",
    "\n",
    "    no_purchases_pattern = []\n",
    "    for pred, gt in zip(sub.prediction.str.split(), validation_set.prediction.str.split()):\n",
    "        if skip_cust_with_no_purchases and (gt == no_purchases_pattern): continue\n",
    "        apks.append(apk(gt, pred, k=12))\n",
    "    return np.mean(apks)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:06.324819Z",
     "iopub.execute_input": "2023-11-21T08:38:06.325174Z",
     "iopub.status.idle": "2023-11-21T08:38:06.344902Z",
     "shell.execute_reply.started": "2023-11-21T08:38:06.325128Z",
     "shell.execute_reply": "2023-11-21T08:38:06.343820Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:06.346267Z",
     "iopub.execute_input": "2023-11-21T08:38:06.346603Z",
     "iopub.status.idle": "2023-11-21T08:38:06.361479Z",
     "shell.execute_reply.started": "2023-11-21T08:38:06.346561Z",
     "shell.execute_reply": "2023-11-21T08:38:06.360309Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "transactions = pd.read_parquet('../input/warmup/transactions_train.parquet')\n",
    "customers = pd.read_parquet('../input/warmup/customers.parquet')\n",
    "articles = pd.read_parquet('../input/warmup/articles.parquet')\n",
    "\n",
    "# sample = 0.05\n",
    "# transactions = pd.read_parquet(f'data/transactions_train_sample_{sample}.parquet')\n",
    "# customers = pd.read_parquet(f'data/customers_sample_{sample}.parquet')\n",
    "# articles = pd.read_parquet(f'data/articles_train_sample_{sample}.parquet')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:06.363214Z",
     "iopub.execute_input": "2023-11-21T08:38:06.363516Z",
     "iopub.status.idle": "2023-11-21T08:38:14.898740Z",
     "shell.execute_reply.started": "2023-11-21T08:38:06.363465Z",
     "shell.execute_reply": "2023-11-21T08:38:14.897739Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering\n",
    "We want to add some features or change some values, therefore we engineer some features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# define age groups\n",
    "def get_age_group(age):\n",
    "    if age < 18:\n",
    "        return 0\n",
    "    elif age >= 18 and age < 25:\n",
    "        return 1\n",
    "    elif age >= 25 and age < 35:\n",
    "        return 2\n",
    "    elif age >= 35 and age < 45:\n",
    "        return 3\n",
    "    elif age >= 45 and age < 55:\n",
    "        return 4\n",
    "    elif age >= 55 and age < 65:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:14.901887Z",
     "iopub.execute_input": "2023-11-21T08:38:14.902526Z",
     "iopub.status.idle": "2023-11-21T08:38:14.910664Z",
     "shell.execute_reply.started": "2023-11-21T08:38:14.902477Z",
     "shell.execute_reply": "2023-11-21T08:38:14.909652Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transactions"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:14.912675Z",
     "iopub.execute_input": "2023-11-21T08:38:14.913385Z",
     "iopub.status.idle": "2023-11-21T08:38:14.945275Z",
     "shell.execute_reply.started": "2023-11-21T08:38:14.913333Z",
     "shell.execute_reply": "2023-11-21T08:38:14.944195Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making a recall evaluation function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# return the average recall of generated candidates versus the actual bought items\n",
    "def average_recall(purchases, candidates):\n",
    "    joined = pd.merge(purchases, candidates, how='inner').drop_duplicates()\n",
    "    true_positives = joined.groupby('customer_id').count()\n",
    "    total_positives = purchases.groupby('customer_id').count()\n",
    "    recall = true_positives.divide(total_positives, fill_value=0)\n",
    "    return recall.mean().values[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:14.946886Z",
     "iopub.execute_input": "2023-11-21T08:38:14.948197Z",
     "iopub.status.idle": "2023-11-21T08:38:14.956610Z",
     "shell.execute_reply.started": "2023-11-21T08:38:14.948146Z",
     "shell.execute_reply": "2023-11-21T08:38:14.955219Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating candidates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def candidate_generation(data, test_week):\n",
    "    \"\"\"\n",
    "    Return the candidates for test_week based on data given. The candidates that are generated are the repurchase candidates, the bestsellers and the bestsellers based on an age group.\n",
    "    :param data: a pandas dataframe with the transactions\n",
    "    :param test_week: a value that indicates the week that is supposed to be taken, advised is using either 104 as testing or 105 as final target week\n",
    "    :return: pandas dataframe containing the candidates for all customers available in data\n",
    "    \"\"\"\n",
    "    ################\n",
    "    ## Repurchase ##\n",
    "    ################\n",
    "    \"\"\"\n",
    "    The repurchases as they were generated in the Radek baseline. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    c2weeks = transactions.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = transactions.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(transactions['customer_id'], transactions['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "    \n",
    "    ################\n",
    "    ## bestseller ##\n",
    "    ################\n",
    "    \"\"\"\n",
    "    The bestsellers as they were generated in the Radek baseline. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    mean_price = transactions \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = transactions \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    unique_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "    \n",
    "    # testing the recall of the bestseller candidates which are generated\n",
    "    # done by taking the subsets of the actual transactions and the generated candidates ONLY WHEN the test week is not bigger than the absolute max_week which is the highest week available in the dataset\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        t_candidates = candidates_bestsellers[candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        print(f\"Average recall of bestsellers : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    ###################################\n",
    "    ## Bestseller based on age group ##\n",
    "    ###################################\n",
    "    \"\"\"\n",
    "    The bestsellers as they were generated in the previous file. All pieces of code are kept intact. No further changes are applied.\n",
    "    \"\"\"\n",
    "    # Group the mean_price not per week/article but by week/article/age_group\n",
    "    # this is so we know the value per age group per week\n",
    "    mean_price_age_group = transactions \\\n",
    "        .groupby(['week', 'age_group', 'article_id'])['price'].mean()\n",
    "\n",
    "    # group the sales by week AND the age group and so find the most popular article for each age group in each week\n",
    "    sales_age_group = transactions \\\n",
    "        .groupby(['week', 'age_group'])['article_id'].value_counts() \\\n",
    "        .groupby(['week', 'age_group']).rank(method='dense', ascending=False) \\\n",
    "        .groupby(['week', 'age_group']).head(12).rename('age_group_bestseller_rank').astype('int8')\n",
    "\n",
    "    # now calculate the bestsellers for these week - age_group combos\n",
    "    bestsellers_previous_week_age_group = pd.merge(sales_age_group, mean_price_age_group, on=['week', 'age_group', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week_age_group.week += 1\n",
    "\n",
    "    unique_age_group_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "\n",
    "    age_group_candidates_bestsellers = pd.merge(\n",
    "        unique_age_group_transactions,\n",
    "        bestsellers_previous_week_age_group,\n",
    "        on=['week', 'age_group'],\n",
    "    )\n",
    "    test_set_age_group_transactions = unique_age_group_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_age_group_transactions.week = test_week\n",
    "\n",
    "    age_group_candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_age_group_transactions,\n",
    "        bestsellers_previous_week_age_group,\n",
    "        on=['week', 'age_group'],\n",
    "    )\n",
    "    age_group_candidates_bestsellers = pd.concat([age_group_candidates_bestsellers, age_group_candidates_bestsellers_test_week])\n",
    "    age_group_candidates_bestsellers.drop(columns='age_group_bestseller_rank', inplace=True)\n",
    "    \n",
    "    # testing the recall of the age group bestseller candidates which are generated\n",
    "    # done by taking the subsets of the actual transactions and the generated candidates ONLY WHEN the test week is not bigger than the absolute max_week which is the highest week available in the dataset\n",
    "    if not test_week > absolute_max_week:\n",
    "        t_purchases = test_week_transactions[test_week_transactions.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        t_candidates = age_group_candidates_bestsellers[age_group_candidates_bestsellers.week == test_week][['customer_id', 'article_id']].drop_duplicates()\n",
    "        print(f\"Average recall of age group bestsellers : {average_recall(t_purchases, t_candidates)}\")\n",
    "    \n",
    "    \n",
    "    ###################################################\n",
    "    # Combine the transactions and negative examples ##\n",
    "    ###################################################\n",
    "    \"\"\"\n",
    "    Here the code which I have written in one of the previous steps is omitted. This is the part of the sin values of age etc. They did not seem pertinent to the objective of this step of my research. The rest is still the same as the code written in the baseline file with age groups. No further changes are made. The initial assumption was that the results would be similar, but they aren't.\n",
    "    \"\"\"\n",
    "    purchased_transactions = data.copy()\n",
    "    transactions['purchased'] = 1\n",
    "    result = pd.concat([\n",
    "        data, candidates_last_purchase, candidates_bestsellers, age_group_candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(0, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    # merge the data with the bestsellers information from the age_group popularity study\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week_age_group[['week', 'age_group', 'article_id', 'age_group_bestseller_rank']],\n",
    "        on=['week', 'age_group', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "    result.age_group_bestseller_rank.fillna(999, inplace=True)\n",
    "    \n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:14.960379Z",
     "iopub.execute_input": "2023-11-21T08:38:14.961029Z",
     "iopub.status.idle": "2023-11-21T08:38:15.009613Z",
     "shell.execute_reply.started": "2023-11-21T08:38:14.960976Z",
     "shell.execute_reply": "2023-11-21T08:38:15.008749Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A watchful eye may have already noticed that the code in the previous file (previous version) was the same, as in exactly the same. Then why did the outcome change?\n",
    "This is because of the data parameter. The candidates are generated on the same dataset, not one on a dataset with the age group feature and the other on a dataset without the age group feature. This means that the age group has no NAN values anymore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_features(data):\n",
    "    columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n",
    "    'perceived_colour_master_id', 'department_no', 'index_code',\n",
    "    'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n",
    "    'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', 'bestseller_rank', 'age_group_bestseller_rank', 'age_group']\n",
    "    \n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on=['customer_id', 'age_group'])\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "    \n",
    "    # features from assignment 2 can go here\n",
    "    \n",
    "    return result[columns_to_use]\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:15.011102Z",
     "iopub.execute_input": "2023-11-21T08:38:15.011620Z",
     "iopub.status.idle": "2023-11-21T08:38:15.020731Z",
     "shell.execute_reply.started": "2023-11-21T08:38:15.011580Z",
     "shell.execute_reply": "2023-11-21T08:38:15.019638Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "now both age group and age group bestsellers are used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# use the generation for training and testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# define the test week and limit the data to a set of previous weeks\n",
    "\"\"\"\n",
    "The same code as the previous step. Choosing a week, using it to set boundaries on used data\n",
    "\"\"\"\n",
    "test_week = 104\n",
    "num_training_weeks = 10\n",
    "absolute_max_week = transactions.week.max()\n",
    "print(test_week)\n",
    "test_week_transactions = transactions[transactions.week == test_week]\n",
    "transactions = transactions[(transactions.week > test_week - num_training_weeks - 1) & (transactions.week < test_week)].reset_index(drop=True)\n",
    "\n",
    "customers[\"age_group\"] = customers[\"age\"].apply(get_age_group)\n",
    "# firstly take the age_groups and the customer ids\n",
    "age_groups_customers = customers[['customer_id', 'age_group']].drop_duplicates()\n",
    "\n",
    "# now join them into the transactions to create a new transactions set to work with\n",
    "transactions = pd.merge(transactions, age_groups_customers)\n",
    "# now the age_group is included, we will have to change some values and names to ensure this is used\n",
    "\n",
    "# assemble training data by using positive and negative samples\n",
    "examples = candidate_generation(transactions, test_week)\n",
    "print(examples)\n",
    "# take the weeks that are before the test week to train on\n",
    "train_examples = examples[examples.week != test_week]\n",
    "# add the features to make sure all data is present and all not used features are omitted\n",
    "train_x = add_features(train_examples)      # note that adding the features also removes the purchased feature\n",
    "train_y = train_examples['purchased']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Same code as before. This code does not yet work as it hinges on the availability of certain variables that are either renamed or enclosed within a scope. \n",
    "This code was the first stepping stone to the functionality solution.\n",
    "\"\"\"\n",
    "# make the ranker, make the train_groups\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "# sort the training_examples\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(train_x, train_y, group=train_groups)\n",
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    print(train_x.columns[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())\n",
    "    \n",
    "\n",
    "# testing\n",
    "test_examples = examples[examples.week == test_week]\n",
    "test_x = add_features(test_examples)\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:38:15.022490Z",
     "iopub.execute_input": "2023-11-21T08:38:15.022980Z",
     "iopub.status.idle": "2023-11-21T08:40:56.027322Z",
     "shell.execute_reply.started": "2023-11-21T08:38:15.022928Z",
     "shell.execute_reply": "2023-11-21T08:40:56.026083Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Until this point the solution has been altered to incorporate function to solve the memory usage issue with the previous implementations. \n",
    "From here on the code has not been altered. This file could not yet produce submissions, which was a problem, but it did have the ability to show the recall which stayed the exact same.\n",
    "Also checking if there were NAN values was easier and showed no NAN values anymore.\n",
    "\n",
    "The next step was changing the submissions so that a submission to Kaggle is possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%time\n",
    "# these variables and operations are not available as the functions are changed.\n",
    "test['preds'] = ranker.predict(test_X)\n",
    "\n",
    "c_id2predicted_article_ids = test \\\n",
    "    .sort_values(['customer_id', 'preds'], ascending=False) \\\n",
    "    .groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "# the bestsellers of last week aren't publicly available anymore\n",
    "bestsellers_last_week = \\\n",
    "    bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()]['article_id'].tolist()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:40:56.029918Z",
     "iopub.execute_input": "2023-11-21T08:40:56.030868Z",
     "iopub.status.idle": "2023-11-21T08:40:56.072688Z",
     "shell.execute_reply.started": "2023-11-21T08:40:56.030823Z",
     "shell.execute_reply": "2023-11-21T08:40:56.071125Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submission"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "sub = pd.read_csv('/kaggle/input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:40:56.074046Z",
     "iopub.status.idle": "2023-11-21T08:40:56.074589Z",
     "shell.execute_reply.started": "2023-11-21T08:40:56.074279Z",
     "shell.execute_reply": "2023-11-21T08:40:56.074309Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "preds = []\n",
    "for c_id in customer_hex_id_to_int(sub.customer_id):\n",
    "    pred = c_id2predicted_article_ids.get(c_id, [])\n",
    "    pred = pred + bestsellers_last_week\n",
    "    preds.append(pred[:12])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:40:56.076047Z",
     "iopub.status.idle": "2023-11-21T08:40:56.076586Z",
     "shell.execute_reply.started": "2023-11-21T08:40:56.076303Z",
     "shell.execute_reply": "2023-11-21T08:40:56.076332Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preds = [' '.join(['0' + str(p) for p in ps]) for ps in preds]\n",
    "sub.prediction = preds"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:40:56.079718Z",
     "iopub.status.idle": "2023-11-21T08:40:56.080260Z",
     "shell.execute_reply.started": "2023-11-21T08:40:56.079969Z",
     "shell.execute_reply": "2023-11-21T08:40:56.079998Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sub_name = 'basic_model_submission'\n",
    "sub.to_csv(f'{sub_name}.csv.gz', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-21T08:40:56.081731Z",
     "iopub.status.idle": "2023-11-21T08:40:56.082243Z",
     "shell.execute_reply.started": "2023-11-21T08:40:56.081959Z",
     "shell.execute_reply": "2023-11-21T08:40:56.081987Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
