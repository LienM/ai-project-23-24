{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:27.839690200Z",
     "start_time": "2023-11-18T16:01:27.776245400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os;os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:29.735280400Z",
     "start_time": "2023-11-18T16:01:27.779701200Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:29.735280400Z",
     "start_time": "2023-11-18T16:01:29.731574400Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 20193575\n",
    "SEQUENCE_COLUMN = \"product_type_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:29.755887900Z",
     "start_time": "2023-11-18T16:01:29.734280300Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:32.571811400Z",
     "start_time": "2023-11-18T16:01:29.756887600Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = pd.read_parquet('../data/articles.parquet')\n",
    "customers = pd.read_parquet('../data/customers.parquet')\n",
    "transactions = pd.read_parquet('../data/transactions_train.parquet')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.merge(transactions, articles[[\"article_id\", \"prod_name\", \"product_type_name\"]], on=\"article_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:33.865976200Z",
     "start_time": "2023-11-18T16:01:32.572814700Z"
    }
   },
   "outputs": [],
   "source": [
    "article_id_map = {original: (idx + 1) for idx, original in enumerate(articles[\"article_id\"].unique())}\n",
    "inverse_article_id_map = {(idx + 1): original for idx, original in enumerate(articles[\"article_id\"].unique())}\n",
    "articles[\"article_id_mapped\"] = articles[\"article_id\"].map(article_id_map)\n",
    "transactions[\"article_id_mapped\"] = transactions[\"article_id\"].map(article_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:33.894010800Z",
     "start_time": "2023-11-18T16:01:33.866976700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id                      105542\n",
       "product_code                     47224\n",
       "prod_name                        45875\n",
       "product_type_no                    132\n",
       "product_type_name                  131\n",
       "product_group_name                  19\n",
       "graphical_appearance_no             30\n",
       "graphical_appearance_name           30\n",
       "colour_group_code                   50\n",
       "colour_group_name                   50\n",
       "perceived_colour_value_id            8\n",
       "perceived_colour_value_name          8\n",
       "perceived_colour_master_id          20\n",
       "perceived_colour_master_name        20\n",
       "department_no                      299\n",
       "department_name                    250\n",
       "index_code                          10\n",
       "index_name                          10\n",
       "index_group_no                       5\n",
       "index_group_name                     5\n",
       "section_no                          57\n",
       "section_name                        56\n",
       "garment_group_no                    21\n",
       "garment_group_name                  21\n",
       "detail_desc                      43405\n",
       "article_id_mapped               105542\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:33.899874300Z",
     "start_time": "2023-11-18T16:01:33.892009400Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMRecommender(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_dim, hidden_dim, n_articles, num_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_articles = n_articles\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding articles to a lower dimension\n",
    "        self.embedding = nn.Embedding(n_articles, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, n_articles)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_directions * self.num_layers, x.size(0), self.hidden_dim, requires_grad=True, device=device)\n",
    "        c0 = torch.zeros(self.n_directions * self.num_layers, x.size(0), self.hidden_dim, requires_grad=True, device=device)\n",
    "        # Embed\n",
    "        embedded_sequence = self.embedding(x)\n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(embedded_sequence, (h0.detach(), c0.detach()))\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "        # out = F.softmax(out, dim=1)\n",
    "        # return torch.max(out, dim=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:36.406481700Z",
     "start_time": "2023-11-18T16:01:33.898874200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split 80/20 on customer id\n",
    "train_customers, val_customers = train_test_split(transactions.customer_id.unique(), test_size=0.2, random_state=SEED)\n",
    "training_transactions_df = transactions[transactions.customer_id.isin(train_customers)]\n",
    "validation_transactions_df = transactions[transactions.customer_id.isin(val_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:36.409939800Z",
     "start_time": "2023-11-18T16:01:36.407480900Z"
    }
   },
   "outputs": [],
   "source": [
    "N_TRAINING_WEEKS = 5\n",
    "MAX_WEEK = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:36.424677800Z",
     "start_time": "2023-11-18T16:01:36.409939800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_transactions_by_weeks(transactions, column=\"article_id_mapped\"):\n",
    "    _transactions = transactions[[\"customer_id\", column, \"week\"]]\n",
    "    filtered_transactions = _transactions[_transactions.week.between(MAX_WEEK - N_TRAINING_WEEKS, MAX_WEEK - 1)]\n",
    "    filtered_transactions = filtered_transactions.groupby(\"customer_id\")[column].apply(list).reset_index(name=\"history\")\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:39.078819600Z",
     "start_time": "2023-11-18T16:01:36.425676900Z"
    }
   },
   "outputs": [],
   "source": [
    "training_transactions = filter_transactions_by_weeks(training_transactions_df, SEQUENCE_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:39.682684300Z",
     "start_time": "2023-11-18T16:01:39.079820900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_transactions = filter_transactions_by_weeks(validation_transactions_df, SEQUENCE_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:40.664591700Z",
     "start_time": "2023-11-18T16:01:39.684684500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.362281e+06\n",
       "mean     2.333463e+01\n",
       "std      3.924225e+01\n",
       "min      1.000000e+00\n",
       "25%      3.000000e+00\n",
       "50%      9.000000e+00\n",
       "75%      2.700000e+01\n",
       "max      1.895000e+03\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.customer_id.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:41.900947600Z",
     "start_time": "2023-11-18T16:01:40.664591700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_transactions_df.customer_id.value_counts().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.431461200Z",
     "start_time": "2023-11-18T16:01:41.899540400Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Padding article added to map, actual articles start at 1\n",
    "article_id_map[-1] = 0\n",
    "PADDING_ARTICLE = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "NUM_ARTICLES_IN_SEQUENCE = 12\n",
    "N_ARTICLES = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "model = LSTMRecommender(\n",
    "    input_dim=NUM_ARTICLES_IN_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    # Output dim is only the number of articles while n_articles is for the embedding and has to include the padding\n",
    "    n_articles=N_ARTICLES+1,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    "    )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.442080600Z",
     "start_time": "2023-11-18T16:01:42.432463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    105542.000000\n",
       "mean         12.750573\n",
       "std          15.250827\n",
       "min           0.000000\n",
       "25%           2.000000\n",
       "50%           7.000000\n",
       "75%          18.000000\n",
       "max         130.000000\n",
       "Name: product_type_name, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[SEQUENCE_COLUMN].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.456433400Z",
     "start_time": "2023-11-18T16:01:42.437978700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[SEQUENCE_COLUMN].max() - articles[SEQUENCE_COLUMN].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.469793800Z",
     "start_time": "2023-11-18T16:01:42.455434Z"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "   def __init__(self, sequences, targets):\n",
    "       self.sequences = sequences\n",
    "       self.targets = targets\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.sequences)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "def combine_sequences(user_transactions):\n",
    "    combined_sequence_batch = []\n",
    "    combined_target_batch = []\n",
    "    \n",
    "    for idx, (customer, history) in user_transactions.iterrows():\n",
    "        history_batch, target_batch = create_batch(history)\n",
    "        if history_batch is None or target_batch is None:\n",
    "            continue\n",
    "        combined_sequence_batch.extend(history_batch)\n",
    "        combined_target_batch.extend(target_batch)\n",
    "    \n",
    "    sequence_dataset = SequenceDataset(combined_sequence_batch, combined_target_batch)\n",
    "    dataloader = DataLoader(sequence_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def create_batch(history):\n",
    "    # Create batch of sequences\n",
    "    if len(history) <= 1:\n",
    "        return None, None\n",
    "    history_batch = []\n",
    "    target_batch = []\n",
    "    for i in range(1, len(history)):\n",
    "        if i < 12:\n",
    "            # Add padding to the beginning of the sequence\n",
    "            history_batch.append(torch.tensor([PADDING_ARTICLE] * (NUM_ARTICLES_IN_SEQUENCE - i) + history[:i], dtype=torch.int32))\n",
    "        else:\n",
    "            history_batch.append(torch.tensor(history[i-12:i], dtype=torch.int32))\n",
    "        target_batch.append(torch.tensor(history[i], dtype=torch.float32, requires_grad=True))\n",
    "    return history_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.485764Z",
     "start_time": "2023-11-18T16:01:42.460853500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116809474287335</td>\n",
       "      <td>[2, 2, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200292573348128</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>329094189075899</td>\n",
       "      <td>[44, 44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>690285180337957</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>745180086074610</td>\n",
       "      <td>[17, 1, 15, 1, 17, 13, 3, 1, 9, 1, 4, 1, 1, 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id                                            history\n",
       "0  116809474287335                                 [2, 2, 0, 2, 2, 2]\n",
       "1  200292573348128                                                [8]\n",
       "2  329094189075899                                           [44, 44]\n",
       "3  690285180337957                                          [2, 2, 2]\n",
       "4  745180086074610  [17, 1, 15, 1, 17, 13, 3, 1, 9, 1, 4, 1, 1, 17..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.487763400Z",
     "start_time": "2023-11-18T16:01:42.467793700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    y_true, y_pred = [], []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dataloader = combine_sequences(validation_transactions)\n",
    "        for sequences, targets in dataloader:\n",
    "            sequence = sequences.to(device)\n",
    "            target = targets.to(device).long()\n",
    "            \n",
    "            # Predict\n",
    "            out = model(sequence)\n",
    "            loss = criterion(out, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(out, dim=1)\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "        \n",
    "    val_loss /= len(dataloader)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    return accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T15:45:57.864590900Z",
     "start_time": "2023-11-18T15:21:26.725958300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218069\n",
      "Training start: Sat Nov 18 22:05:43 2023\n",
      "+-------+---------------------+-------------------+-------------------------+--------------------+----------------------+------------+\n",
      "| Epoch | Validation Accuracy | Training Accuracy |     Validation Loss     |     Epoch Loss     |     Running Loss     | Epoch Time |\n",
      "+-------+---------------------+-------------------+-------------------------+--------------------+----------------------+------------+\n",
      "| 1     | 30.5747%            | 28.7003%          | 2.7050                  | 2.8379             | 2.8379               | 32.46s     |\n",
      "| 2     | 31.0021%            | 31.1157%          | 2.6700                  | 2.6791             | 5.5170               | 33.38s     |\n",
      "| 3     | 31.1585%            | 31.3260%          | 2.6554                  | 2.6562             | 8.1732               | 34.33s     |\n",
      "| 4     | 31.2182%            | 31.4594%          | 2.6463                  | 2.6440             | 10.8171              | 34.69s     |\n",
      "| 5     | 31.3587%            | 31.5342%          | 2.6402                  | 2.6356             | 13.4528              | 33.57s     |\n",
      "| 6     | 31.4121%            | 31.6368%          | 2.6357                  | 2.6298             | 16.0825              | 34.24s     |\n",
      "| 7     | 31.5632%            | 31.7212%          | 2.6323                  | 2.6256             | 18.7081              | 33.77s     |\n",
      "| 8     | 31.5810%            | 31.7835%          | 2.6293                  | 2.6220             | 21.3301              | 34.13s     |\n",
      "| 9     | 31.5695%            | 31.8394%          | 2.6277                  | 2.6195             | 23.9496              | 32.83s     |\n",
      "| 10    | 31.6715%            | 31.8951%          | 2.6261                  | 2.6170             | 26.5666              | 33.97s     |\n",
      "| 11    | 31.6629%            | 31.9157%          | 2.6241                  | 2.6148             | 29.1815              | 33.73s     |\n",
      "| 12    | 31.6807%            | 31.9770%          | 2.6242                  | 2.6129             | 31.7944              | 32.41s     |\n",
      "| 13    | 31.7899%            | 32.0311%          | 2.6226                  | 2.6113             | 34.4057              | 33.40s     |\n",
      "| 14    | 31.7509%            | 32.0213%          | 2.6217                  | 2.6094             | 37.0151              | 32.95s     |\n",
      "| 15    | 31.8145%            | 32.0796%          | 2.6212                  | 2.6083             | 39.6233              | 33.66s     |\n",
      "| 16    | 31.8515%            | 32.1225%          | 2.6218                  | 2.6066             | 42.2299              | 34.58s     |\n",
      "| 17    | 31.7995%            | 32.1422%          | 2.6200                  | 2.6059             | 44.8358              | 33.89s     |\n",
      "| 18    | 31.8409%            | 32.1598%          | 2.6188                  | 2.6048             | 47.4406              | 32.38s     |\n",
      "| 19    | 31.8842%            | 32.1827%          | 2.6185                  | 2.6040             | 50.0446              | 33.70s     |\n",
      "| 20    | 31.8024%            | 32.1722%          | 2.6190                  | 2.6032             | 52.6477              | 33.74s     |\n",
      "| 21    | 31.9021%            | 32.2264%          | 2.6177                  | 2.6019             | 55.2496              | 33.25s     |\n",
      "| 22    | 31.8703%            | 32.2256%          | 2.6184                  | 2.6013             | 57.8509              | 34.89s     |\n",
      "| 23    | 31.9434%            | 32.2731%          | 2.6174                  | 2.6001             | 60.4510              | 33.72s     |\n",
      "| 24    | 31.9680%            | 32.2683%          | 2.6179                  | 2.5995             | 63.0505              | 33.72s     |\n",
      "| 25    | 31.9117%            | 32.2749%          | 2.6174                  | 2.5987             | 65.6492              | 33.82s     |\n",
      "| 26    | 31.9064%            | 32.2973%          | 2.6171                  | 2.5982             | 68.2474              | 33.43s     |\n",
      "| 27    | 32.0209%            | 32.3106%          | 2.6175                  | 2.5972             | 70.8446              | 34.36s     |\n",
      "| 28    | 32.0041%            | 32.3106%          | 2.6170                  | 2.5971             | 73.4416              | 34.32s     |\n",
      "| 29    | 31.9887%            | 32.3244%          | 2.6177                  | 2.5962             | 76.0379              | 34.54s     |\n",
      "| 30    | 31.9540%            | 32.3646%          | 2.6175                  | 2.5954             | 78.6333              | 33.45s     |\n",
      "| 31    | 31.9088%            | 32.3529%          | 2.6172                  | 2.5949             | 81.2282              | 34.35s     |\n",
      "| 32    | 31.8717%            | 32.3642%          | 2.6165                  | 2.5944             | 83.8226              | 33.44s     |\n",
      "| 33    | 31.9752%            | 32.3496%          | 2.6157                  | 2.5938             | 86.4164              | 34.43s     |\n",
      "| 34    | 31.9901%            | 32.3257%          | 2.6164                  | 2.5931             | 89.0095              | 34.50s     |\n",
      "| 35    | 31.9733%            | 32.3708%          | 2.6171                  | 2.5928             | 91.6023              | 34.28s     |\n",
      "| 36    | 31.9892%            | 32.3545%          | 2.6162                  | 2.5925             | 94.1949              | 33.31s     |\n",
      "| 37    | 32.0060%            | 32.3729%          | 2.6162                  | 2.5915             | 96.7864              | 34.36s     |\n",
      "| 38    | 31.9410%            | 32.3920%          | 2.6159                  | 2.5916             | 99.3780              | 34.26s     |\n",
      "| 39    | 31.9372%            | 32.3996%          | 2.6183                  | 2.5905             | 101.9685             | 33.39s     |\n",
      "| 40    | 31.9920%            | 32.4367%          | 2.6165                  | 2.5901             | 104.5586             | 34.36s     |\n",
      "| 41    | 31.9776%            | 32.4250%          | 2.6165                  | 2.5897             | 107.1483             | 33.36s     |\n",
      "| 42    | 31.9954%            | 32.4297%          | 2.6169                  | 2.5893             | 109.7375             | 34.21s     |\n",
      "| 43    | 31.9300%            | 32.4488%          | 2.6178                  | 2.5890             | 112.3265             | 34.42s     |\n",
      "| 44    | 31.9815%            | 32.4080%          | 2.6169                  | 2.5884             | 114.9149             | 33.39s     |\n",
      "| 45    | 31.9709%            | 32.4435%          | 2.6177                  | 2.5878             | 117.5028             | 34.46s     |\n",
      "| 46    | 31.9641%            | 32.4410%          | 2.6165                  | 2.5871             | 120.0899             | 34.50s     |\n",
      "| 47    | 31.9271%            | 32.4246%          | 2.6168                  | 2.5870             | 122.6769             | 34.56s     |\n",
      "| 48    | 31.9304%            | 32.4604%          | 2.6180                  | 2.5868             | 125.2637             | 33.59s     |\n",
      "| 49    | 31.9940%            | 32.4771%          | 2.6177                  | 2.5861             | 127.8498             | 34.38s     |\n",
      "| 50    | 31.9766%            | 32.4831%          | 2.6175                  | 2.5853             | 130.4351             | 34.52s     |\n",
      "Training time: 1710.33s\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "print(len(training_transactions))\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "print(\"Training start:\", time.asctime(time.localtime()))\n",
    "\n",
    "header_printed = False\n",
    "col_widths = []\n",
    "table_seperator = \"\"\n",
    "\n",
    "running_loss = 0.0\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "model.to(device)\n",
    "dataloader = combine_sequences(training_transactions)\n",
    "len(dataloader)\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    \n",
    "    training_accuracy = 0\n",
    "    epoch_loss = 0.0\n",
    "    for idx, (sequence, target) in enumerate(dataloader):\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device).long()\n",
    "        # Predict\n",
    "        out = model(sequence)\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(\n",
    "            # torch.tensor(out, dtype=torch.float32, requires_grad=True), torch.tensor(target[0], dtype=torch.float32, requires_grad=True)\n",
    "            # out.to(torch.float32).clone().detach().requires_grad_(True), target\n",
    "            out, target\n",
    "        )\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        predicted = torch.argmax(out, dim=1)\n",
    "        epoch_loss += loss.item()\n",
    "        training_accuracy += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    running_loss += epoch_loss\n",
    "    training_accuracy /= len(dataloader.dataset)\n",
    "    \n",
    "    val_accuracy, val_loss = run_validation()\n",
    "    \n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(float(val_loss))\n",
    "    \n",
    "    if not header_printed:\n",
    "        header_printed = True\n",
    "        header_text = \"| Epoch | Validation Accuracy | Training Accuracy |     Validation Loss     |     Epoch Loss     |     Running Loss     | Epoch Time |\"\n",
    "        col_widths = [len(s)-2 for s in header_text.split(\"|\")[1:-1]]\n",
    "        table_seperator = f\"+{'+'.join(['-' * (x + 2) for x in col_widths])}+\"\n",
    "        print(table_seperator)\n",
    "        print(header_text)\n",
    "        print(table_seperator)\n",
    "    \n",
    "    print(f\"| {str(epoch + 1):<{col_widths[0]}} | \"\n",
    "              f\"{f'{val_accuracy:.4%}':<{col_widths[1]}} | \"\n",
    "              f\"{f'{training_accuracy:.4%}':<{col_widths[2]}} | \"\n",
    "              f\"{f'{val_loss:.4f}':<{col_widths[3]}} | \"\n",
    "              f\"{f'{epoch_loss:.4f}':<{col_widths[4]}} | \"\n",
    "              f\"{f'{running_loss:.4f}':<{col_widths[5]}} | \"\n",
    "              f\"{f'{time.perf_counter() - epoch_start_time:.2f}s':<{col_widths[6]}} |\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"./models/LSTM_Model_Epoch_{epoch + 1}.pt\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.487763400Z",
     "start_time": "2023-11-18T16:01:42.471853Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:44.497680500Z",
     "start_time": "2023-11-18T16:01:42.474962Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:22.848548400Z",
     "start_time": "2023-11-18T16:02:22.670128800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMRecommender(\n",
    "    input_dim=NUM_ARTICLES_IN_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    # Output dim is only the number of articles while n_articles is for the embedding and has to include the padding\n",
    "    n_articles=N_ARTICLES+1,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:28.680832900Z",
     "start_time": "2023-11-18T16:02:26.797893300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./models/LSTM_Model_Epoch_25.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:30.801553700Z",
     "start_time": "2023-11-18T16:02:30.787153600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:32.757889700Z",
     "start_time": "2023-11-18T16:02:32.736819200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HistoryDataset(Dataset):\n",
    "    def __init__(self, history):\n",
    "        self.histories = history\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.histories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        history = self.histories[idx]\n",
    "        if len(history) < 12:\n",
    "            history = [PADDING_ARTICLE] * (NUM_ARTICLES_IN_SEQUENCE - len(history)) + history\n",
    "        return torch.tensor(history[-12:], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:00.350668500Z",
     "start_time": "2023-11-18T16:02:35.170346700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the dataframe by \"customer_id\" and \"t_dat\" in descending order\n",
    "df = transactions.sort_values(by=['customer_id', 't_dat'], ascending=[True, True])\n",
    "\n",
    "# Group by \"customer_id\" and get the last 12 transactions for each customer\n",
    "df_grouped = df.groupby('customer_id')['article_id_mapped'].apply(list)\n",
    "transactions_filtered = pd.DataFrame({\"customer_id\": df_grouped.index, \"sequence\": df_grouped.apply(lambda x: x[-12:])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:00.867631700Z",
     "start_time": "2023-11-18T16:03:00.350668500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert customer ids to integers\n",
    "customer_ids = sub.customer_id.apply(hex_id_to_int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:01.305242600Z",
     "start_time": "2023-11-18T16:03:00.867631700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_customer_ids = list(set(customer_ids).difference(set(transactions_filtered.customer_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:01.343808Z",
     "start_time": "2023-11-18T16:03:01.305242600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with the missing customer_ids and an empty list as the sequence\n",
    "df_missing = pd.DataFrame({\n",
    "  'customer_id': missing_customer_ids,\n",
    "  'sequence': [[] for _ in range(len(missing_customer_ids))]\n",
    "})\n",
    "\n",
    "# Concatenate df_result and df_missing\n",
    "transactions_filtered = pd.concat([transactions_filtered, df_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:01.888163800Z",
     "start_time": "2023-11-18T16:03:01.344807300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sorting based on customer_ids in submission\n",
    "transactions_df = transactions_filtered.copy()\n",
    "transactions_df['customer_id'] = pd.Categorical(transactions_df['customer_id'], categories=customer_ids, ordered=True)\n",
    "transactions_df_sorted = transactions_df.sort_values(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:43.167978100Z",
     "start_time": "2023-11-18T16:03:43.152727500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "history_dataset = HistoryDataset(transactions_df_sorted.sequence.tolist())\n",
    "history_dataloader = DataLoader(history_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "counter = 0\n",
    "history_batches = []\n",
    "history_batch = []\n",
    "\n",
    "for idx, batch in enumerate(history_dataloader):\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    for i in range(12):\n",
    "        # Pass padded batches to the model\n",
    "        with torch.no_grad():\n",
    "            out = model(batch[:, -12:])\n",
    "            out = torch.argmax(out, dim=1).unsqueeze(1)\n",
    "\n",
    "        # Append model's output to each transaction in the batch\n",
    "        batch = torch.cat((batch, out), dim=1)\n",
    "    for i in range(batch.shape[0]):\n",
    "        preds.append(batch[i, -12:].tolist())\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-08T00:37:22.710732400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"rec_list.bin\", \"wb\") as f:\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-08T00:37:22.711733200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"rec_list.bin\", \"rb\") as f:\n",
    "    some_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T00:37:22.726754600Z",
     "start_time": "2023-11-08T00:37:22.712733100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(preds), len(some_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-08T00:37:22.713733700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions[\"article_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T01:32:25.419889300Z",
     "start_time": "2023-11-08T01:32:21.863104600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_preds = [' '.join(['0' + str(inverse_article_id_map.get(p, 706016001)) for p in ps]) for ps in preds]\n",
    "sub.prediction = _preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T01:33:06.418490400Z",
     "start_time": "2023-11-08T01:32:36.509053500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_name = 'lstm_model_submission_e25_fix1'\n",
    "sub.to_csv(f'{sub_name}.csv.gz', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
