{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd011469-040b-46ae-b0c7-ca57c682a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# make external scripts auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from experiment_template import *\n",
    "from LSTMRecommender import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ba89dec-cb25-4243-b9d7-a857d770e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '../data/'\n",
    "\n",
    "# make sure the same data preprocessing as in the radek notebook have been performed\n",
    "# (see 02 FE/DataProcessingRadek.ipynb)\n",
    "transactions = pd.read_parquet(BASE_PATH + 'transactions_train.parquet')\n",
    "customers = pd.read_parquet(BASE_PATH + 'customers.parquet')\n",
    "articles = pd.read_parquet(BASE_PATH + 'articles.parquet')\n",
    "sample_submission = pd.read_csv(BASE_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c72811db-7994-4aa4-ac87-e90fa7275b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generation of Radek notebook\n",
    "def get_data(data, test_week):\n",
    "    ### repurchase\n",
    "    # each week is seen as a basket\n",
    "    # the items bought in one basket, will be example for the next basket\n",
    "    # the items bought in the last basket, will be candidates for the test basket\n",
    "    c2weeks = data.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = data.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(data['customer_id'], data['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "\n",
    "    ### bestseller\n",
    "    # if a user bought an item in a given week, the 12 most popular items in the previous week are example for that week\n",
    "    # the best selling items in the last week are candidates for all users\n",
    "    mean_price = data \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    \n",
    "    sales = data \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    unique_transactions = data \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "\n",
    "    ### combine\n",
    "    d = data.copy()\n",
    "    d['purchased'] = True\n",
    "    \n",
    "    result = pd.concat([\n",
    "        d, candidates_last_purchase, candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(False, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "\n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result\n",
    "\n",
    "def get_examples(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week != test_week]\n",
    "\n",
    "def get_candidates(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week == test_week]\n",
    "\n",
    "def add_features(data):\n",
    "    columns_to_use = [\n",
    "        'article_id', \n",
    "        'prod_name',\n",
    "        'product_type_name',\n",
    "        'product_type_no', \n",
    "        'graphical_appearance_no', \n",
    "        'colour_group_code', \n",
    "        'perceived_colour_value_id',\n",
    "        'perceived_colour_master_id', \n",
    "        'department_no', \n",
    "        'index_code',\n",
    "        'index_group_no', \n",
    "        'section_no', \n",
    "        'garment_group_no', \n",
    "        'FN', \n",
    "        'Active',\n",
    "        'club_member_status', \n",
    "        'fashion_news_frequency', \n",
    "        'age', \n",
    "        'postal_code',\n",
    "        'bestseller_rank'\n",
    "    ]\n",
    "\n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on='customer_id')\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "\n",
    "    # features from assignment 2 could go here\n",
    "    customer_avg_price = transactions.groupby('customer_id')['price'].mean().to_frame('preferred_price')\n",
    "    result = pd.merge(result, customer_avg_price, how=\"left\", on=\"customer_id\")\n",
    "    \n",
    "    return result[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split into training and testing\n",
    "# one week is used for testing\n",
    "# a number of weeks leading up to the test week are used to train the ranker\n",
    "test_week = 105\n",
    "num_training_weeks = 10\n",
    "testing_weeks = np.arange(test_week-num_training_weeks, test_week)\n",
    "train_data = transactions[transactions.week.isin(testing_weeks)].reset_index(drop=True)\n",
    "\n",
    "### assemble training data (positive + negative examples)\n",
    "# each example has at least a customer_id, article_id and whether it was purchased or not (positive/negative)\n",
    "# add_features extracts and adds features to the examples\n",
    "train_examples = get_examples(train_data, test_week)\n",
    "X_train = add_features(train_examples)\n",
    "Y_train = train_examples['purchased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.149201\n",
      "[LightGBM] [Info] Total Bins 1430\n",
      "[LightGBM] [Info] Number of data points in the train set: 11381612, number of used features: 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "               bestseller_rank 0.99896\n",
      "                           age 0.00025\n",
      "                    article_id 0.00016\n",
      "              garment_group_no 0.00014\n",
      "                 department_no 0.00010\n",
      "               product_type_no 0.00009\n",
      "                    section_no 0.00007\n",
      "                   postal_code 0.00007\n",
      "            club_member_status 0.00007\n",
      "             colour_group_code 0.00005\n",
      "                     prod_name 0.00004\n",
      "             product_type_name 0.00000\n",
      "       graphical_appearance_no 0.00000\n",
      "        fashion_news_frequency 0.00000\n",
      "     perceived_colour_value_id 0.00000\n",
      "    perceived_colour_master_id 0.00000\n",
      "                index_group_no 0.00000\n",
      "                            FN 0.00000\n",
      "                        Active 0.00000\n",
      "                    index_code 0.00000\n"
     ]
    }
   ],
   "source": [
    "### fit ranker\n",
    "# training_groups tells LGBM that each (week, customer_id) combination is a seperate basket\n",
    "# !!! it is important that the training_examples are sorted according to week, customer_id for this to work\n",
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(X_train, Y_train, group=train_groups)\n",
    "print_importance(ranker, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "# candidates are generated similarly to the examples, only we don't know whether they are purchased\n",
    "# the same features are extracted and added\n",
    "# each candidate is scored by the ranker and predictions are generated using the highest scoring candidates\n",
    "test_candidates = get_candidates(train_data, test_week)\n",
    "X_test = add_features(test_candidates)\n",
    "predictions = get_predictions(test_candidates, X_test, ranker, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMRecommender(\n",
       "  (embedding): Embedding(132, 64)\n",
       "  (lstm): LSTM(64, 100, batch_first=True)\n",
       "  (fc): Linear(in_features=100, out_features=132, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# SEQUENCE_COLUMN = \"prod_name\"\n",
    "SEQUENCE_COLUMN = \"product_type_name\"\n",
    "PREDICTIONS = 6\n",
    "MOST_POPULAR_VALUE = articles[articles[\"article_id\"] == transactions[\"article_id\"] \\\n",
    "                              .value_counts().index[0]][SEQUENCE_COLUMN].item()\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "PADDING_ARTICLE = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "NUM_ARTICLES_IN_SEQUENCE = 12\n",
    "N_ARTICLES = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "model = LSTMRecommender(\n",
    "    input_dim=NUM_ARTICLES_IN_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    # Output dim is only the number of articles while n_articles is for the embedding and has to include the padding\n",
    "    n_articles=N_ARTICLES+1,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./models/{SEQUENCE_COLUMN}_model/LSTM_Model_Epoch_5.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get purchases in testing weeks as a list sorted by date\n",
    "# Merge articles into transactions\n",
    "df = pd.merge(transactions, articles, how='left', on='article_id')\n",
    "df = df.sort_values(by=['customer_id', 't_dat'], ascending=[True, True])\n",
    "\n",
    "# Group by \"customer_id\" and get the last 12 transactions for each customer\n",
    "df_grouped = df.groupby('customer_id')[SEQUENCE_COLUMN].apply(list)\n",
    "transactions_filtered = pd.DataFrame({\"customer_id\": df_grouped.index, \"sequence\": df_grouped.apply(lambda x: x[-12:])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataset = TransactionsDataset(transactions_filtered, PADDING_ARTICLE, NUM_ARTICLES_IN_SEQUENCE)\n",
    "history_dataloader = DataLoader(history_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9534b9c2c5d04b308f34a3c3722d9660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "counter = 0\n",
    "history_batches = []\n",
    "history_batch = []\n",
    "\n",
    "PRED_AMNT = 6\n",
    "\n",
    "for idx, sequences in enumerate(tqdm(history_dataloader)):\n",
    "    sequences = sequences.to(device)\n",
    "    \n",
    "    for i in range(PRED_AMNT):\n",
    "        # Pass padded batches to the model\n",
    "        with torch.no_grad():\n",
    "            out = model(sequences[:, -12:])\n",
    "            out = torch.argmax(out, dim=1).unsqueeze(1)\n",
    "            # out[out==PADDING_ARTICLE] = \n",
    "\n",
    "        # Append model's output to each transaction in the batch\n",
    "        sequences = torch.cat((sequences, out), dim=1)\n",
    "    for i in range(sequences.shape[0]):\n",
    "        preds.append(sequences[i, -PRED_AMNT:].tolist())\n",
    "transactions_filtered[\"preds\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_most_popular(predictions, k):\n",
    "    most_popular_k = df.groupby(SEQUENCE_COLUMN)[\"article_id\"].value_counts().reset_index(name=\"count\")\n",
    "    for idx, (customer_id, prediction) in tqdm(predictions.iterrows(), total=predictions.shape[0]):\n",
    "        new_preds = prediction[:-k]\n",
    "        lstm_preds = transactions_filtered[transactions_filtered[\"customer_id\"] == customer_id][\"preds\"].values[0]\n",
    "        for i in range(k):\n",
    "            cat_value = lstm_preds[i] if lstm_preds[i] != PADDING_ARTICLE else MOST_POPULAR_VALUE\n",
    "            for article in most_popular_k[most_popular_k[SEQUENCE_COLUMN] == cat_value][\"article_id\"].values:\n",
    "                if article not in new_preds:\n",
    "                    new_preds.append(article)\n",
    "                    break\n",
    "        if len(new_preds) != 12:\n",
    "            new_preds += prediction[-k:]\n",
    "            new_preds = new_preds[:12]\n",
    "        predictions.at[idx, \"prediction\"] = new_preds\n",
    "\n",
    "\n",
    "def replace_last_k(predictions, k):\n",
    "    most_popular_filtered = df.groupby([SEQUENCE_COLUMN, \"article_id\"]).size().reset_index(name=\"count\").sort_values([SEQUENCE_COLUMN, \"count\"], ascending=[True, False])\n",
    "    most_popular_filtered = most_popular_filtered.groupby(SEQUENCE_COLUMN)[\"article_id\"].apply(list).reset_index(name=\"articles\")\n",
    "    popular_articles_dict = dict(zip(most_popular_filtered[SEQUENCE_COLUMN], most_popular_filtered[\"articles\"]))\n",
    "\n",
    "    # lstm preds to dict\n",
    "    lstm_prediction_dict = transactions_filtered.set_index(\"customer_id\")[\"preds\"].to_dict()\n",
    "\n",
    "    for idx, (customer_id, prediction) in tqdm(predictions.iterrows(), total=predictions.shape[0]):\n",
    "        new_preds = prediction[:-k]\n",
    "        lstm_preds = lstm_prediction_dict.get(customer_id, [])\n",
    "        for i in range(k):\n",
    "            cat_value = lstm_preds[i] if i < len(lstm_preds) and lstm_preds[i] != PADDING_ARTICLE else MOST_POPULAR_VALUE\n",
    "            for article in popular_articles_dict.get(cat_value, []):\n",
    "                if article not in new_preds:\n",
    "                    new_preds.append(article)\n",
    "                    break\n",
    "\n",
    "        if len(new_preds) != 12:\n",
    "            new_preds += prediction[-k:]\n",
    "            new_preds = new_preds[:12]\n",
    "        predictions.at[idx, \"prediction\"] = new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28847241659200</td>\n",
       "      <td>[925246001, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41318098387474</td>\n",
       "      <td>[868879003, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116809474287335</td>\n",
       "      <td>[906305002, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200292573348128</td>\n",
       "      <td>[903861001, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248294615847351</td>\n",
       "      <td>[720504008, 337991001, 878987003, 471714002, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id                                         prediction\n",
       "0   28847241659200  [925246001, 924243002, 918522001, 924243001, 9...\n",
       "1   41318098387474  [868879003, 924243002, 918522001, 924243001, 9...\n",
       "2  116809474287335  [906305002, 924243002, 918522001, 924243001, 9...\n",
       "3  200292573348128  [903861001, 924243002, 918522001, 924243001, 9...\n",
       "4  248294615847351  [720504008, 337991001, 878987003, 471714002, 9..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f67db2960254cccbe1b814050f81159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/437365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# replace_most_popular(predictions, 5)\n",
    "replace_last_k(predictions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28847241659200</td>\n",
       "      <td>[925246001, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41318098387474</td>\n",
       "      <td>[868879003, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116809474287335</td>\n",
       "      <td>[906305002, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200292573348128</td>\n",
       "      <td>[903861001, 924243002, 918522001, 924243001, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248294615847351</td>\n",
       "      <td>[720504008, 337991001, 878987003, 471714002, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id                                         prediction\n",
       "0   28847241659200  [925246001, 924243002, 918522001, 924243001, 9...\n",
       "1   41318098387474  [868879003, 924243002, 918522001, 924243001, 9...\n",
       "2  116809474287335  [906305002, 924243002, 918522001, 924243001, 9...\n",
       "3  200292573348128  [903861001, 924243002, 918522001, 924243001, 9...\n",
       "4  248294615847351  [720504008, 337991001, 878987003, 471714002, 9..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "predictions = fill_missing_predictions(predictions, customers.customer_id, popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = create_submission(predictions, sample_submission)\n",
    "sub.to_csv(BASE_PATH + 'sub1.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e22bfaf-d1dd-4ea8-9afc-6f4332be82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### evaluate\n",
    "# if test_week < transactions.week.max() + 1:\n",
    "#     # get ground truth data for test week\n",
    "#     purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    \n",
    "#     # fill missing prediction for customers in test set with popular items in last week\n",
    "#     # only for customers in test set because only those are evaluated\n",
    "#     popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "#     predictions = fill_missing_predictions(predictions, purchases.customer_id, popular)\n",
    "    \n",
    "#     # calculate score\n",
    "#     score = mean_average_precision(predictions, purchases, 12)\n",
    "#     print(score)\n",
    "\n",
    "# ### submit\n",
    "# else:\n",
    "#     # fill missing predictions for all customers with popular items in last week\n",
    "#     # all customers because we don't know which ones will be evaluated\n",
    "#     popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "#     predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "#     # write submission\n",
    "#     sub = create_submission(predictions)\n",
    "#     sub.to_csv(BASE_PATH + 'sub1.csv.gz', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
