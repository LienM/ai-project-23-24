{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:27.839690200Z",
     "start_time": "2023-11-18T16:01:27.776245400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os;os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:29.735280400Z",
     "start_time": "2023-11-18T16:01:27.779701200Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:29.735280400Z",
     "start_time": "2023-11-18T16:01:29.731574400Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 20193575\n",
    "SEQUENCE_COLUMN = \"prod_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:29.755887900Z",
     "start_time": "2023-11-18T16:01:29.734280300Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:32.571811400Z",
     "start_time": "2023-11-18T16:01:29.756887600Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = pd.read_parquet('../data/articles.parquet')\n",
    "customers = pd.read_parquet('../data/customers.parquet')\n",
    "transactions = pd.read_parquet('../data/transactions_train.parquet')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.merge(transactions, articles[[\"article_id\", \"prod_name\", \"product_type_name\"]], on=\"article_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:33.865976200Z",
     "start_time": "2023-11-18T16:01:32.572814700Z"
    }
   },
   "outputs": [],
   "source": [
    "article_id_map = {original: (idx + 1) for idx, original in enumerate(articles[\"article_id\"].unique())}\n",
    "inverse_article_id_map = {(idx + 1): original for idx, original in enumerate(articles[\"article_id\"].unique())}\n",
    "articles[\"article_id_mapped\"] = articles[\"article_id\"].map(article_id_map)\n",
    "transactions[\"article_id_mapped\"] = transactions[\"article_id\"].map(article_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:33.894010800Z",
     "start_time": "2023-11-18T16:01:33.866976700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id                      105542\n",
       "product_code                     47224\n",
       "prod_name                        45875\n",
       "product_type_no                    132\n",
       "product_type_name                  131\n",
       "product_group_name                  19\n",
       "graphical_appearance_no             30\n",
       "graphical_appearance_name           30\n",
       "colour_group_code                   50\n",
       "colour_group_name                   50\n",
       "perceived_colour_value_id            8\n",
       "perceived_colour_value_name          8\n",
       "perceived_colour_master_id          20\n",
       "perceived_colour_master_name        20\n",
       "department_no                      299\n",
       "department_name                    250\n",
       "index_code                          10\n",
       "index_name                          10\n",
       "index_group_no                       5\n",
       "index_group_name                     5\n",
       "section_no                          57\n",
       "section_name                        56\n",
       "garment_group_no                    21\n",
       "garment_group_name                  21\n",
       "detail_desc                      43405\n",
       "article_id_mapped               105542\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:33.899874300Z",
     "start_time": "2023-11-18T16:01:33.892009400Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMRecommender(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_dim, hidden_dim, n_articles, num_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_articles = n_articles\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding articles to a lower dimension\n",
    "        self.embedding = nn.Embedding(n_articles, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, n_articles)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_directions * self.num_layers, x.size(0), self.hidden_dim, requires_grad=True, device=device)\n",
    "        c0 = torch.zeros(self.n_directions * self.num_layers, x.size(0), self.hidden_dim, requires_grad=True, device=device)\n",
    "        # Embed\n",
    "        embedded_sequence = self.embedding(x)\n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(embedded_sequence, (h0.detach(), c0.detach()))\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "        # out = F.softmax(out, dim=1)\n",
    "        # return torch.max(out, dim=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:36.406481700Z",
     "start_time": "2023-11-18T16:01:33.898874200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split 80/20 on customer id\n",
    "train_customers, val_customers = train_test_split(transactions.customer_id.unique(), test_size=0.2, random_state=SEED)\n",
    "training_transactions_df = transactions[transactions.customer_id.isin(train_customers)]\n",
    "validation_transactions_df = transactions[transactions.customer_id.isin(val_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:36.409939800Z",
     "start_time": "2023-11-18T16:01:36.407480900Z"
    }
   },
   "outputs": [],
   "source": [
    "N_TRAINING_WEEKS = 5\n",
    "MAX_WEEK = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:36.424677800Z",
     "start_time": "2023-11-18T16:01:36.409939800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_transactions_by_weeks(transactions, column=\"article_id_mapped\"):\n",
    "    _transactions = transactions[[\"customer_id\", column, \"week\"]]\n",
    "    filtered_transactions = _transactions[_transactions.week.between(MAX_WEEK - N_TRAINING_WEEKS, MAX_WEEK - 1)]\n",
    "    filtered_transactions = filtered_transactions.groupby(\"customer_id\")[column].apply(list).reset_index(name=\"history\")\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:39.078819600Z",
     "start_time": "2023-11-18T16:01:36.425676900Z"
    }
   },
   "outputs": [],
   "source": [
    "training_transactions = filter_transactions_by_weeks(training_transactions_df, SEQUENCE_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:39.682684300Z",
     "start_time": "2023-11-18T16:01:39.079820900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_transactions = filter_transactions_by_weeks(validation_transactions_df, SEQUENCE_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:40.664591700Z",
     "start_time": "2023-11-18T16:01:39.684684500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.362281e+06\n",
       "mean     2.333463e+01\n",
       "std      3.924225e+01\n",
       "min      1.000000e+00\n",
       "25%      3.000000e+00\n",
       "50%      9.000000e+00\n",
       "75%      2.700000e+01\n",
       "max      1.895000e+03\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.customer_id.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:41.900947600Z",
     "start_time": "2023-11-18T16:01:40.664591700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_transactions_df.customer_id.value_counts().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.431461200Z",
     "start_time": "2023-11-18T16:01:41.899540400Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Padding article added to map, actual articles start at 1\n",
    "article_id_map[-1] = 0\n",
    "PADDING_ARTICLE = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "NUM_ARTICLES_IN_SEQUENCE = 12\n",
    "N_ARTICLES = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "model = LSTMRecommender(\n",
    "    input_dim=NUM_ARTICLES_IN_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    # Output dim is only the number of articles while n_articles is for the embedding and has to include the padding\n",
    "    n_articles=N_ARTICLES+1,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    "    )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.442080600Z",
     "start_time": "2023-11-18T16:01:42.432463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    105542.000000\n",
       "mean      13482.907828\n",
       "std       12937.813422\n",
       "min           0.000000\n",
       "25%        2508.000000\n",
       "50%        9172.000000\n",
       "75%       21221.750000\n",
       "max       45874.000000\n",
       "Name: prod_name, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[SEQUENCE_COLUMN].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.456433400Z",
     "start_time": "2023-11-18T16:01:42.437978700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45874"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[SEQUENCE_COLUMN].max() - articles[SEQUENCE_COLUMN].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.469793800Z",
     "start_time": "2023-11-18T16:01:42.455434Z"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "   def __init__(self, sequences, targets):\n",
    "       self.sequences = sequences\n",
    "       self.targets = targets\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.sequences)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "def combine_sequences(user_transactions):\n",
    "    combined_sequence_batch = []\n",
    "    combined_target_batch = []\n",
    "    \n",
    "    for idx, (customer, history) in user_transactions.iterrows():\n",
    "        history_batch, target_batch = create_batch(history)\n",
    "        if history_batch is None or target_batch is None:\n",
    "            continue\n",
    "        combined_sequence_batch.extend(history_batch)\n",
    "        combined_target_batch.extend(target_batch)\n",
    "    \n",
    "    sequence_dataset = SequenceDataset(combined_sequence_batch, combined_target_batch)\n",
    "    dataloader = DataLoader(sequence_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def create_batch(history):\n",
    "    # Create batch of sequences\n",
    "    if len(history) <= 1:\n",
    "        return None, None\n",
    "    history_batch = []\n",
    "    target_batch = []\n",
    "    for i in range(1, len(history)):\n",
    "        if i < 12:\n",
    "            # Add padding to the beginning of the sequence\n",
    "            history_batch.append(torch.tensor([PADDING_ARTICLE] * (NUM_ARTICLES_IN_SEQUENCE - i) + history[:i], dtype=torch.int32))\n",
    "        else:\n",
    "            history_batch.append(torch.tensor(history[i-12:i], dtype=torch.int32))\n",
    "        target_batch.append(torch.tensor(history[i], dtype=torch.float32, requires_grad=True))\n",
    "    return history_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.485764Z",
     "start_time": "2023-11-18T16:01:42.460853500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116809474287335</td>\n",
       "      <td>[7046, 19554, 2790, 7546, 4954, 20631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200292573348128</td>\n",
       "      <td>[3732]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>329094189075899</td>\n",
       "      <td>[4368, 4368]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>690285180337957</td>\n",
       "      <td>[7046, 7046, 7046]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>745180086074610</td>\n",
       "      <td>[28106, 18449, 29373, 195, 126, 8959, 9507, 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id                                            history\n",
       "0  116809474287335             [7046, 19554, 2790, 7546, 4954, 20631]\n",
       "1  200292573348128                                             [3732]\n",
       "2  329094189075899                                       [4368, 4368]\n",
       "3  690285180337957                                 [7046, 7046, 7046]\n",
       "4  745180086074610  [28106, 18449, 29373, 195, 126, 8959, 9507, 22..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.487763400Z",
     "start_time": "2023-11-18T16:01:42.467793700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    y_true, y_pred = [], []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dataloader = combine_sequences(validation_transactions)\n",
    "        for sequences, targets in dataloader:\n",
    "            sequence = sequences.to(device)\n",
    "            target = targets.to(device).long()\n",
    "            \n",
    "            # Predict\n",
    "            out = model(sequence)\n",
    "            loss = criterion(out, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(out, dim=1)\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "        \n",
    "    val_loss /= len(dataloader)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    return accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T15:45:57.864590900Z",
     "start_time": "2023-11-18T15:21:26.725958300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218069\n",
      "Training start: Sun Nov 19 19:36:11 2023\n",
      "+-------+---------------------+-------------------+-------------------------+--------------------+----------------------+------------+\n",
      "| Epoch | Validation Accuracy | Training Accuracy |     Validation Loss     |     Epoch Loss     |     Running Loss     | Epoch Time |\n",
      "+-------+---------------------+-------------------+-------------------------+--------------------+----------------------+------------+\n",
      "| 1     | 0.8119%             | 0.4151%           | 8.1181                  | 8.9986             | 8.9986               | 39.21s     |\n",
      "| 2     | 0.8360%             | 0.8126%           | 7.9904                  | 8.0108             | 17.0094              | 49.12s     |\n",
      "| 3     | 0.8567%             | 0.8070%           | 7.9836                  | 7.9558             | 24.9652              | 38.23s     |\n",
      "| 4     | 0.9563%             | 0.8721%           | 7.9750                  | 7.9441             | 32.9092              | 36.34s     |\n",
      "| 5     | 1.0376%             | 0.9630%           | 7.9653                  | 7.9327             | 40.8419              | 35.85s     |\n",
      "| 6     | 1.1906%             | 1.0768%           | 7.9536                  | 7.9209             | 48.7628              | 36.45s     |\n",
      "| 7     | 1.2527%             | 1.1766%           | 7.9414                  | 7.9069             | 56.6698              | 37.46s     |\n",
      "| 8     | 1.3302%             | 1.2639%           | 7.9285                  | 7.8935             | 64.5633              | 37.29s     |\n",
      "| 9     | 1.4385%             | 1.3383%           | 7.9172                  | 7.8810             | 72.4443              | 36.07s     |\n",
      "| 10    | 1.5097%             | 1.4412%           | 7.9053                  | 7.8671             | 80.3114              | 36.83s     |\n",
      "| 11    | 1.6084%             | 1.5561%           | 7.8931                  | 7.8538             | 88.1652              | 37.24s     |\n",
      "| 12    | 1.7224%             | 1.6738%           | 7.8811                  | 7.8382             | 96.0033              | 36.54s     |\n",
      "| 13    | 1.8745%             | 1.8019%           | 7.8668                  | 7.8232             | 103.8266             | 37.14s     |\n",
      "| 14    | 2.0203%             | 1.9616%           | 7.8534                  | 7.8080             | 111.6345             | 36.08s     |\n",
      "| 15    | 2.1715%             | 2.1188%           | 7.8392                  | 7.7922             | 119.4267             | 36.44s     |\n",
      "| 16    | 2.3616%             | 2.3148%           | 7.8249                  | 7.7749             | 127.2016             | 36.47s     |\n",
      "| 17    | 2.5839%             | 2.5182%           | 7.8107                  | 7.7584             | 134.9600             | 36.71s     |\n",
      "| 18    | 2.8390%             | 2.7376%           | 7.7954                  | 7.7406             | 142.7007             | 36.33s     |\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "print(len(training_transactions))\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "print(\"Training start:\", time.asctime(time.localtime()))\n",
    "\n",
    "header_printed = False\n",
    "col_widths = []\n",
    "table_seperator = \"\"\n",
    "\n",
    "running_loss = 0.0\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "model.to(device)\n",
    "dataloader = combine_sequences(training_transactions)\n",
    "len(dataloader)\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    \n",
    "    training_accuracy = 0\n",
    "    epoch_loss = 0.0\n",
    "    for idx, (sequence, target) in enumerate(dataloader):\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device).long()\n",
    "        # Predict\n",
    "        out = model(sequence)\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(\n",
    "            # torch.tensor(out, dtype=torch.float32, requires_grad=True), torch.tensor(target[0], dtype=torch.float32, requires_grad=True)\n",
    "            # out.to(torch.float32).clone().detach().requires_grad_(True), target\n",
    "            out, target\n",
    "        )\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        predicted = torch.argmax(out, dim=1)\n",
    "        epoch_loss += loss.item()\n",
    "        training_accuracy += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    running_loss += epoch_loss\n",
    "    training_accuracy /= len(dataloader.dataset)\n",
    "    \n",
    "    val_accuracy, val_loss = run_validation()\n",
    "    \n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(float(val_loss))\n",
    "    \n",
    "    if not header_printed:\n",
    "        header_printed = True\n",
    "        header_text = \"| Epoch | Validation Accuracy | Training Accuracy |     Validation Loss     |     Epoch Loss     |     Running Loss     | Epoch Time |\"\n",
    "        col_widths = [len(s)-2 for s in header_text.split(\"|\")[1:-1]]\n",
    "        table_seperator = f\"+{'+'.join(['-' * (x + 2) for x in col_widths])}+\"\n",
    "        print(table_seperator)\n",
    "        print(header_text)\n",
    "        print(table_seperator)\n",
    "    \n",
    "    print(f\"| {str(epoch + 1):<{col_widths[0]}} | \"\n",
    "              f\"{f'{val_accuracy:.4%}':<{col_widths[1]}} | \"\n",
    "              f\"{f'{training_accuracy:.4%}':<{col_widths[2]}} | \"\n",
    "              f\"{f'{val_loss:.4f}':<{col_widths[3]}} | \"\n",
    "              f\"{f'{epoch_loss:.4f}':<{col_widths[4]}} | \"\n",
    "              f\"{f'{running_loss:.4f}':<{col_widths[5]}} | \"\n",
    "              f\"{f'{time.perf_counter() - epoch_start_time:.2f}s':<{col_widths[6]}} |\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"./models/LSTM_Model_Epoch_{epoch + 1}.pt\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Training time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:42.487763400Z",
     "start_time": "2023-11-18T16:01:42.471853Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:01:44.497680500Z",
     "start_time": "2023-11-18T16:01:42.474962Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:22.848548400Z",
     "start_time": "2023-11-18T16:02:22.670128800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMRecommender(\n",
    "    input_dim=NUM_ARTICLES_IN_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    # Output dim is only the number of articles while n_articles is for the embedding and has to include the padding\n",
    "    n_articles=N_ARTICLES+1,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:28.680832900Z",
     "start_time": "2023-11-18T16:02:26.797893300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./models/LSTM_Model_Epoch_25.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:30.801553700Z",
     "start_time": "2023-11-18T16:02:30.787153600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:02:32.757889700Z",
     "start_time": "2023-11-18T16:02:32.736819200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HistoryDataset(Dataset):\n",
    "    def __init__(self, history):\n",
    "        self.histories = history\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.histories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        history = self.histories[idx]\n",
    "        if len(history) < 12:\n",
    "            history = [PADDING_ARTICLE] * (NUM_ARTICLES_IN_SEQUENCE - len(history)) + history\n",
    "        return torch.tensor(history[-12:], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:00.350668500Z",
     "start_time": "2023-11-18T16:02:35.170346700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the dataframe by \"customer_id\" and \"t_dat\" in descending order\n",
    "df = transactions.sort_values(by=['customer_id', 't_dat'], ascending=[True, True])\n",
    "\n",
    "# Group by \"customer_id\" and get the last 12 transactions for each customer\n",
    "df_grouped = df.groupby('customer_id')['article_id_mapped'].apply(list)\n",
    "transactions_filtered = pd.DataFrame({\"customer_id\": df_grouped.index, \"sequence\": df_grouped.apply(lambda x: x[-12:])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:00.867631700Z",
     "start_time": "2023-11-18T16:03:00.350668500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert customer ids to integers\n",
    "customer_ids = sub.customer_id.apply(hex_id_to_int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:01.305242600Z",
     "start_time": "2023-11-18T16:03:00.867631700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_customer_ids = list(set(customer_ids).difference(set(transactions_filtered.customer_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:01.343808Z",
     "start_time": "2023-11-18T16:03:01.305242600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with the missing customer_ids and an empty list as the sequence\n",
    "df_missing = pd.DataFrame({\n",
    "  'customer_id': missing_customer_ids,\n",
    "  'sequence': [[] for _ in range(len(missing_customer_ids))]\n",
    "})\n",
    "\n",
    "# Concatenate df_result and df_missing\n",
    "transactions_filtered = pd.concat([transactions_filtered, df_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:01.888163800Z",
     "start_time": "2023-11-18T16:03:01.344807300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sorting based on customer_ids in submission\n",
    "transactions_df = transactions_filtered.copy()\n",
    "transactions_df['customer_id'] = pd.Categorical(transactions_df['customer_id'], categories=customer_ids, ordered=True)\n",
    "transactions_df_sorted = transactions_df.sort_values(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T16:03:43.167978100Z",
     "start_time": "2023-11-18T16:03:43.152727500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "history_dataset = HistoryDataset(transactions_df_sorted.sequence.tolist())\n",
    "history_dataloader = DataLoader(history_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "counter = 0\n",
    "history_batches = []\n",
    "history_batch = []\n",
    "\n",
    "for idx, batch in enumerate(history_dataloader):\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    for i in range(12):\n",
    "        # Pass padded batches to the model\n",
    "        with torch.no_grad():\n",
    "            out = model(batch[:, -12:])\n",
    "            out = torch.argmax(out, dim=1).unsqueeze(1)\n",
    "\n",
    "        # Append model's output to each transaction in the batch\n",
    "        batch = torch.cat((batch, out), dim=1)\n",
    "    for i in range(batch.shape[0]):\n",
    "        preds.append(batch[i, -12:].tolist())\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-08T00:37:22.710732400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"rec_list.bin\", \"wb\") as f:\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-08T00:37:22.711733200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"rec_list.bin\", \"rb\") as f:\n",
    "    some_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T00:37:22.726754600Z",
     "start_time": "2023-11-08T00:37:22.712733100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(preds), len(some_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-08T00:37:22.713733700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions[\"article_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T01:32:25.419889300Z",
     "start_time": "2023-11-08T01:32:21.863104600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_preds = [' '.join(['0' + str(inverse_article_id_map.get(p, 706016001)) for p in ps]) for ps in preds]\n",
    "sub.prediction = _preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T01:33:06.418490400Z",
     "start_time": "2023-11-08T01:32:36.509053500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_name = 'lstm_model_submission_e25_fix1'\n",
    "sub.to_csv(f'{sub_name}.csv.gz', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
