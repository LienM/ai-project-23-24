{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook was originally written by Noah Daniels implementing Radek's LGBMRanker algorithm for this competition\n",
    "I adapted this notebook to work with my LSTM model and extra engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd011469-040b-46ae-b0c7-ca57c682a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# make external scripts auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from experiment_template import *\n",
    "from LSTMRecommender import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba89dec-cb25-4243-b9d7-a857d770e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '../data/'\n",
    "\n",
    "# make sure the same data preprocessing as in the radek notebook have been performed\n",
    "# (see 02 FE/DataProcessingRadek.ipynb)\n",
    "transactions = pd.read_parquet(BASE_PATH + 'transactions_train.parquet')\n",
    "customers = pd.read_parquet(BASE_PATH + 'customers.parquet')\n",
    "articles = pd.read_parquet(BASE_PATH + 'articles.parquet')\n",
    "sample_submission = pd.read_csv(BASE_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMRecommender(\n",
       "  (embedding): Embedding(45876, 64)\n",
       "  (lstm): LSTM(64, 100, batch_first=True)\n",
       "  (fc): Linear(in_features=100, out_features=45876, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEQUENCE_COLUMN = \"prod_name\"\n",
    "# SEQUENCE_COLUMN = \"product_type_name\"\n",
    "N_PREDICTIONS = 6\n",
    "MOST_POPULAR_VALUE = articles[articles[\"article_id\"] == transactions[\"article_id\"] \\\n",
    "                              .value_counts().index[0]][SEQUENCE_COLUMN].item()\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "PADDING_ARTICLE = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "NUM_ARTICLES_IN_SEQUENCE = 12\n",
    "N_ARTICLES = articles[SEQUENCE_COLUMN].nunique()\n",
    "\n",
    "model = LSTMRecommender(\n",
    "    input_dim=NUM_ARTICLES_IN_SEQUENCE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    # Output dim is only the number of articles while n_articles is for the embedding and has to include the padding\n",
    "    n_articles=N_ARTICLES+1,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./models/{SEQUENCE_COLUMN}_model/LSTM_Model_Epoch_19.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequence(sequence, max_length=12, padding_value=PADDING_ARTICLE):\n",
    "    if len(sequence) > max_length:\n",
    "        return sequence[-max_length:]\n",
    "    else:\n",
    "        return [padding_value] * (max_length - len(sequence)) + sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lstm_ranker_features(data, test_week, lstm_data):\n",
    "    data = pd.merge(data, articles, how='left', on='article_id')\n",
    "    model.eval()\n",
    "    \n",
    "    data[\"lstm_score\"] = 0.0\n",
    "\n",
    "    # Define batches\n",
    "    unique_customers = data[\"customer_id\"].unique()\n",
    "    n_batches = int(np.ceil(len(unique_customers) / BATCH_SIZE))\n",
    "\n",
    "    for batch_idx in tqdm(range(n_batches)):\n",
    "        batch_customers = unique_customers[batch_idx * BATCH_SIZE : (batch_idx + 1) * BATCH_SIZE]\n",
    "        sequences = []\n",
    "        index_to_seq_map = []\n",
    "\n",
    "        for customer_id in batch_customers:\n",
    "            customer_data = data[data[\"customer_id\"] == customer_id]\n",
    "            for week in customer_data[\"week\"].unique():\n",
    "                previous_weeks = customer_data[customer_data[\"week\"] < week].copy()\n",
    "                previous_weeks.sort_values(\"t_dat\", inplace=True)\n",
    "                sequence = get_padded_sequence(lstm_data.get(customer_id, []) + (previous_weeks[SEQUENCE_COLUMN].tolist()))\n",
    "                sequences.append(sequence)\n",
    "\n",
    "                current_week_indices = customer_data[customer_data['week'] == week].index\n",
    "                for idx in current_week_indices:\n",
    "                    index_to_seq_map.append((idx, len(sequences) - 1))\n",
    "            \n",
    "        input_tensor = torch.tensor(sequences, dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        for idx, seq_idx in index_to_seq_map:\n",
    "            col_record = data.at[idx, SEQUENCE_COLUMN]\n",
    "            data.loc[idx, \"lstm_score\"] = probs[seq_idx, col_record].item()\n",
    "    # data.week += 1\n",
    "\n",
    "    return data[[\"customer_id\", \"week\", \"article_id\", \"lstm_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72811db-7994-4aa4-ac87-e90fa7275b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generation of Radek notebook\n",
    "def get_data(data, test_week):\n",
    "    ### Add lstm score\n",
    "    candidates_lstm = add_lstm_ranker_features(data, test_week, lstm_data)\n",
    "    data[\"lstm_score\"] = candidates_lstm[\"lstm_score\"]\n",
    "    \n",
    "    ### repurchase\n",
    "    # each week is seen as a basket\n",
    "    # the items bought in one basket, will be example for the next basket\n",
    "    # the items bought in the last basket, will be candidates for the test basket\n",
    "    c2weeks = data.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = data.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(data['customer_id'], data['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "\n",
    "    ### bestseller\n",
    "    # if a user bought an item in a given week, the 12 most popular items in the previous week are example for that week\n",
    "    # the best selling items in the last week are candidates for all users\n",
    "    mean_price = data \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    \n",
    "    sales = data \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    unique_transactions = data \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "\n",
    "    ### combine\n",
    "    d = data.copy()\n",
    "    d['purchased'] = True\n",
    "    \n",
    "    result = pd.concat([\n",
    "        d, candidates_last_purchase, candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(False, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "    result[\"lstm_score\"].fillna(0.0, inplace=True)\n",
    "    result[\"lstm_score\"] = result[\"lstm_score\"] / result[\"lstm_score\"].max()\n",
    "\n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result\n",
    "\n",
    "def get_examples(data, test_week):\n",
    "    return data[data.week != test_week]\n",
    "\n",
    "def get_candidates(data, test_week):\n",
    "    return data[data.week == test_week]\n",
    "\n",
    "def add_features(data):\n",
    "    columns_to_use = [\n",
    "        'article_id', \n",
    "        'prod_name',\n",
    "        'product_type_name',\n",
    "        'product_type_no', \n",
    "        'graphical_appearance_no', \n",
    "        'colour_group_code', \n",
    "        'perceived_colour_value_id',\n",
    "        'perceived_colour_master_id', \n",
    "        'department_no', \n",
    "        'index_code',\n",
    "        'index_group_no', \n",
    "        'section_no', \n",
    "        'garment_group_no', \n",
    "        'FN', \n",
    "        'Active',\n",
    "        'club_member_status', \n",
    "        'fashion_news_frequency', \n",
    "        'age', \n",
    "        'postal_code',\n",
    "        'bestseller_rank',\n",
    "        'discount',\n",
    "        'price',\n",
    "        'price_sensitivity',\n",
    "        \"lstm_score\"\n",
    "    ]\n",
    "\n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on='customer_id')\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "\n",
    "    # features from assignment 2 could go here\n",
    "\n",
    "    # Clipping price\n",
    "    result[\"price\"] = result[\"price\"].clip(0, 0.17)\n",
    "\n",
    "    customer_avg_price = transactions.groupby('customer_id')['price'].mean().to_frame('preferred_price')\n",
    "    result = pd.merge(result, customer_avg_price, how=\"left\", on=\"customer_id\")\n",
    "\n",
    "    # Customer price senstivity\n",
    "    customer_data = result.groupby('customer_id')\n",
    "    price_sensitivity = customer_data['price'].std() / customer_data['price'].mean()\n",
    "    price_sensitivity_df = pd.DataFrame({'customer_id': price_sensitivity.index, 'price_sensitivity': price_sensitivity.values})\n",
    "    result = pd.merge(result, price_sensitivity_df, on='customer_id', how='left')\n",
    "\n",
    "    max_price = transactions.groupby(\"article_id\")[\"price\"].max().reset_index().rename(columns={\"price\": \"max_price\"})\n",
    "    result[\"discount\"] = 1 - result[\"price\"] / result.merge(max_price, on=\"article_id\", how=\"left\")[\"max_price\"]\n",
    "\n",
    "    \n",
    "    return result[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split into training and testing\n",
    "# one week is used for testing\n",
    "# a number of weeks leading up to the test week are used to train the ranker\n",
    "test_week = 105\n",
    "num_training_weeks = 10\n",
    "testing_weeks = np.arange(test_week-num_training_weeks, test_week)\n",
    "train_data = transactions[transactions.week.isin(testing_weeks)].reset_index(drop=True)\n",
    "\n",
    "# Get 5 weeks before first training week for lstm\n",
    "lstm_weeks = np.arange(test_week-num_training_weeks-5, test_week-num_training_weeks)\n",
    "lstm_data = transactions[transactions.week.isin(lstm_weeks)].reset_index(drop=True)\n",
    "lstm_data = lstm_data.merge(articles, how='left', on='article_id')\n",
    "lstm_data = lstm_data.groupby('customer_id')[SEQUENCE_COLUMN].apply(list).reset_index(name='history').set_index('customer_id')['history'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assemble training data (positive + negative examples)\n",
    "each example has at least a customer_id, article_id and whether it was purchased or not (positive/negative)\n",
    "\n",
    "add_features extracts and adds features to the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(train_data, test_week)\n",
    "pickle.dump(data, open(f\"./data_lstm_{SEQUENCE_COLUMN}_{test_week}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(f\"./data_lstm_{SEQUENCE_COLUMN}_{test_week}.pkl\", \"rb\"))\n",
    "train_examples = get_examples(data, test_week)\n",
    "X_train = add_features(train_examples)\n",
    "Y_train = train_examples['purchased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.139723\n",
      "[LightGBM] [Info] Total Bins 1755\n",
      "[LightGBM] [Info] Number of data points in the train set: 11557594, number of used features: 21\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "               bestseller_rank 0.99602\n",
      "                    lstm_score 0.00255\n",
      "                     prod_name 0.00043\n",
      "              garment_group_no 0.00041\n",
      "                    article_id 0.00022\n",
      "                           age 0.00011\n",
      "               product_type_no 0.00009\n",
      "            club_member_status 0.00005\n",
      "                   postal_code 0.00004\n",
      "        fashion_news_frequency 0.00003\n",
      "             product_type_name 0.00003\n",
      "                        Active 0.00000\n",
      "                            FN 0.00000\n",
      "                    section_no 0.00000\n",
      "                    index_code 0.00000\n",
      "                 department_no 0.00000\n",
      "    perceived_colour_master_id 0.00000\n",
      "     perceived_colour_value_id 0.00000\n",
      "             colour_group_code 0.00000\n",
      "       graphical_appearance_no 0.00000\n",
      "                index_group_no 0.00000\n"
     ]
    }
   ],
   "source": [
    "### fit ranker\n",
    "# training_groups tells LGBM that each (week, customer_id) combination is a seperate basket\n",
    "# !!! it is important that the training_examples are sorted according to week, customer_id for this to work\n",
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(X_train, Y_train, group=train_groups)\n",
    "print_importance(ranker, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "# candidates are generated similarly to the examples, only we don't know whether they are purchased\n",
    "# the same features are extracted and added\n",
    "# each candidate is scored by the ranker and predictions are generated using the highest scoring candidates\n",
    "test_candidates = get_candidates(data, test_week)\n",
    "X_test = add_features(test_candidates)\n",
    "predictions = get_predictions(test_candidates, X_test, ranker, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM prediction replacement part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get purchases in testing weeks as a list sorted by date\n",
    "# Merge articles into transactions\n",
    "df = pd.merge(transactions, articles, how='left', on='article_id')\n",
    "df = df.sort_values(by=['customer_id', 't_dat'], ascending=[True, True])\n",
    "\n",
    "# Group by \"customer_id\" and get the last 12 transactions for each customer\n",
    "df_grouped = df.groupby('customer_id')[SEQUENCE_COLUMN].apply(list)\n",
    "transactions_filtered = pd.DataFrame({\"customer_id\": df_grouped.index, \"sequence\": df_grouped.apply(lambda x: x[-12:])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataset = TransactionsDataset(transactions_filtered, PADDING_ARTICLE, NUM_ARTICLES_IN_SEQUENCE)\n",
    "history_dataloader = DataLoader(history_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lstm(logits, temperature=1.0):\n",
    "    scaled = logits / temperature\n",
    "    probabilities = F.softmax(scaled, dim=1)\n",
    "    return torch.multinomial(probabilities, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "counter = 0\n",
    "history_batches = []\n",
    "history_batch = []\n",
    "\n",
    "for idx, sequences in enumerate(tqdm(history_dataloader)):\n",
    "    sequences = sequences.to(device)\n",
    "    \n",
    "    for i in range(N_PREDICTIONS):\n",
    "        # Pass padded batches to the model\n",
    "        with torch.no_grad():\n",
    "            out = model(sequences[:, -12:])\n",
    "            out = sample_lstm(out, temperature=0.1)\n",
    "            # out = torch.argmax(out, dim=1).unsqueeze(1)\n",
    "\n",
    "        # Append model's output to each transaction in the batch\n",
    "        sequences = torch.cat((sequences, out), dim=1)\n",
    "    for i in range(sequences.shape[0]):\n",
    "        preds.append(sequences[i, -N_PREDICTIONS:].tolist())\n",
    "transactions_filtered[\"preds\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_last_k(predictions, k):\n",
    "    most_popular_filtered = df.groupby([SEQUENCE_COLUMN, \"article_id\"]).size().reset_index(name=\"count\").sort_values([SEQUENCE_COLUMN, \"count\"], ascending=[True, False])\n",
    "    most_popular_filtered = most_popular_filtered.groupby(SEQUENCE_COLUMN)[\"article_id\"].apply(list).reset_index(name=\"articles\")\n",
    "    popular_articles_dict = dict(zip(most_popular_filtered[SEQUENCE_COLUMN], most_popular_filtered[\"articles\"]))\n",
    "\n",
    "    # lstm preds to dict\n",
    "    lstm_prediction_dict = transactions_filtered.set_index(\"customer_id\")[\"preds\"].to_dict()\n",
    "\n",
    "    for idx, (customer_id, prediction) in tqdm(predictions.iterrows(), total=predictions.shape[0]):\n",
    "        new_preds = prediction[:-k]\n",
    "        lstm_preds = lstm_prediction_dict.get(customer_id, [])\n",
    "\n",
    "        for i in range(k):\n",
    "            cat_value = lstm_preds[i] if i < len(lstm_preds) and lstm_preds[i] != PADDING_ARTICLE else MOST_POPULAR_VALUE\n",
    "            for article in popular_articles_dict.get(cat_value, []):\n",
    "                if article not in new_preds:\n",
    "                    new_preds.append(article)\n",
    "                    break\n",
    "\n",
    "        if len(new_preds) != 12:\n",
    "            new_preds += prediction[-k:]\n",
    "            new_preds = new_preds[:12]\n",
    "        predictions.at[idx, \"prediction\"] = new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_most_popular(predictions, 5)\n",
    "replace_last_k(predictions, N_PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(preds, purchases, k=12):\n",
    "    df = pd.merge(preds, purchases, how='left', on='customer_id')\n",
    "    # Recall = TP / (TP + FN) => TP / 12\n",
    "    return df.apply(lambda x: len(set(x[\"prediction\"][:k]).intersection(x[\"purchases\"])) / k, axis=1).mean()\n",
    "\n",
    "def hits(preds, purchases, k=12):\n",
    "    df = pd.merge(preds, purchases, how='left', on='customer_id')\n",
    "    # Hits = TP / (TP + FP) => TP / 12\n",
    "    return df.apply(lambda x: len(set(x[\"prediction\"][:k]).intersection(x[\"purchases\"])), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3e22bfaf-d1dd-4ea8-9afc-6f4332be82f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@12: 0.0256    Recall@12: 0.0102    Hits@12: 0.1225\n"
     ]
    }
   ],
   "source": [
    "### evaluate\n",
    "if test_week < transactions.week.max() + 1:\n",
    "    # get ground truth data for test week\n",
    "    purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    \n",
    "    # fill missing prediction for customers in test set with popular items in last week\n",
    "    # only for customers in test set because only those are evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, purchases.customer_id, popular)\n",
    "    \n",
    "    # calculate score\n",
    "    score = mean_average_precision(predictions, purchases, 12)\n",
    "    print(f\"MAP@12: {score:.4f}    Recall@12: {recall(purchases, predictions):.4f}    Hits@12: {hits(purchases, predictions):.4f}\")\n",
    "\n",
    "### submit\n",
    "else:\n",
    "    # fill missing predictions for all customers with popular items in last week\n",
    "    # all customers because we don't know which ones will be evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "    # write submission\n",
    "    sub = create_submission(predictions, sample_submission)\n",
    "    sub.to_csv(BASE_PATH + 'sub1.csv.gz', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
