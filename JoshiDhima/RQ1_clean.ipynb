{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read data from radek's parquet files\n",
    "transactions = pd.read_parquet('data/parquet/transactions_train.parquet')\n",
    "customers = pd.read_parquet('data/parquet/customers.parquet')\n",
    "articles = pd.read_parquet('data/parquet/articles.parquet')\n",
    "\n",
    "all_data = transactions.merge(customers, on='customer_id', how='left')\n",
    "all_data = all_data.merge(articles, on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test week\n",
    "TEST_WEEK = 105\n",
    "# Number of weeks before test week to train on\n",
    "TRAINING_WEEKS = 10\n",
    "# Number of weeks to consider for popularity methods (sliding window)\n",
    "POPULARITY_WEEKS = 3\n",
    "# Number of unique customers to consider recommending for (total number in training set of 10 weeks = 437365)\n",
    "# NUM_CUSTOMERS = 437365\n",
    "\n",
    "# Limit the transaction to the training set & number of customers\n",
    "transactions = transactions[transactions.week > transactions.week.max() - TRAINING_WEEKS]\n",
    "# transactions = transactions[transactions['customer_id'].isin(transactions['customer_id'].unique()[:NUM_CUSTOMERS])]\n",
    "\n",
    "# Split up the transactions in train and test set\n",
    "train_weeks = range(TEST_WEEK - TRAINING_WEEKS, TEST_WEEK)\n",
    "train = transactions[transactions.week.isin(train_weeks)]\n",
    "test = transactions[transactions.week == TEST_WEEK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine mean price and most common sales channel for each item to be used in candidate generation\n",
    "mean_price = transactions.groupby(['article_id'])['price'].mean()\n",
    "common_sales_channel = transactions.groupby(['article_id'])['sales_channel_id'].agg(lambda x: x.value_counts().index[0])\n",
    "\n",
    "# Add custom features age_group, avg_price_spent, max_price_spent, avg_price_group, max_price_group\n",
    "customers['age_group'] = pd.cut(customers['age'], bins=[0, 25, 40, 60, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "avg_price_spent = train.groupby('customer_id')['price'].mean()\n",
    "max_price_spent = train.groupby('customer_id')['price'].max()\n",
    "\n",
    "customers['avg_price_spent'] = customers['customer_id'].map(avg_price_spent).fillna(0)\n",
    "customers['max_price_spent'] = customers['customer_id'].map(max_price_spent).fillna(0)\n",
    "\n",
    "customers['avg_price_group'] = pd.cut(customers['avg_price_spent'], bins=[-1, 0.02, 0.04, 0.1, 0.3, 1], labels=[0, 1, 2, 3, 4])\n",
    "customers['max_price_group'] = pd.cut(customers['max_price_spent'], bins=[-1, 0.02, 0.04, 0.1, 0.3, 1], labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "# Add custom feature avg_purchaser_age, NaN values are filled with -1\n",
    "transactions_with_age = pd.merge(transactions, customers[['customer_id', 'age']], on='customer_id', how='left')\n",
    "avg_purchaser_age = transactions_with_age.groupby('article_id')['age'].mean().reset_index()\n",
    "avg_purchaser_age.rename(columns={'age': 'avg_purchaser_age'}, inplace=True)\n",
    "avg_purchaser_age['avg_purchaser_age'].fillna(-1, inplace=True)\n",
    "articles = pd.merge(articles, avg_purchaser_age, on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table with number of purchases per customer per index group\n",
    "pivot_table = pd.pivot_table(\n",
    "    all_data[all_data.week < TEST_WEEK],\n",
    "    index='customer_id',\n",
    "    columns='index_code',\n",
    "    values='article_id',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Determine the total amount of purchases from all categories, as well as the percentages of purchases for women's, children's and men's products\n",
    "pivot_table['total_purchases'] = pivot_table.sum(axis=1)\n",
    "pivot_table['percentage_women_purchases'] = ((pivot_table[0] + pivot_table[7] + pivot_table[6]) / pivot_table['total_purchases'])\n",
    "pivot_table['percentage_children_purchases'] = ((pivot_table[5] + pivot_table[3] + pivot_table[4] + pivot_table[8]) / pivot_table['total_purchases'])\n",
    "pivot_table['percentage_men_purchases'] = (pivot_table[2] / pivot_table['total_purchases'])\n",
    "pivot_table.reset_index(inplace=True)\n",
    "# Add a feature for which of these categories is most bought from\n",
    "pivot_table['most_bought_gender'] = pivot_table[['percentage_women_purchases', 'percentage_children_purchases', 'percentage_men_purchases']].idxmax(axis=1)\n",
    "\n",
    "# Add the features to the customers dataframe\n",
    "customers['most_bought_gender'] = customers['customer_id'].map(pivot_table.set_index('customer_id')['most_bought_gender'])\n",
    "customers['percentage_women_purchases'] = customers['customer_id'].map(pivot_table.set_index('customer_id')['percentage_women_purchases'])\n",
    "customers['percentage_children_purchases'] = customers['customer_id'].map(pivot_table.set_index('customer_id')['percentage_children_purchases'])\n",
    "customers['percentage_men_purchases'] = customers['customer_id'].map(pivot_table.set_index('customer_id')['percentage_men_purchases'])\n",
    "customers['total_purchases'] = customers['customer_id'].map(pivot_table.set_index('customer_id')['total_purchases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the unique customer data, and one merged with the transactions to be used in candidate generation\n",
    "unique_customers = pd.DataFrame(train['customer_id'].unique(), columns=['customer_id']).merge(customers, on='customer_id', how='left')\n",
    "train_customers = train.merge(customers, on='customer_id', how='left')\n",
    "train_customers = train_customers[train_customers['customer_id'].isin(unique_customers['customer_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function for creating candidates based on popularity within a group of customers matching a given feature\n",
    "def candidates_user_feature(feature, count=12):\n",
    "    candidates = pd.DataFrame()\n",
    "    # For each week in the test period (with enough prior weeks to determine popularity)\n",
    "    for week in range(TEST_WEEK - TRAINING_WEEKS + POPULARITY_WEEKS, TEST_WEEK):\n",
    "        # Get the number of purchases per article for each of the relevant weeks and group them based on the given feature\n",
    "        relevant_weeks = train_customers[(week - POPULARITY_WEEKS) < train_customers.week][train_customers.week <= week]\n",
    "        recent_article_counts = relevant_weeks.groupby([feature, 'article_id']).size().reset_index(name='count')\n",
    "        article_counts_sorted = recent_article_counts.sort_values([feature, 'count'], ascending=[True, False])\n",
    "        top_articles_feature = article_counts_sorted.groupby(feature).head(count)\n",
    "        # Create candidates by merging the top articles for each feature group with the customers that match on that feature\n",
    "        curr_candidates = unique_customers.merge(top_articles_feature, on=[feature], how='left')[['customer_id', 'article_id']]\n",
    "        # Increase week by one, add the mean price, most common sales channel and a randomly sampled t_dat for the candidate\n",
    "        curr_candidates['week'] = week + 1\n",
    "        curr_candidates = pd.merge(curr_candidates, mean_price, on=['article_id'])\n",
    "        curr_candidates = pd.merge(curr_candidates, common_sales_channel, on=['article_id'])\n",
    "        curr_candidates['t_dat'] = transactions[transactions['week'] == week]['t_dat'].sample(n=len(curr_candidates), random_state=1, replace=True).values\n",
    "        # Add the candidates for this week to the total candidates dataframe\n",
    "        candidates = pd.concat([candidates, curr_candidates])\n",
    "    # Change the datatypes of the id columns since otherwise they are somehow converted to floats!\n",
    "    candidates['article_id'] = candidates['article_id'].astype('int32')\n",
    "    candidates['customer_id'] = candidates['customer_id'].astype('uint64')\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate radek's repurchase candidates\n",
    "def candidates_radek_repurchase():\n",
    "    c2weeks = transactions.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = TEST_WEEK\n",
    "        \n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(transactions['customer_id'], transactions['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "        \n",
    "    candidates_last_purchase = transactions.copy()\n",
    "    candidates_last_purchase.week=weeks\n",
    "    return candidates_last_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate radek's bestseller candidates\n",
    "def candidates_radek_bestseller(count=12):\n",
    "    mean_price = transactions \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = transactions \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(count).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1    \n",
    "    bestsellers_previous_week.pipe(lambda df: df[df['week']==96])\n",
    "    unique_transactions = transactions \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = TEST_WEEK\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "    return candidates_bestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidates for each of the features, as well as radek's candidates\n",
    "candidates_age_group = candidates_user_feature('age_group')\n",
    "candidates_avg_price = candidates_user_feature('avg_price_group')\n",
    "candidates_max_price = candidates_user_feature('max_price_group')\n",
    "candidates_gender = candidates_user_feature('most_bought_gender')\n",
    "candidates_repurchase = candidates_radek_repurchase()\n",
    "candidates_bestseller = candidates_radek_bestseller()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict with all the candidate methods as well as a dataframe with all candidates merged\n",
    "all_candidate_methods = {\n",
    "    \"Popularity (age group)\": candidates_age_group, \n",
    "    \"Popularity (avg price group)\": candidates_avg_price, \n",
    "    \"Popularity (max price group)\": candidates_max_price, \n",
    "    \"Popularity (gender)\": candidates_gender, \n",
    "    \"Repurchase (radek)\": candidates_repurchase, \n",
    "    \"Bestsellers (radek)\": candidates_bestseller\n",
    "    }\n",
    "merged_candidates = pd.concat(all_candidate_methods.values()).drop_duplicates([\"customer_id\", \"week\", \"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all the article ids are of type int32, otherwise the recommendations will not be counted\n",
    "for method, candidates in all_candidate_methods.items():\n",
    "    assert candidates['article_id'].dtype == 'int32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add purchased column to distinguish between candidates and real transactions\n",
    "data = transactions\n",
    "data['purchased'] = 1\n",
    "data = pd.concat([transactions, merged_candidates]).drop_duplicates([\"customer_id\", \"week\", \"article_id\"])\n",
    "\n",
    "data.purchased.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a one hot encoding for each method, showing for each candidate which methods generated it\n",
    "for method, candidates in all_candidate_methods.items():\n",
    "    candidates[method] = 1\n",
    "    data = data.merge(candidates[['customer_id', 'week', 'article_id', method]], on=['customer_id', 'week', 'article_id'], how='left')\n",
    "    data[method].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the customer and article information to the data\n",
    "data = pd.merge(data, articles, on='article_id', how='left')\n",
    "data = pd.merge(data, customers, on='customer_id', how='left')\n",
    "data.sort_values(['week', 'customer_id'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a feature for the number of methods that generated a candidate\n",
    "data['num_methods'] = data[list(all_candidate_methods.keys())].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bestseller_rank feature\n",
    "mean_price = transactions \\\n",
    "    .groupby(['week', 'article_id'])['price'].mean()\n",
    "sales = transactions \\\n",
    "    .groupby('week')['article_id'].value_counts() \\\n",
    "    .groupby('week').rank(method='dense', ascending=False) \\\n",
    "    .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "# Generate the bestsellers of previous week to fill in missing recommendations\n",
    "bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "bestsellers_previous_week.week += 1\n",
    "bestsellers_last_week = \\\n",
    "    bestsellers_previous_week[bestsellers_previous_week.week == bestsellers_previous_week.week.max()]['article_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(\n",
    "    data,\n",
    "    bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "    on=['week', 'article_id'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing bestseller ranks with 999\n",
    "data = data[data.week != data.week.min()]\n",
    "data.bestseller_rank.fillna(999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for various tests involving different methods\n",
    "\n",
    "# Only radek candidates, 0.01959 - 0.02018 --> One hot encoded: 0.02062 - 0.02124\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Bestsellers (radek)'] == 1) | (data['Repurchase (radek)'] == 1)]\n",
    "\n",
    "# Only popularity candidates, 0.00695 - 0.00703 --> One hot encoded: 0.00680 - 0.00659\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Bestsellers (radek)'] == 1)]\n",
    "\n",
    "# Only repurchase candidates, 0.02030 - 0.02077 --> 0.02049 - 0.02116\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Repurchase (radek)'] == 1)]\n",
    "\n",
    "# Only age group bestsellers, 0.00674 - 0.00655 --> 0.00725 - 0.00698\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Popularity (age group)'] == 1)]\n",
    "\n",
    "# Only avg price group bestsellers, 0.00552 - 0,00540 --> 0,00643 - 0,00645\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Popularity (avg price group)'] == 1)]\n",
    "\n",
    "# Only max price group bestsellers, 0.00526 - 0,00537 --> 0,00665 - 0,00641\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Popularity (max price group)'] == 1)]\n",
    "\n",
    "# Only gender based bestsellers, 0.00685 - 0,00676 --> 0,00658 - 0,00625\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Popularity (gender)'] == 1)]\n",
    "\n",
    "# All custom methods, 0.00524 - 0.00504 --> 0.00662 - 0.00660\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Popularity (age group)'] == 1) | (data['Popularity (avg price group)'] == 1) | (data['Popularity (max price group)'] == 1) | (data['Popularity (gender)'] == 1)]\n",
    "\n",
    "# All methods, 0.01261 - 0.01385 --> 0.1966 - 0.2050 --> 0.1949 - 0.2039 (num_methods) \n",
    "filtered_data = data\n",
    "\n",
    "# Filter based on num_methods --> >1: 0.02011 - 0.02058 --> >2: 0.02019 - 0.02083 --> >3: 0.01918 - 0.01965\n",
    "# filtered_data = data[(data.purchased == 1) | (data['Repurchase (radek)'] == 1) | (data['num_methods'] > 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate train and test data using the filtered data, and create baskets for training\n",
    "train = filtered_data[filtered_data.week.isin(train_weeks)]\n",
    "test = filtered_data[filtered_data.week == TEST_WEEK]\n",
    "\n",
    "train_baskets = train.groupby(['week', 'customer_id'])['article_id'].count().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to use for training\n",
    "columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n",
    "'perceived_colour_master_id', 'department_no', 'index_code',\n",
    "'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n",
    "'club_member_status', 'fashion_news_frequency', 'age', 'postal_code',\n",
    "'age_group', 'avg_purchaser_age', 'percentage_children_purchases', 'percentage_men_purchases', 'percentage_women_purchases', 'total_purchases']\n",
    "# , 'num_methods', 'bestseller_rank']\n",
    "\n",
    "# Add the one hot encoding columns for each of the used candidate methods\n",
    "columns_to_use.extend(list(all_candidate_methods.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_X = train[columns_to_use]\n",
    "train_y = train['purchased']\n",
    "\n",
    "test_X = test[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit the ranker model\n",
    "ranker = ranker.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    group=train_baskets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importances determined by the ranker\n",
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    print(columns_to_use[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# Use the ranker to creat predictions\n",
    "test['preds'] = ranker.predict(test_X)\n",
    "\n",
    "c_id2predicted_article_ids = test \\\n",
    "    .sort_values(['customer_id', 'preds'], ascending=False) \\\n",
    "    .groupby('customer_id')['article_id'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/original/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = []\n",
    "# Store predictions for each customer\n",
    "for c_id in customer_hex_id_to_int(sub.customer_id):\n",
    "    pred = c_id2predicted_article_ids.get(c_id, [])\n",
    "    pred = pred + bestsellers_last_week\n",
    "    preds.append(pred[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions file for kaggle\n",
    "preds = [' '.join(['0' + str(p) for p in ps]) for ps in preds]\n",
    "sub.prediction = preds\n",
    "sub_name = 'candidate_generation_model'\n",
    "sub.to_csv(f'{sub_name}.csv.gz', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
