{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two tower model\n",
    "\n",
    "In this notebook I implement my two tower model. This includes two sub models that are trained on learning two embeddings. One model learns the item (article) embedding, and the other the user (customer) embedding. I use Pytorch for the neural network imeplementations. Additional resources are required to train the models on the full transaction dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-nlp\n",
      "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 2.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/jasperdelaet/opt/anaconda3/lib/python3.9/site-packages (from pytorch-nlp) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /Users/jasperdelaet/opt/anaconda3/lib/python3.9/site-packages (from pytorch-nlp) (4.64.0)\n",
      "Installing collected packages: pytorch-nlp\n",
      "Successfully installed pytorch-nlp-0.5.0\n"
     ]
    }
   ],
   "source": [
    "# Necessary additional packages\n",
    "!pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchnlp.encoders import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (to fine-tune)\n",
    "\n",
    "# learning rate\n",
    "lr = 0.05\n",
    "\n",
    "# batch size\n",
    "batch_size = 512\n",
    "\n",
    "# embedded dimension\n",
    "embed_dim = 128\n",
    "\n",
    "# internal criterion\n",
    "internal_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load data in dataframes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# article_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# customer_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/customers.csv\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m transactions_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# split data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_df \u001b[38;5;241m=\u001b[39m transactions_df[(transactions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_dat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-09-05\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (transactions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_dat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-09-10\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv'"
     ]
    }
   ],
   "source": [
    "# load data in dataframes -> paths need to be adjusted based on where the notebook is ran\n",
    "# article_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\")\n",
    "# customer_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/customers.csv\")\n",
    "transactions_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")\n",
    "\n",
    "# split data\n",
    "\n",
    "train_df = transactions_df[(transactions_df['t_dat'] >= '2020-09-05') & (transactions_df['t_dat'] <= '2020-09-10')]\n",
    "test_df = transactions_df[(transactions_df['t_dat'] >= '2020-09-15')]\n",
    "\n",
    "# Encoders to encode customer and article ids into numerical values\n",
    "\n",
    "customer_encoder = LabelEncoder(train_df['customer_id'].unique(), reserved_labels=['unknown'], unknown_index=0)\n",
    "article_encoder = LabelEncoder(train_df['article_id'].unique(), reserved_labels=['unknown'], unknown_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.242331Z",
     "iopub.status.busy": "2023-11-07T17:02:12.241109Z",
     "iopub.status.idle": "2023-11-07T17:02:12.248818Z",
     "shell.execute_reply": "2023-11-07T17:02:12.248065Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.242287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataset (for more flexibility if needed)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transactions):\n",
    "        self.transactions = transactions\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.transactions)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        row = self.transactions.iloc[[index]]\n",
    "        return row['customer_id'].item(), row['article_id'].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.250429Z",
     "iopub.status.busy": "2023-11-07T17:02:12.250133Z",
     "iopub.status.idle": "2023-11-07T17:02:12.277230Z",
     "shell.execute_reply": "2023-11-07T17:02:12.276146Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.250403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset and dataloader intialization\n",
    "train_data = CustomDataset(train_df[['customer_id','article_id']])\n",
    "test_data = CustomDataset(test_df[['customer_id', 'article_id']])\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.279257Z",
     "iopub.status.busy": "2023-11-07T17:02:12.278960Z",
     "iopub.status.idle": "2023-11-07T17:02:12.287815Z",
     "shell.execute_reply": "2023-11-07T17:02:12.286517Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.279230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Two tower model using PyTorch\n",
    "class TwoTower(nn.Module):\n",
    "    # In our case, items are articles and users are customers\n",
    "    def __init__(self, n_users, n_items):\n",
    "        super(TwoTower, self).__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_embeddings=n_users, embedding_dim=embed_dim)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=n_items, embedding_dim=embed_dim)\n",
    "        \n",
    "        self.user_layers = nn.Sequential(\n",
    "            nn.Linear(128, 64,bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32,bias=True),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.item_layers = nn.Sequential(\n",
    "            nn.Linear(128, 64,bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32,bias=True),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.dot_product = torch.matmul\n",
    "    \n",
    "    def forward(self, users, items):\n",
    "        \n",
    "        user_embedding = self.user_embedding(users)\n",
    "        item_embedding = self.item_embedding(items)\n",
    "        \n",
    "        user_embedding = self.user_layers(user_embedding)\n",
    "        item_embedding = self.item_layers(item_embedding)\n",
    "    \n",
    "        return self.dot_product(user_embedding, item_embedding.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.289895Z",
     "iopub.status.busy": "2023-11-07T17:02:12.289403Z",
     "iopub.status.idle": "2023-11-07T17:02:12.323766Z",
     "shell.execute_reply": "2023-11-07T17:02:12.322862Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.289845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probabilities of an article being selected from the transactions dataset -> necessary for in batch negative sampling\n",
    "article_counts = train_df.groupby('article_id')['article_id'].count().to_dict()\n",
    "amount_of_transactions = len(train_df)\n",
    "article_probs = {i: article_counts[i]/amount_of_transactions for i in article_counts.keys()}\n",
    "\n",
    "# Custom loss class, to seperate the training and evaluation loss computation\n",
    "class CustomLoss:\n",
    "    def __init__(self, article_probs):\n",
    "        self.article_probs = article_probs\n",
    "    \n",
    "    def __call__(self, predicted_values, true_values, training):\n",
    "        if training:\n",
    "            decoded_values = article_encoder.batch_decode(true_values)\n",
    "            true_value_probs = list(map(lambda x: self.article_probs[x], decoded_values))\n",
    "            predicted_values = torch.sub(predicted_values, torch.log(torch.FloatTensor(true_value_probs)))\n",
    "            true_values = torch.arange(batch_size)\n",
    "        \n",
    "        loss = internal_criterion(predicted_values, true_values)\n",
    "        return loss\n",
    "\n",
    "criterion = CustomLoss(article_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.327001Z",
     "iopub.status.busy": "2023-11-07T17:02:12.326654Z",
     "iopub.status.idle": "2023-11-07T17:02:12.332942Z",
     "shell.execute_reply": "2023-11-07T17:02:12.331821Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.326971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.335257Z",
     "iopub.status.busy": "2023-11-07T17:02:12.334807Z",
     "iopub.status.idle": "2023-11-07T17:02:12.355136Z",
     "shell.execute_reply": "2023-11-07T17:02:12.354000Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.335217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, device, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batchidx, data in enumerate(dataloader):\n",
    "        print(str(batchidx) + \"/\" + str(len(dataloader)))\n",
    "        customers, articles = data\n",
    "        customers = customer_encoder.batch_encode(customers)\n",
    "        articles = article_encoder.batch_encode(articles.tolist())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(customers, articles)\n",
    "        loss = criterion(outputs, articles, training=True)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    return train_loss\n",
    "        \n",
    "# Evaluatioon function      \n",
    "def evaluation(model, device, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    top_1000_correct = 0\n",
    "    top_500_correct = 0\n",
    "    top_100_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batchidx, data in enumerate(dataloader):\n",
    "            print(str(batchidx) + \"/\" + str(len(dataloader)))\n",
    "            customers, articles = data\n",
    "            customers = customer_encoder.batch_encode(customers)\n",
    "            articles = article_encoder.batch_encode(articles.tolist())\n",
    "            all_articles = article_encoder.batch_encode(test_df['article_id'].unique())\n",
    "            \n",
    "            # all articles -> to get accurate recall values\n",
    "            outputs = model(customers, all_articles)\n",
    "\n",
    "            loss = criterion(outputs, articles, training=False)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Top k=1000\n",
    "            top_indices = torch.topk(outputs, 1000).indices\n",
    "            for i, article in enumerate(articles):\n",
    "                if article in top_indices[i]:\n",
    "                    top_1000_correct += 1\n",
    "                if article in top_indices[i][:500]:\n",
    "                    top_500_correct += 1\n",
    "                if article in top_indices[i][:100]:\n",
    "                    top_100_correct += 1\n",
    "    return (val_loss, top_1000_correct / len(dataloader.dataset), top_500_correct / len(dataloader.dataset), top_100_correct / len(dataloader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T17:02:12.357295Z",
     "iopub.status.busy": "2023-11-07T17:02:12.356938Z",
     "iopub.status.idle": "2023-11-07T17:06:39.953034Z",
     "shell.execute_reply": "2023-11-07T17:06:39.951622Z",
     "shell.execute_reply.started": "2023-11-07T17:02:12.357264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts\n",
      "0/457\n",
      "1/457\n",
      "2/457\n",
      "3/457\n",
      "4/457\n",
      "5/457\n",
      "6/457\n",
      "7/457\n",
      "8/457\n",
      "9/457\n",
      "10/457\n",
      "11/457\n",
      "12/457\n",
      "13/457\n",
      "14/457\n",
      "15/457\n",
      "16/457\n",
      "17/457\n",
      "18/457\n",
      "19/457\n",
      "20/457\n",
      "21/457\n",
      "22/457\n",
      "23/457\n",
      "24/457\n",
      "25/457\n",
      "26/457\n",
      "27/457\n",
      "28/457\n",
      "29/457\n",
      "30/457\n",
      "31/457\n",
      "32/457\n",
      "33/457\n",
      "34/457\n",
      "35/457\n",
      "36/457\n",
      "37/457\n",
      "38/457\n",
      "39/457\n",
      "40/457\n",
      "41/457\n",
      "42/457\n",
      "43/457\n",
      "44/457\n",
      "45/457\n",
      "46/457\n",
      "47/457\n",
      "48/457\n",
      "49/457\n",
      "50/457\n",
      "51/457\n",
      "52/457\n",
      "53/457\n",
      "54/457\n",
      "55/457\n",
      "56/457\n",
      "57/457\n",
      "58/457\n",
      "59/457\n",
      "60/457\n",
      "61/457\n",
      "62/457\n",
      "63/457\n",
      "64/457\n",
      "65/457\n",
      "66/457\n",
      "67/457\n",
      "68/457\n",
      "69/457\n",
      "70/457\n",
      "71/457\n",
      "72/457\n",
      "73/457\n",
      "74/457\n",
      "75/457\n",
      "76/457\n",
      "77/457\n",
      "78/457\n",
      "79/457\n",
      "80/457\n",
      "81/457\n",
      "82/457\n",
      "83/457\n",
      "84/457\n",
      "85/457\n",
      "86/457\n",
      "87/457\n",
      "88/457\n",
      "89/457\n",
      "90/457\n",
      "91/457\n",
      "92/457\n",
      "93/457\n",
      "94/457\n",
      "95/457\n",
      "96/457\n",
      "97/457\n",
      "98/457\n",
      "99/457\n",
      "100/457\n",
      "101/457\n",
      "102/457\n",
      "103/457\n",
      "104/457\n",
      "105/457\n",
      "106/457\n",
      "107/457\n",
      "108/457\n",
      "109/457\n",
      "110/457\n",
      "111/457\n",
      "112/457\n",
      "113/457\n",
      "114/457\n",
      "115/457\n",
      "116/457\n",
      "117/457\n",
      "118/457\n",
      "119/457\n",
      "120/457\n",
      "121/457\n",
      "122/457\n",
      "123/457\n",
      "124/457\n",
      "125/457\n",
      "126/457\n",
      "127/457\n",
      "128/457\n",
      "129/457\n",
      "130/457\n",
      "131/457\n",
      "132/457\n",
      "133/457\n",
      "134/457\n",
      "135/457\n",
      "136/457\n",
      "137/457\n",
      "138/457\n",
      "139/457\n",
      "140/457\n",
      "141/457\n",
      "142/457\n",
      "143/457\n",
      "144/457\n",
      "145/457\n",
      "146/457\n",
      "147/457\n",
      "148/457\n",
      "149/457\n",
      "150/457\n",
      "151/457\n",
      "152/457\n",
      "153/457\n",
      "154/457\n",
      "155/457\n",
      "156/457\n",
      "157/457\n",
      "158/457\n",
      "159/457\n",
      "160/457\n",
      "161/457\n",
      "162/457\n",
      "163/457\n",
      "164/457\n",
      "165/457\n",
      "166/457\n",
      "167/457\n",
      "168/457\n",
      "169/457\n",
      "170/457\n",
      "171/457\n",
      "172/457\n",
      "173/457\n",
      "174/457\n",
      "175/457\n",
      "176/457\n",
      "177/457\n",
      "178/457\n",
      "179/457\n",
      "180/457\n",
      "181/457\n",
      "182/457\n",
      "183/457\n",
      "184/457\n",
      "185/457\n",
      "186/457\n",
      "187/457\n",
      "188/457\n",
      "189/457\n",
      "190/457\n",
      "191/457\n",
      "192/457\n",
      "193/457\n",
      "194/457\n",
      "195/457\n",
      "196/457\n",
      "197/457\n",
      "198/457\n",
      "199/457\n",
      "200/457\n",
      "201/457\n",
      "202/457\n",
      "203/457\n",
      "204/457\n",
      "205/457\n",
      "206/457\n",
      "207/457\n",
      "208/457\n",
      "209/457\n",
      "210/457\n",
      "211/457\n",
      "212/457\n",
      "213/457\n",
      "214/457\n",
      "215/457\n",
      "216/457\n",
      "217/457\n",
      "218/457\n",
      "219/457\n",
      "220/457\n",
      "221/457\n",
      "222/457\n",
      "223/457\n",
      "224/457\n",
      "225/457\n",
      "226/457\n",
      "227/457\n",
      "228/457\n",
      "229/457\n",
      "230/457\n",
      "231/457\n",
      "232/457\n",
      "233/457\n",
      "234/457\n",
      "235/457\n",
      "236/457\n",
      "237/457\n",
      "238/457\n",
      "239/457\n",
      "240/457\n",
      "241/457\n",
      "242/457\n",
      "243/457\n",
      "244/457\n",
      "245/457\n",
      "246/457\n",
      "247/457\n",
      "248/457\n",
      "249/457\n",
      "250/457\n",
      "251/457\n",
      "252/457\n",
      "253/457\n",
      "254/457\n",
      "255/457\n",
      "256/457\n",
      "257/457\n",
      "258/457\n",
      "259/457\n",
      "260/457\n",
      "261/457\n",
      "262/457\n",
      "263/457\n",
      "264/457\n",
      "265/457\n",
      "266/457\n",
      "267/457\n",
      "268/457\n",
      "269/457\n",
      "270/457\n",
      "271/457\n",
      "272/457\n",
      "273/457\n",
      "274/457\n",
      "275/457\n",
      "276/457\n",
      "277/457\n",
      "278/457\n",
      "279/457\n",
      "280/457\n",
      "281/457\n",
      "282/457\n",
      "283/457\n",
      "284/457\n",
      "285/457\n",
      "286/457\n",
      "287/457\n",
      "288/457\n",
      "289/457\n",
      "290/457\n",
      "291/457\n",
      "292/457\n",
      "293/457\n",
      "294/457\n",
      "295/457\n",
      "296/457\n",
      "297/457\n",
      "298/457\n",
      "299/457\n",
      "300/457\n",
      "301/457\n",
      "302/457\n",
      "303/457\n",
      "304/457\n",
      "305/457\n",
      "306/457\n",
      "307/457\n",
      "308/457\n",
      "309/457\n",
      "310/457\n",
      "311/457\n",
      "312/457\n",
      "313/457\n",
      "314/457\n",
      "315/457\n",
      "316/457\n",
      "317/457\n",
      "318/457\n",
      "319/457\n",
      "320/457\n",
      "321/457\n",
      "322/457\n",
      "323/457\n",
      "324/457\n",
      "325/457\n",
      "326/457\n",
      "327/457\n",
      "328/457\n",
      "329/457\n",
      "330/457\n",
      "331/457\n",
      "332/457\n",
      "333/457\n",
      "334/457\n",
      "335/457\n",
      "336/457\n",
      "337/457\n",
      "338/457\n",
      "339/457\n",
      "340/457\n",
      "341/457\n",
      "342/457\n",
      "343/457\n",
      "344/457\n",
      "345/457\n",
      "346/457\n",
      "347/457\n",
      "348/457\n",
      "349/457\n",
      "350/457\n",
      "351/457\n",
      "352/457\n",
      "353/457\n",
      "354/457\n",
      "355/457\n",
      "356/457\n",
      "357/457\n",
      "358/457\n",
      "359/457\n",
      "360/457\n",
      "361/457\n",
      "362/457\n",
      "363/457\n",
      "364/457\n",
      "365/457\n",
      "366/457\n",
      "367/457\n",
      "368/457\n",
      "369/457\n",
      "370/457\n",
      "371/457\n",
      "372/457\n",
      "373/457\n",
      "374/457\n",
      "375/457\n",
      "376/457\n",
      "377/457\n",
      "378/457\n",
      "379/457\n",
      "380/457\n",
      "381/457\n",
      "382/457\n",
      "383/457\n",
      "384/457\n",
      "385/457\n",
      "386/457\n",
      "387/457\n",
      "388/457\n",
      "389/457\n",
      "390/457\n",
      "391/457\n",
      "392/457\n",
      "393/457\n",
      "394/457\n",
      "395/457\n",
      "396/457\n",
      "397/457\n",
      "398/457\n",
      "399/457\n",
      "400/457\n",
      "401/457\n",
      "402/457\n",
      "403/457\n",
      "404/457\n",
      "405/457\n",
      "406/457\n",
      "407/457\n",
      "408/457\n",
      "409/457\n",
      "410/457\n",
      "411/457\n",
      "412/457\n",
      "413/457\n",
      "414/457\n",
      "415/457\n",
      "416/457\n",
      "417/457\n",
      "418/457\n",
      "419/457\n",
      "420/457\n",
      "421/457\n",
      "422/457\n",
      "423/457\n",
      "424/457\n",
      "425/457\n",
      "426/457\n",
      "427/457\n",
      "428/457\n",
      "429/457\n",
      "430/457\n",
      "431/457\n",
      "432/457\n",
      "433/457\n",
      "434/457\n",
      "435/457\n",
      "436/457\n",
      "437/457\n",
      "438/457\n",
      "439/457\n",
      "440/457\n",
      "441/457\n",
      "442/457\n",
      "443/457\n",
      "444/457\n",
      "445/457\n",
      "446/457\n",
      "447/457\n",
      "448/457\n",
      "449/457\n",
      "450/457\n",
      "451/457\n",
      "452/457\n",
      "453/457\n",
      "454/457\n",
      "455/457\n",
      "456/457\n",
      "evaluation starts\n",
      "0/520\n",
      "1/520\n",
      "2/520\n",
      "3/520\n",
      "4/520\n",
      "5/520\n",
      "6/520\n",
      "7/520\n",
      "8/520\n",
      "9/520\n",
      "10/520\n",
      "11/520\n",
      "12/520\n",
      "13/520\n",
      "14/520\n",
      "15/520\n",
      "16/520\n",
      "17/520\n",
      "18/520\n",
      "19/520\n",
      "20/520\n",
      "21/520\n",
      "22/520\n",
      "23/520\n",
      "24/520\n",
      "25/520\n",
      "26/520\n",
      "27/520\n",
      "28/520\n",
      "29/520\n",
      "30/520\n",
      "31/520\n",
      "32/520\n",
      "33/520\n",
      "34/520\n",
      "35/520\n",
      "36/520\n",
      "37/520\n",
      "38/520\n",
      "39/520\n",
      "40/520\n",
      "41/520\n",
      "42/520\n",
      "43/520\n",
      "44/520\n",
      "45/520\n",
      "46/520\n",
      "47/520\n",
      "48/520\n",
      "49/520\n",
      "50/520\n",
      "51/520\n",
      "52/520\n",
      "53/520\n",
      "54/520\n",
      "55/520\n",
      "56/520\n",
      "57/520\n",
      "58/520\n",
      "59/520\n",
      "60/520\n",
      "61/520\n",
      "62/520\n",
      "63/520\n",
      "64/520\n",
      "65/520\n",
      "66/520\n",
      "67/520\n",
      "68/520\n",
      "69/520\n",
      "70/520\n",
      "71/520\n",
      "72/520\n",
      "73/520\n",
      "74/520\n",
      "75/520\n",
      "76/520\n",
      "77/520\n",
      "78/520\n",
      "79/520\n",
      "80/520\n",
      "81/520\n",
      "82/520\n",
      "83/520\n",
      "84/520\n",
      "85/520\n",
      "86/520\n",
      "87/520\n",
      "88/520\n",
      "89/520\n",
      "90/520\n",
      "91/520\n",
      "92/520\n",
      "93/520\n",
      "94/520\n",
      "95/520\n",
      "96/520\n",
      "97/520\n",
      "98/520\n",
      "99/520\n",
      "100/520\n",
      "101/520\n",
      "102/520\n",
      "103/520\n",
      "104/520\n",
      "105/520\n",
      "106/520\n",
      "107/520\n",
      "108/520\n",
      "109/520\n",
      "110/520\n",
      "111/520\n",
      "112/520\n",
      "113/520\n",
      "114/520\n",
      "115/520\n",
      "116/520\n",
      "117/520\n",
      "118/520\n",
      "119/520\n",
      "120/520\n",
      "121/520\n",
      "122/520\n",
      "123/520\n",
      "124/520\n",
      "125/520\n",
      "126/520\n",
      "127/520\n",
      "128/520\n",
      "129/520\n",
      "130/520\n",
      "131/520\n",
      "132/520\n",
      "133/520\n",
      "134/520\n",
      "135/520\n",
      "136/520\n",
      "137/520\n",
      "138/520\n",
      "139/520\n",
      "140/520\n",
      "141/520\n",
      "142/520\n",
      "143/520\n",
      "144/520\n",
      "145/520\n",
      "146/520\n",
      "147/520\n",
      "148/520\n",
      "149/520\n",
      "150/520\n",
      "151/520\n",
      "152/520\n",
      "153/520\n",
      "154/520\n",
      "155/520\n",
      "156/520\n",
      "157/520\n",
      "158/520\n",
      "159/520\n",
      "160/520\n",
      "161/520\n",
      "162/520\n",
      "163/520\n",
      "164/520\n",
      "165/520\n",
      "166/520\n",
      "167/520\n",
      "168/520\n",
      "169/520\n",
      "170/520\n",
      "171/520\n",
      "172/520\n",
      "173/520\n",
      "174/520\n",
      "175/520\n",
      "176/520\n",
      "177/520\n",
      "178/520\n",
      "179/520\n",
      "180/520\n",
      "181/520\n",
      "182/520\n",
      "183/520\n",
      "184/520\n",
      "185/520\n",
      "186/520\n",
      "187/520\n",
      "188/520\n",
      "189/520\n",
      "190/520\n",
      "191/520\n",
      "192/520\n",
      "193/520\n",
      "194/520\n",
      "195/520\n",
      "196/520\n",
      "197/520\n",
      "198/520\n",
      "199/520\n",
      "200/520\n",
      "201/520\n",
      "202/520\n",
      "203/520\n",
      "204/520\n",
      "205/520\n",
      "206/520\n",
      "207/520\n",
      "208/520\n",
      "209/520\n",
      "210/520\n",
      "211/520\n",
      "212/520\n",
      "213/520\n",
      "214/520\n",
      "215/520\n",
      "216/520\n",
      "217/520\n",
      "218/520\n",
      "219/520\n",
      "220/520\n",
      "221/520\n",
      "222/520\n",
      "223/520\n",
      "224/520\n",
      "225/520\n",
      "226/520\n",
      "227/520\n",
      "228/520\n",
      "229/520\n",
      "230/520\n",
      "231/520\n",
      "232/520\n",
      "233/520\n",
      "234/520\n",
      "235/520\n",
      "236/520\n",
      "237/520\n",
      "238/520\n",
      "239/520\n",
      "240/520\n",
      "241/520\n",
      "242/520\n",
      "243/520\n",
      "244/520\n",
      "245/520\n",
      "246/520\n",
      "247/520\n",
      "248/520\n",
      "249/520\n",
      "250/520\n",
      "251/520\n",
      "252/520\n",
      "253/520\n",
      "254/520\n",
      "255/520\n",
      "256/520\n",
      "257/520\n",
      "258/520\n",
      "259/520\n",
      "260/520\n",
      "261/520\n",
      "262/520\n",
      "263/520\n",
      "264/520\n",
      "265/520\n",
      "266/520\n",
      "267/520\n",
      "268/520\n",
      "269/520\n",
      "270/520\n",
      "271/520\n",
      "272/520\n",
      "273/520\n",
      "274/520\n",
      "275/520\n",
      "276/520\n",
      "277/520\n",
      "278/520\n",
      "279/520\n",
      "280/520\n",
      "281/520\n",
      "282/520\n",
      "283/520\n",
      "284/520\n",
      "285/520\n",
      "286/520\n",
      "287/520\n",
      "288/520\n",
      "289/520\n",
      "290/520\n",
      "291/520\n",
      "292/520\n",
      "293/520\n",
      "294/520\n",
      "295/520\n",
      "296/520\n",
      "297/520\n",
      "298/520\n",
      "299/520\n",
      "300/520\n",
      "301/520\n",
      "302/520\n",
      "303/520\n",
      "304/520\n",
      "305/520\n",
      "306/520\n",
      "307/520\n",
      "308/520\n",
      "309/520\n",
      "310/520\n",
      "311/520\n",
      "312/520\n",
      "313/520\n",
      "314/520\n",
      "315/520\n",
      "316/520\n",
      "317/520\n",
      "318/520\n",
      "319/520\n",
      "320/520\n",
      "321/520\n",
      "322/520\n",
      "323/520\n",
      "324/520\n",
      "325/520\n",
      "326/520\n",
      "327/520\n",
      "328/520\n",
      "329/520\n",
      "330/520\n",
      "331/520\n",
      "332/520\n",
      "333/520\n",
      "334/520\n",
      "335/520\n",
      "336/520\n",
      "337/520\n",
      "338/520\n",
      "339/520\n",
      "340/520\n",
      "341/520\n",
      "342/520\n",
      "343/520\n",
      "344/520\n",
      "345/520\n",
      "346/520\n",
      "347/520\n",
      "348/520\n",
      "349/520\n",
      "350/520\n",
      "351/520\n",
      "352/520\n",
      "353/520\n",
      "354/520\n",
      "355/520\n",
      "356/520\n",
      "357/520\n",
      "358/520\n",
      "359/520\n",
      "360/520\n",
      "361/520\n",
      "362/520\n",
      "363/520\n",
      "364/520\n",
      "365/520\n",
      "366/520\n",
      "367/520\n",
      "368/520\n",
      "369/520\n",
      "370/520\n",
      "371/520\n",
      "372/520\n",
      "373/520\n",
      "374/520\n",
      "375/520\n",
      "376/520\n",
      "377/520\n",
      "378/520\n",
      "379/520\n",
      "380/520\n",
      "381/520\n",
      "382/520\n",
      "383/520\n",
      "384/520\n",
      "385/520\n",
      "386/520\n",
      "387/520\n",
      "388/520\n",
      "389/520\n",
      "390/520\n",
      "391/520\n",
      "392/520\n",
      "393/520\n",
      "394/520\n",
      "395/520\n",
      "396/520\n",
      "397/520\n",
      "398/520\n",
      "399/520\n",
      "400/520\n",
      "401/520\n",
      "402/520\n",
      "403/520\n",
      "404/520\n",
      "405/520\n",
      "406/520\n",
      "407/520\n",
      "408/520\n",
      "409/520\n",
      "410/520\n",
      "411/520\n",
      "412/520\n",
      "413/520\n",
      "414/520\n",
      "415/520\n",
      "416/520\n",
      "417/520\n",
      "418/520\n",
      "419/520\n",
      "420/520\n",
      "421/520\n",
      "422/520\n",
      "423/520\n",
      "424/520\n",
      "425/520\n",
      "426/520\n",
      "427/520\n",
      "428/520\n",
      "429/520\n",
      "430/520\n",
      "431/520\n",
      "432/520\n",
      "433/520\n",
      "434/520\n",
      "435/520\n",
      "436/520\n",
      "437/520\n",
      "438/520\n",
      "439/520\n",
      "440/520\n",
      "441/520\n",
      "442/520\n",
      "443/520\n",
      "444/520\n",
      "445/520\n",
      "446/520\n",
      "447/520\n",
      "448/520\n",
      "449/520\n",
      "450/520\n",
      "451/520\n",
      "452/520\n",
      "453/520\n",
      "454/520\n",
      "455/520\n",
      "456/520\n",
      "457/520\n",
      "458/520\n",
      "459/520\n",
      "460/520\n",
      "461/520\n",
      "462/520\n",
      "463/520\n",
      "464/520\n",
      "465/520\n",
      "466/520\n",
      "467/520\n",
      "468/520\n",
      "469/520\n",
      "470/520\n",
      "471/520\n",
      "472/520\n",
      "473/520\n",
      "474/520\n",
      "475/520\n",
      "476/520\n",
      "477/520\n",
      "478/520\n",
      "479/520\n",
      "480/520\n",
      "481/520\n",
      "482/520\n",
      "483/520\n",
      "484/520\n",
      "485/520\n",
      "486/520\n",
      "487/520\n",
      "488/520\n",
      "489/520\n",
      "490/520\n",
      "491/520\n",
      "492/520\n",
      "493/520\n",
      "494/520\n",
      "495/520\n",
      "496/520\n",
      "497/520\n",
      "498/520\n",
      "499/520\n",
      "500/520\n",
      "501/520\n",
      "502/520\n",
      "503/520\n",
      "504/520\n",
      "505/520\n",
      "506/520\n",
      "507/520\n",
      "508/520\n",
      "509/520\n",
      "510/520\n",
      "511/520\n",
      "512/520\n",
      "513/520\n",
      "514/520\n",
      "515/520\n",
      "516/520\n",
      "517/520\n",
      "518/520\n",
      "519/520\n",
      "3589.9612860679626\n",
      "7155.085865020752\n",
      "0.13528479824600922\n",
      "0.07670330825486928\n",
      "0.01803922451983001\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = TwoTower(len(train_df.customer_id.unique()) + 1, len(train_df.article_id.unique()) + 1)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "top_100_recall_lst = []\n",
    "top_500_recall_lst = []\n",
    "top_1000_recall_lst = []\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(\"Epoch:\" + str(epoch))\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    val_loss, top_1000_recall, top_500_recall, top_100_recall = evaluation(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    top_100_recall_lst.append(top_100_recall)\n",
    "    top_500_recall_lst.append(top_500_recall)\n",
    "    top_1000_recall_lst.append(top_1000_recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
