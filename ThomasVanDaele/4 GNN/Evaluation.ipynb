{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Based on the template by Noah Daniels\n",
    "\n",
    "Additions are in the last few blocks of code. The rest is copied from the template.\n",
    "I will inject another text block from where my code starts."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9cd2a76cd55ceb5"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# make external scripts auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from baseline import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T15:00:32.798397698Z",
     "start_time": "2023-12-29T15:00:32.419635074Z"
    }
   },
   "id": "a28915ef24fa4a8d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba89dec-cb25-4243-b9d7-a857d770e97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T15:00:34.600725550Z",
     "start_time": "2023-12-29T15:00:32.800419196Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '../data/'\n",
    "\n",
    "# make sure the same data preprocessing as in the radek notebook have been performed\n",
    "# (see 02 FE/DataProcessingRadek.ipynb)\n",
    "transactions = pd.read_parquet(BASE_PATH + 'parquet/transactions_train.parquet')\n",
    "customers = pd.read_parquet(BASE_PATH + 'parquet/customers.parquet')\n",
    "articles = pd.read_parquet(BASE_PATH + 'parquet/articles.parquet')\n",
    "sample_submission = pd.read_csv(BASE_PATH + 'csv/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72811db-7994-4aa4-ac87-e90fa7275b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T15:00:34.612903470Z",
     "start_time": "2023-12-29T15:00:34.603764044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Candidate generation of Radek notebook\n",
    "def get_data(data, test_week):\n",
    "    ### repurchase\n",
    "    # each week is seen as a basket\n",
    "    # the items bought in one basket, will be example for the next basket\n",
    "    # the items bought in the last basket, will be candidates for the test basket\n",
    "    c2weeks = data.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = data.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(data['customer_id'], data['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "\n",
    "    ### bestseller\n",
    "    # if a user bought an item in a given week, the 12 most popular items in the previous week are example for that week\n",
    "    # the best selling items in the last week are candidates for all users\n",
    "    mean_price = data \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = data \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    unique_transactions = data \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "\n",
    "    ### combine\n",
    "    d = data.copy()\n",
    "    d['purchased'] = True\n",
    "    \n",
    "    result = pd.concat([\n",
    "        d, candidates_last_purchase, candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(False, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "\n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result\n",
    "\n",
    "def get_examples(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week != test_week]\n",
    "\n",
    "def get_candidates(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week == test_week]\n",
    "\n",
    "def add_features(data):\n",
    "    columns_to_use = [\n",
    "        'article_id', \n",
    "        'product_type_no', \n",
    "        'graphical_appearance_no', \n",
    "        'colour_group_code', \n",
    "        'perceived_colour_value_id',\n",
    "        'perceived_colour_master_id', \n",
    "        'department_no', \n",
    "        'index_code',\n",
    "        'index_group_no', \n",
    "        'section_no', \n",
    "        'garment_group_no', \n",
    "        'FN', \n",
    "        'Active',\n",
    "        'club_member_status', \n",
    "        'fashion_news_frequency', \n",
    "        'age', \n",
    "        'postal_code',\n",
    "        'bestseller_rank'\n",
    "    ]\n",
    "\n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on='customer_id')\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "\n",
    "    # features from assignment 2 could go here\n",
    "    customer_avg_price = transactions.groupby('customer_id')['price'].mean().to_frame('preferred_price')\n",
    "    result = pd.merge(result, customer_avg_price, how=\"left\", on=\"customer_id\")\n",
    "    \n",
    "    return result[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e22bfaf-d1dd-4ea8-9afc-6f4332be82f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T15:01:20.945857700Z",
     "start_time": "2023-12-29T15:00:34.614825680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.151109\n",
      "[LightGBM] [Info] Total Bins 1149\n",
      "[LightGBM] [Info] Number of data points in the train set: 11557594, number of used features: 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "               bestseller_rank 0.99907\n",
      "                    article_id 0.00028\n",
      "                           age 0.00024\n",
      "              garment_group_no 0.00007\n",
      "            club_member_status 0.00007\n",
      "                   postal_code 0.00007\n",
      "               product_type_no 0.00006\n",
      "             colour_group_code 0.00005\n",
      "                 department_no 0.00004\n",
      "                        Active 0.00002\n",
      "       graphical_appearance_no 0.00001\n",
      "     perceived_colour_value_id 0.00001\n",
      "                            FN 0.00000\n",
      "        fashion_news_frequency 0.00000\n",
      "                index_group_no 0.00000\n",
      "                    section_no 0.00000\n",
      "    perceived_colour_master_id 0.00000\n",
      "                    index_code 0.00000\n",
      "        customer_id                                         prediction\n",
      "0    28847241659200  [925246001, 909370001, 918522001, 918292001, 8...\n",
      "1    41318098387474  [868879003, 918522001, 918292001, 909370001, 8...\n",
      "2   116809474287335  [906305002, 909370001, 918522001, 918292001, 8...\n",
      "3   200292573348128  [903861001, 909370001, 918522001, 918292001, 8...\n",
      "4   208119717816961  [572797049, 540334001, 572797002, 859105007, 9...\n",
      "5   248294615847351  [720504008, 337991001, 471714002, 878987003, 9...\n",
      "6   272412481300040  [922381001, 923460002, 921906005, 918522001, 9...\n",
      "7   329094189075899  [821338004, 918522001, 918292001, 909370001, 8...\n",
      "8   330092272649261  [800691016, 863937003, 863937010, 909370001, 9...\n",
      "9   366493139417506  [859737004, 909370001, 918522001, 918292001, 8...\n",
      "10  375055163245029  [859105003, 716672014, 918522001, 918292001, 9...\n",
      "11  519262836338427  [804992016, 852584006, 909370001, 918522001, 9...\n",
      "12  649760207043851  [696587001, 869198003, 878502001, 554450036, 9...\n",
      "13  690285180337957  [915529005, 915529001, 918522001, 918292001, 9...\n",
      "14  736218475114453  [824341001, 854021001, 783978008, 918522001, 9...\n",
      "15  745180086074610  [867948001, 929603002, 841668001, 372860002, 8...\n",
      "16  762483386043116  [826209001, 529589001, 878479001, 909370001, 9...\n",
      "17  805095543045062  [859737002, 909370001, 918522001, 918292001, 8...\n",
      "18  857913002275398  [636207006, 863456003, 909370001, 918522001, 9...\n",
      "19  879819981624203  [562245062, 794468001, 695632001, 690933005, 8...\n",
      "0.025080605661718477\n"
     ]
    }
   ],
   "source": [
    "### split into training and testing\n",
    "# one week is used for testing\n",
    "# a number of weeks leading up to the test week are used to train the ranker\n",
    "test_week = 104\n",
    "num_training_weeks = 10\n",
    "testing_weeks = np.arange(test_week-num_training_weeks, test_week)\n",
    "train_data = transactions[transactions.week.isin(testing_weeks)].reset_index(drop=True)\n",
    "\n",
    "### assemble training data (positive + negative examples)\n",
    "# each example has at least a customer_id, article_id and whether it was purchased or not (positive/negative)\n",
    "# add_features extracts and adds features to the examples\n",
    "train_examples = get_examples(train_data, test_week)\n",
    "X_train = add_features(train_examples)\n",
    "Y_train = train_examples['purchased']\n",
    "\n",
    "### fit ranker\n",
    "# training_groups tells LGBM that each (week, customer_id) combination is a seperate basket\n",
    "# !!! it is important that the training_examples are sorted according to week, customer_id for this to work\n",
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(X_train, Y_train, group=train_groups)\n",
    "print_importance(ranker, X_train.columns)\n",
    "\n",
    "### test\n",
    "# candidates are generated similarly to the examples, only we don't know whether they are purchased\n",
    "# the same features are extracted and added\n",
    "# each candidate is scored by the ranker and predictions are generated using the highest scoring candidates\n",
    "test_candidates = get_candidates(train_data, test_week)\n",
    "X_test = add_features(test_candidates)\n",
    "predictions = get_predictions(test_candidates, X_test, ranker, 12)\n",
    "print(predictions.head(20))\n",
    "\n",
    "### evaluate\n",
    "if test_week < transactions.week.max() + 1:\n",
    "    # get ground truth data for test week\n",
    "    purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    \n",
    "    # fill missing prediction for customers in test set with popular items in last week\n",
    "    # only for customers in test set because only those are evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, purchases.customer_id, popular)\n",
    "    \n",
    "    # calculate score\n",
    "    score = mean_average_precision(predictions, purchases, 12)\n",
    "    print(score)\n",
    "\n",
    "### submit\n",
    "else:\n",
    "    # fill missing predictions for all customers with popular items in last week\n",
    "    # all customers because we don't know which ones will be evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "    # write submission\n",
    "    sub = create_submission(predictions)\n",
    "    sub.to_csv(BASE_PATH + 'sub1.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde7943-4997-457f-8137-1a5f6d4419c9",
   "metadata": {},
   "source": [
    "Scores from using various weeks as the test week:\n",
    "\n",
    "+ 105: 0.02087 (kaggle)\n",
    "+ 104: 0.025080605661718477\n",
    "+ 103: 0.023774082148643252\n",
    "+ 102: 0.022159069556621\n",
    "+ 101: 0.01881722188115503\n",
    "+ 100: 0.019754936922870146\n",
    "\n",
    "I am pretty sure that my implementation of MAP@12 is correct and these deviations are due to noise in the dataset. The submission generated by this code for week 105 has the same score as the submission from the Radek notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start code additions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de25142c743387b2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pickle\n",
    "predictions = pickle.load(open('../data/LightGCN/predictions.pkl', 'rb'))\n",
    "# print(predictions.head(20))\n",
    "\n",
    "# Change the test week to switch between MAP@12 and generating the submission\n",
    "test_week = 105\n",
    "### evaluate\n",
    "if test_week < transactions.week.max() + 1:\n",
    "    # get ground truth data for test week\n",
    "    purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    \n",
    "    # fill missing prediction for customers in test set with popular items in last week\n",
    "    # only for customers in test set because only those are evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, purchases.customer_id, popular)\n",
    "    \n",
    "    # calculate score\n",
    "    score = mean_average_precision(predictions, purchases, 12)\n",
    "    print(score)\n",
    "\n",
    "### submit\n",
    "else:\n",
    "    # fill missing predictions for all customers with popular items in last week\n",
    "    # all customers because we don't know which ones will be evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "    sample_submission = pd.read_csv(\"../data/csv/sample_submission.csv\")\n",
    "    # write submission\n",
    "    sub = create_submission(predictions, sample_submission)\n",
    "    # Change the submission file depending on the model\n",
    "    sub.to_csv(BASE_PATH + 'LightGCN_4layer_15sample.csv.gz', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T15:01:35.708683042Z",
     "start_time": "2023-12-29T15:01:20.945431202Z"
    }
   },
   "id": "8df9ff3c890fae56"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    week  article_id  bestseller_rank\n",
      "0    104   924243001                1\n",
      "1    104   924243002                2\n",
      "2    104   918522001                3\n",
      "3    104   923758001                4\n",
      "4    104   866731001                5\n",
      "5    104   909370001                6\n",
      "6    104   751471001                7\n",
      "7    104   915529003                8\n",
      "8    104   915529005                9\n",
      "9    104   448509014               10\n",
      "10   104   762846027               11\n",
      "11   104   714790020               12\n"
     ]
    }
   ],
   "source": [
    "popular_items = transactions[transactions.week == 104] \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "\n",
    "popular_items = popular_items.reset_index()\n",
    "print(popular_items.head(20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T15:28:12.737591604Z",
     "start_time": "2023-12-29T15:28:12.696020581Z"
    }
   },
   "id": "b587f5142c33484d"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{714790020, 751471001, 915529003, 762846027, 915529005, 923758001, 918522001, 909370001, 448509014, 924243001, 924243002, 866731001}\n"
     ]
    }
   ],
   "source": [
    "popular_items = set(popular_items[\"article_id\"].unique())\n",
    "print(popular_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T15:28:13.182746125Z",
     "start_time": "2023-12-29T15:28:13.170858777Z"
    }
   },
   "id": "11d5267d7e97b83d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13994201 / 16463760 = 85.00003036973328%\n"
     ]
    }
   ],
   "source": [
    "total_predictions = len(predictions) * 12\n",
    "popular_predictions = 0\n",
    "\n",
    "for (i, row) in predictions.iterrows():\n",
    "    for prediction in row[\"prediction\"]:\n",
    "        if prediction in popular_items:\n",
    "            popular_predictions += 1\n",
    "            \n",
    "print(f\"{popular_predictions} / {total_predictions} = {popular_predictions/total_predictions * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T15:28:28.497365717Z",
     "start_time": "2023-12-29T15:28:13.848047269Z"
    }
   },
   "id": "6c6bfb69331c4655"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Important note\n",
    "\n",
    "During my presentation and in my report, I said the predictions aren't really similar to popular items. While cleaning my code and refactoring a bit to make it cleaner I found a bug in the code that was used for calculating the similarity. \n",
    "\n",
    "If we take the top 12 popular items of the test week (the last week of the dataset, week 104) we get that +-85% of the items recommended by LightGCN are indeed popular items. If we take the top 12 popular items of all weeks in the dataset we also get +-85% (slightly higher but not much). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26d31b1c6b4ae8a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8519543423377a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
