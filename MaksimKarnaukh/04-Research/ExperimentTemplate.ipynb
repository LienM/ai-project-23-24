{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd011469-040b-46ae-b0c7-ca57c682a58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T02:37:10.886339900Z",
     "start_time": "2023-11-03T02:37:10.775284100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# make external scripts auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from experiment_template import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba89dec-cb25-4243-b9d7-a857d770e97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T02:37:14.061127900Z",
     "start_time": "2023-11-03T02:37:10.843400700Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '../input/'\n",
    "\n",
    "# make sure the same data preprocessing as in the radek notebook have been performed\n",
    "# (see 02 FE/DataProcessingRadek.ipynb)\n",
    "transactions = pd.read_parquet(BASE_PATH + 'transactions_train.parquet')\n",
    "customers = pd.read_parquet(BASE_PATH + 'customers.parquet')\n",
    "articles = pd.read_parquet(BASE_PATH + 'articles.parquet')\n",
    "sample_submission = pd.read_csv(BASE_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72811db-7994-4aa4-ac87-e90fa7275b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T02:37:14.124925900Z",
     "start_time": "2023-11-03T02:37:14.065637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Candidate generation of Radek notebook\n",
    "def get_data(data, test_week):\n",
    "    ### repurchase\n",
    "    # each week is seen as a basket\n",
    "    # the items bought in one basket, will be example for the next basket\n",
    "    # the items bought in the last basket, will be candidates for the test basket\n",
    "    c2weeks = data.groupby('customer_id')['week'].unique()\n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        c2weeks2shifted_weeks[c_id] = {}\n",
    "        for i in range(weeks.shape[0]-1):\n",
    "            c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i+1]\n",
    "        c2weeks2shifted_weeks[c_id][weeks[-1]] = test_week\n",
    "    candidates_last_purchase = data.copy()\n",
    "    weeks = []\n",
    "    for i, (c_id, week) in enumerate(zip(data['customer_id'], data['week'])):\n",
    "        weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
    "    candidates_last_purchase.week=weeks\n",
    "\n",
    "    ### bestseller\n",
    "    # if a user bought an item in a given week, the 12 most popular items in the previous week are example for that week\n",
    "    # the best selling items in the last week are candidates for all users\n",
    "    mean_price = data \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = data \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    unique_transactions = data \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "\n",
    "    ### combine\n",
    "    d = data.copy()\n",
    "    d['purchased'] = True\n",
    "    \n",
    "    result = pd.concat([\n",
    "        d, candidates_last_purchase, candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(False, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "\n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result\n",
    "    \n",
    "\n",
    "# these functions don't necessarily need to use the same underlying data function, but this is how Radek did it\n",
    "# !!! it is important that the examples are sorted according to (week, customer_id) for LGBM ranker\n",
    "def get_examples(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week != test_week]\n",
    "\n",
    "def get_candidates(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week == test_week]\n",
    "\n",
    "def add_features(data):\n",
    "    columns_to_use = [\n",
    "        'article_id', \n",
    "        'product_type_no', \n",
    "        'graphical_appearance_no', \n",
    "        'colour_group_code', \n",
    "        'perceived_colour_value_id',\n",
    "        'perceived_colour_master_id', \n",
    "        'department_no', \n",
    "        'index_code',\n",
    "        'index_group_no', \n",
    "        'section_no', \n",
    "        'garment_group_no', \n",
    "        'FN', \n",
    "        'Active',\n",
    "        'club_member_status', \n",
    "        'fashion_news_frequency', \n",
    "        'age', \n",
    "        'postal_code',\n",
    "        'bestseller_rank'\n",
    "    ]\n",
    "\n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on='customer_id')\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "\n",
    "    # features from assignment 2 could go here\n",
    "    customer_avg_price = transactions.groupby('customer_id')['price'].mean().to_frame('preferred_price')\n",
    "    result = pd.merge(result, customer_avg_price, how=\"left\", on=\"customer_id\")\n",
    "    \n",
    "    return result[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1477522\n",
      "Number of edges: 27306439\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "user_item_graph = nx.Graph()\n",
    "\n",
    "user_ids = customers['customer_id'].unique()\n",
    "article_ids = articles['article_id'].unique()\n",
    "\n",
    "user_item_graph.add_nodes_from(user_ids, bipartite=0)\n",
    "user_item_graph.add_nodes_from(article_ids, bipartite=1)\n",
    "\n",
    "# # edges representing interactions (e.g., purchases) between customers and articles\n",
    "# for _, transaction in transactions.iterrows():\n",
    "#     user_id = transaction['customer_id']\n",
    "#     article_id = transaction['article_id']\n",
    "#     user_item_graph.add_edge(user_id, article_id)\n",
    "\n",
    "# Create a DataFrame with unique user-article interactions\n",
    "interactions = transactions[['customer_id', 'article_id']].drop_duplicates()\n",
    "# Add edges representing interactions\n",
    "user_item_graph.add_edges_from(interactions.to_records(index=False))\n",
    "\n",
    "print('Number of nodes: %d' % user_item_graph.number_of_nodes())\n",
    "print('Number of edges: %d' % user_item_graph.number_of_edges())\n",
    "# nx.draw(user_item_graph)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T02:38:59.605249800Z",
     "start_time": "2023-11-03T02:37:14.124925900Z"
    }
   },
   "id": "c70ccfd9a1aa70a0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maksim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py:953: RuntimeWarning: invalid value encountered in cast\n",
      "  arr = np.asarray(values, dtype=dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative row index found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m interactions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124marticle_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m interactions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124marticle_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(item_id_to_index)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Create a CSR sparse matrix\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m interaction_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mcsr_matrix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minteractions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43minteractions\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minteractions\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43marticle_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43muser_ids\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43marticle_ids\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Split data into training and validation\u001B[39;00m\n\u001B[0;32m     28\u001B[0m train_data, val_data \u001B[38;5;241m=\u001B[39m train_test_split(interaction_matrix, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_compressed.py:52\u001B[0m, in \u001B[0;36m_cs_matrix.__init__\u001B[1;34m(self, arg1, shape, dtype, copy)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(arg1) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m     50\u001B[0m         \u001B[38;5;66;03m# (data, ij) format\u001B[39;00m\n\u001B[0;32m     51\u001B[0m         other \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(\n\u001B[1;32m---> 52\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_coo_container\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m         )\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_self(other)\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(arg1) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m     56\u001B[0m         \u001B[38;5;66;03m# (data, indices, indptr) format\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_coo.py:204\u001B[0m, in \u001B[0;36m_coo_base.__init__\u001B[1;34m(self, arg1, shape, dtype, copy)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 204\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_coo.py:295\u001B[0m, in \u001B[0;36m_coo_base._check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolumn index exceeds matrix dimensions\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrow\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnegative row index found\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcol\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnegative column index found\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: negative row index found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the interaction graph to a sparse matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_users = len(user_ids)\n",
    "num_items = len(article_ids)\n",
    "\n",
    "# Create a mapping from user and article IDs to non-negative integers\n",
    "user_id_to_index = {customer_id: i for i, customer_id in enumerate(user_ids)}\n",
    "item_id_to_index = {article_id: i for i, article_id in enumerate(article_ids)}\n",
    "\n",
    "# Update the interaction DataFrame with non-negative integer indices\n",
    "interactions['customer_id'] = interactions['customer_id'].map(user_id_to_index)\n",
    "interactions['article_id'] = interactions['article_id'].map(item_id_to_index)\n",
    "\n",
    "# Create a CSR sparse matrix\n",
    "interaction_matrix = csr_matrix(\n",
    "    (np.ones(len(interactions), dtype=np.float32), (interactions['customer_id'], interactions['article_id'])),\n",
    "    shape=(len(user_ids), len(article_ids))\n",
    ")\n",
    "\n",
    "# Split data into training and validation\n",
    "train_data, val_data = train_test_split(interaction_matrix, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGCN Model\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding_user = nn.Embedding(num_users, embed_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "    def forward(self, interaction_matrix):\n",
    "        user_embedding = self.embedding_user.weight\n",
    "        item_embedding = self.embedding_item.weight\n",
    "        embeddings = torch.cat([user_embedding, item_embedding], dim=0)\n",
    "        \n",
    "        user_output = interaction_matrix @ embeddings\n",
    "        item_output = interaction_matrix.t() @ embeddings\n",
    "        \n",
    "        return user_output, item_output\n",
    "\n",
    "# Instantiate the model\n",
    "embed_dim = 64\n",
    "model = LightGCN(num_users, num_items, embed_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    user_output, item_output = model(interaction_matrix)\n",
    "    loss = criterion(user_output, interaction_matrix) + criterion(item_output, interaction_matrix.t())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T02:47:22.541252600Z",
     "start_time": "2023-11-03T02:47:15.409940300Z"
    }
   },
   "id": "aac087dc6b26a33e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to generate personalized candidates for a list of customers using LightGCN\n",
    "def generate_personalized_candidates_lightgcn(model, customer_ids, num_candidates):\n",
    "    personalized_candidates = {}\n",
    "\n",
    "    user_embeddings, _ = model(interaction_matrix)\n",
    "    \n",
    "    for customer_id in customer_ids:\n",
    "        user_idx = np.where(user_ids == customer_id)[0][0]  # Find the index of the user in the user_ids array\n",
    "        user_embedding = user_embeddings[user_idx]\n",
    "        \n",
    "        # Calculate item scores by dot product of user embedding and item embeddings\n",
    "        item_scores = torch.matmul(user_embedding, model.embedding_item.weight.t())\n",
    "        \n",
    "        # Get the indices of the top-k items with the highest scores\n",
    "        top_item_indices = torch.topk(item_scores, num_candidates).indices\n",
    "        \n",
    "        # Map item indices back to article IDs\n",
    "        recommended_items = [article_ids[i] for i in top_item_indices]\n",
    "        personalized_candidates[customer_id] = recommended_items\n",
    "\n",
    "    return personalized_candidates\n",
    "\n",
    "# Usage example\n",
    "customer_ids = user_ids\n",
    "personalized_candidates_lightgcn = generate_personalized_candidates_lightgcn(model, customer_ids, num_candidates=100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T02:47:22.543255300Z",
     "start_time": "2023-11-03T02:47:22.542252400Z"
    }
   },
   "id": "6dba6cf5a57e46b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "personalized_candidates_lightgcn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-03T02:47:22.542252400Z"
    }
   },
   "id": "1a8c209ed565c40b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22bfaf-d1dd-4ea8-9afc-6f4332be82f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T02:47:22.545255400Z",
     "start_time": "2023-11-03T02:47:22.543255300Z"
    }
   },
   "outputs": [],
   "source": [
    "### split into training and testing\n",
    "# one week is used for testing\n",
    "# a number of weeks leading up to the test week are used to train the ranker\n",
    "test_week = 104\n",
    "num_training_weeks = 10\n",
    "testing_weeks = np.arange(test_week-num_training_weeks, test_week)\n",
    "train_data = transactions[transactions.week.isin(testing_weeks)].reset_index(drop=True)\n",
    "\n",
    "### assemble training data (positive + negative examples)\n",
    "# each example has at least a customer_id, article_id and whether it was purchased or not (positive/negative)\n",
    "# add_features extracts and adds features to the examples\n",
    "train_examples = get_examples(train_data, test_week)\n",
    "X_train = add_features(train_examples)\n",
    "Y_train = train_examples['purchased']\n",
    "\n",
    "### fit ranker\n",
    "# training_groups tells LGBM that each (week, customer_id) combination is a seperate basket\n",
    "# !!! it is important that the training_examples are sorted according to week, customer_id for this to work\n",
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(X_train, Y_train, group=train_groups)\n",
    "print_importance(ranker, X_train.columns)\n",
    "\n",
    "### test\n",
    "# candidates are generated similarly to the examples, only we don't know whether they are purchased\n",
    "# the same features are extracted and added\n",
    "# each candidate is scored by the ranker and predictions are generated using the highest scoring candidates\n",
    "test_candidates = get_candidates(train_data, test_week)\n",
    "X_test = add_features(test_candidates)\n",
    "predictions = get_predictions(test_candidates, X_test, ranker, 12)\n",
    "\n",
    "### evaluate\n",
    "if test_week < transactions.week.max() + 1:\n",
    "    # get ground truth data for test week\n",
    "    purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    \n",
    "    # fill missing prediction for customers in test set with popular items in last week\n",
    "    # only for customers in test set because only those are evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, purchases.customer_id, popular)\n",
    "    \n",
    "    # calculate score\n",
    "    score = mean_average_precision(predictions, purchases, 12)\n",
    "    print(score)\n",
    "\n",
    "### submit\n",
    "else:\n",
    "    # fill missing predictions for all customers with popular items in last week\n",
    "    # all customers because we don't know which ones will be evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "    # write submission\n",
    "    sub = create_submission(predictions)\n",
    "    sub.to_csv(BASE_PATH + 'sub1.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde7943-4997-457f-8137-1a5f6d4419c9",
   "metadata": {},
   "source": [
    "Scores from using various weeks as the test week:\n",
    "\n",
    "+ 105: 0.02087 (kaggle)\n",
    "+ 104: 0.025080605661718477\n",
    "+ 103: 0.023774082148643252\n",
    "+ 102: 0.022159069556621\n",
    "+ 101: 0.01881722188115503\n",
    "+ 100: 0.019754936922870146\n",
    "\n",
    "I am pretty sure that my implementation of MAP@12 is correct and these deviations are due to noise in the dataset. The submission generated by this code for week 105 has the same score as the submission from the Radek notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
