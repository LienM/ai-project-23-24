{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fd8e85-e6e1-4033-9fde-48249fb1755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make external scripts auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd011469-040b-46ae-b0c7-ca57c682a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "from template.experiment_template import *\n",
    "from candidate_generation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19760963-011a-4fa5-944c-5b03cd98fb67",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba89dec-cb25-4243-b9d7-a857d770e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '../../data/'\n",
    "DATA_PATH = BASE_PATH + 'sample_0.05/'\n",
    "# DATA_PATH = BASE_PATH + 'parquet/'\n",
    "\n",
    "# make sure the same data preprocessing as in the radek notebook have been performed\n",
    "# (see radek_preprocessing.ipynb)\n",
    "transactions = pd.read_parquet(DATA_PATH + 'transactions_train.parquet')\n",
    "customers = pd.read_parquet(DATA_PATH + 'customers.parquet')\n",
    "articles = pd.read_parquet(DATA_PATH + 'articles.parquet')\n",
    "sample_submission = pd.read_csv(BASE_PATH + 'original/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67caa58-f427-4b1c-9d43-2d117c9c69db",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c2d275-1ae8-411e-a975-05fc7f40f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_week = 104\n",
    "num_training_weeks = 10\n",
    "handle_cold_customers = True\n",
    "num_trees = 100\n",
    "\n",
    "making_submission = test_week == transactions.week.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022cebb-3cf9-482f-afd0-1d8b0c8dcab4",
   "metadata": {},
   "source": [
    "## Split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d627eb5b-9716-4d74-8d9c-c28571169659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one week is used for testing\n",
    "# a number of weeks leading up to the test week are used to train the ranker\n",
    "transactions = add_relative_week(transactions)\n",
    "training_weeks = np.arange(test_week-num_training_weeks, test_week)\n",
    "train_data = transactions[transactions.week.isin(training_weeks)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3920f2-b2b5-4ad6-a883-e8df089b7d06",
   "metadata": {},
   "source": [
    "## Generate training examples and testing candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e2881b-da3d-4897-b14b-b67166c59e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation: only generate testing candidates for customers with ground truth data\n",
    "# not possible for submission week\n",
    "test_customers = None\n",
    "if not making_submission:\n",
    "    p = get_purchases(transactions[transactions.week == test_week])\n",
    "    test_customers = p.customer_id.values\n",
    "\n",
    "# get the examples and candidates\n",
    "# examples are (customer, week, article, purchased) triplets\n",
    "# candidates are (customer, article) pairs\n",
    "train_examples, test_candidates = get_examples_candidates(train_data, test_week, test_customers, customers, articles)\n",
    "\n",
    "# add features and prepare data for ranker\n",
    "X_train = add_features(train_examples, transactions, customers, articles)\n",
    "X_test = add_features(test_candidates, transactions, customers, articles)\n",
    "Y_train = train_examples['purchased']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e70941-7e11-40c6-a3b7-9a66bfba75d2",
   "metadata": {},
   "source": [
    "## Fit ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e22bfaf-d1dd-4ea8-9afc-6f4332be82f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1247\n",
      "[LightGBM] [Info] Number of data points in the train set: 911325, number of used features: 30\n",
      "        c_af_colour_group_name 0.13052\n",
      "             c_af_product_code 0.11839\n",
      "                      c_cf_age 0.11769\n",
      "          c_af_department_name 0.10837\n",
      "                 c_popularity1 0.10038\n",
      "              c_cf_postal_code 0.08797\n",
      "c_af_graphical_appearance_name 0.08551\n",
      "               c_af_index_name 0.08218\n",
      "                       c_cf_FN 0.07274\n",
      "                  c_repurchase 0.05182\n",
      "                 c_popularity2 0.02347\n",
      "                 article_price 0.00391\n",
      "             colour_group_code 0.00376\n",
      "                 department_no 0.00184\n",
      "               preferred_price 0.00181\n",
      "               product_type_no 0.00168\n",
      "                    section_no 0.00159\n",
      "       graphical_appearance_no 0.00127\n",
      "                   postal_code 0.00126\n",
      "                           age 0.00096\n",
      "              garment_group_no 0.00091\n",
      "    perceived_colour_master_id 0.00082\n",
      "     perceived_colour_value_id 0.00060\n",
      "                    index_code 0.00030\n",
      "                        Active 0.00009\n",
      "                index_group_no 0.00006\n",
      "            club_member_status 0.00004\n",
      "                 buys_for_kids 0.00004\n",
      "                            FN 0.00002\n",
      "        fashion_news_frequency 0.00001\n"
     ]
    }
   ],
   "source": [
    "# training_groups tells LGBM that each (week, customer_id) combination is a seperate basket\n",
    "# !!! it is important that the training_examples are sorted according to week, customer_id for this to work\n",
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=num_trees,\n",
    "    importance_type='gain'\n",
    ")\n",
    "# train_groups = train_examples.groupby(['customer_id'])['article_id'].count().values\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(X_train, Y_train, group=train_groups)\n",
    "print_importance(ranker, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46ba2c-1af2-406e-8fa3-78acea4f50b3",
   "metadata": {},
   "source": [
    "## Evaluate / Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0663f361-4855-46b1-bf91-2fc6f9e48c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022398265847538644\n"
     ]
    }
   ],
   "source": [
    "# generate recommendations\n",
    "predictions = get_predictions(test_candidates, X_test, ranker, 12)\n",
    "\n",
    "# fill missing predictions with top-12 most purchased articles in last week\n",
    "popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "# cold users get served special candidates directly (no ranking)\n",
    "if handle_cold_customers:\n",
    "    active_users = train_data.customer_id.unique()\n",
    "    cold_users = list(set(customers.customer_id) - set(active_users))\n",
    "    \n",
    "    bask = baskets(None, test_week, cold_users)\n",
    "    c = (\n",
    "        pd.concat([\n",
    "            candidates_article_feature(bask, train_data, articles, 'prod_name', 6, 1, 2, 6, True),\n",
    "            candidates_popularity(bask, train_data, 12, 1)\n",
    "        ])\n",
    "        .drop(columns='week')\n",
    "        .drop_duplicates(['customer_id', 'article_id'])\n",
    "        .groupby('customer_id')\n",
    "        .head(12)\n",
    "        .groupby('customer_id', as_index=False)\n",
    "        .article_id.apply(list)\n",
    "        .rename(columns={'article_id':'prediction'})\n",
    "    )\n",
    "    predictions = pd.concat([predictions[predictions.customer_id.isin(active_users)], c])\n",
    "\n",
    "if making_submission:\n",
    "    # write submission\n",
    "    sub = create_submission(predictions, sample_submission)\n",
    "    sub.to_csv(BASE_PATH + 'sub19-06b.csv.gz', index=False)\n",
    "else:\n",
    "    # calculate validation score\n",
    "    purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    score = mean_average_precision(predictions, purchases, 12)\n",
    "    print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
