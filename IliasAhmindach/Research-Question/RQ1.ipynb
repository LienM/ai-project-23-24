{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-05T23:54:42.129812600Z",
     "start_time": "2023-12-05T23:54:41.336665500Z"
    }
   },
   "outputs": [],
   "source": [
    "from Question1 import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BASE_PATH = '../Data/'\n",
    "transactions = pd.read_parquet(BASE_PATH + 'transactions_train.parquet')\n",
    "customers = pd.read_parquet(BASE_PATH + 'customers.parquet')\n",
    "articles = pd.read_parquet(BASE_PATH + 'articles.parquet')\n",
    "sample_submission = pd.read_csv(BASE_PATH + 'sample_submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T23:54:44.101787200Z",
     "start_time": "2023-12-05T23:54:42.131102Z"
    }
   },
   "id": "cf343fc889ff26a5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Candidate generation of Radek notebook\n",
    "def get_data(data, test_week):\n",
    "    ### repurchase\n",
    "    # each week is seen as a basket\n",
    "    # the items bought in one basket, will be example for the next basket\n",
    "    # the items bought in the last basket, will be candidates for the test basket\n",
    "    candidates_last_purchase = data.copy()\n",
    "    c2weeks = data.groupby('customer_id')['week'].unique()\n",
    "    \n",
    "    c2weeks2shifted_weeks = {}\n",
    "    for c_id, weeks in c2weeks.items():\n",
    "        shifted_weeks = weeks[1:].tolist() + [test_week]\n",
    "        c2weeks2shifted_weeks[c_id] = dict(zip(weeks, shifted_weeks))\n",
    "\n",
    "    candidates_last_purchase['week'] = [\n",
    "        c2weeks2shifted_weeks[c_id][week]\n",
    "        for c_id, week in zip(data['customer_id'], data['week'])\n",
    "    ]\n",
    "\n",
    "    ### bestseller\n",
    "    # if a user bought an item in a given week, the 12 most popular items in the previous week are example for that week\n",
    "    # the best selling items in the last week are candidates for all users\n",
    "    mean_price = data \\\n",
    "        .groupby(['week', 'article_id'])['price'].mean()\n",
    "    sales = data \\\n",
    "        .groupby('week')['article_id'].value_counts() \\\n",
    "        .groupby('week').rank(method='dense', ascending=False) \\\n",
    "        .groupby('week').head(12).rename('bestseller_rank').astype('int8')\n",
    "    bestsellers_previous_week = pd.merge(sales, mean_price, on=['week', 'article_id']).reset_index()\n",
    "    bestsellers_previous_week.week += 1\n",
    "    unique_transactions = data \\\n",
    "        .groupby(['week', 'customer_id']) \\\n",
    "        .head(1) \\\n",
    "        .drop(columns=['article_id', 'price']) \\\n",
    "        .copy()\n",
    "    candidates_bestsellers = pd.merge(\n",
    "        unique_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week',\n",
    "    )\n",
    "    test_set_transactions = unique_transactions.drop_duplicates('customer_id').reset_index(drop=True)\n",
    "    test_set_transactions.week = test_week\n",
    "    candidates_bestsellers_test_week = pd.merge(\n",
    "        test_set_transactions,\n",
    "        bestsellers_previous_week,\n",
    "        on='week'\n",
    "    )\n",
    "    candidates_bestsellers = pd.concat([candidates_bestsellers, candidates_bestsellers_test_week])\n",
    "    candidates_bestsellers.drop(columns='bestseller_rank', inplace=True)\n",
    "\n",
    "    ### combine\n",
    "    d = data.copy()\n",
    "    d['purchased'] = True\n",
    "    \n",
    "    result = pd.concat([\n",
    "        d, candidates_last_purchase, candidates_bestsellers\n",
    "    ])\n",
    "    result.purchased.fillna(False, inplace=True)\n",
    "    result.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        bestsellers_previous_week[['week', 'article_id', 'bestseller_rank']],\n",
    "        on=['week', 'article_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    result = result[result.week != result.week.min()]\n",
    "    result.bestseller_rank.fillna(999, inplace=True)\n",
    "\n",
    "    result.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    return result\n",
    "\n",
    "def get_examples(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week != test_week]\n",
    "\n",
    "def get_candidates(data, test_week):\n",
    "    data = get_data(data, test_week)\n",
    "    return data[data.week == test_week]\n",
    "\n",
    "def add_features(data):\n",
    "    columns_to_use = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id','perceived_colour_master_id', 'department_no', 'index_code','index_group_no', 'section_no', 'garment_group_no', 'price','score'\n",
    "    ]\n",
    "\n",
    "    result = data\n",
    "    result = pd.merge(result, customers, how='left', on='customer_id')\n",
    "    result = pd.merge(result, articles, how='left', on='article_id')\n",
    "\n",
    "    result['score'] = result.apply(get_score,axis=1).fillna(0)\n",
    "    \n",
    "    return result[columns_to_use]\n",
    "\n",
    "def get_score(entry):\n",
    "    try:\n",
    "        return ui_score.loc[entry['customer_id'], entry['article_id']]\n",
    "    except KeyError:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T23:54:44.109811300Z",
     "start_time": "2023-12-05T23:54:44.102785400Z"
    }
   },
   "id": "67c3982febc4212a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_sim(recmodel,purchase_sparse):\n",
    "    s2 = cosine_similarity(recmodel.articles_latent_matrix, recmodel.articles_latent_matrix)\n",
    "    return purchase_sparse.dot(s2)\n",
    "\n",
    "def apply_filter(scores, filter_matrix):\n",
    "    chunk_size = 10000\n",
    "    num_rows, num_cols = scores.shape\n",
    "    result = np.zeros((num_rows, num_cols))\n",
    "    \n",
    "    for i in range(0, num_rows, chunk_size):\n",
    "        chunk_end = min(i + chunk_size, num_rows)\n",
    "        ui_chunk = scores.iloc[i:chunk_end].values\n",
    "        filter_chunk = filter_matrix[i:chunk_end]\n",
    "        result[i:chunk_end] = np.multiply(ui_chunk, filter_chunk)\n",
    "    \n",
    "    return pd.DataFrame(result, index=scores.index, columns=scores.columns)\n",
    "\n",
    "\n",
    "def get_useritem_data(recmodel):\n",
    "    itemcf_transactions['article_id'] = itemcf_transactions['article_id'].astype(int)\n",
    "    purchase_counts = itemcf_transactions.groupby(['customer_id', 'article_id']).size().rename('count').reset_index().sort_values('article_id')\n",
    "    \n",
    "    user_to_index = {user_id: index for index, user_id in enumerate(purchase_counts['customer_id'].unique())}\n",
    "    article_to_index = {article_id: index for index, article_id in enumerate(purchase_counts['article_id'].unique())}\n",
    "    \n",
    "    row_indices = purchase_counts['customer_id'].map(user_to_index).values\n",
    "    col_indices = purchase_counts['article_id'].map(article_to_index).values\n",
    "    spdata = purchase_counts['count'].values\n",
    "    \n",
    "    purchase_counts_sparse = csr_matrix((spdata, (row_indices, col_indices)), shape=(len(user_to_index), len(article_to_index)), dtype=int)\n",
    "    result = pd.DataFrame(get_sim(recmodel,purchase_counts_sparse), index=user_to_index.keys(), columns=article_to_index.keys())\n",
    "    \n",
    "    purchase_counts_sparse = csr_matrix((np.ones_like(spdata), (row_indices, col_indices)), shape=(len(user_to_index), len(article_to_index)), dtype=int).toarray()\n",
    "    filter_matrix = 1 - purchase_counts_sparse\n",
    "    del purchase_counts\n",
    "    \n",
    "    return apply_filter(result,filter_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T23:54:44.114810300Z",
     "start_time": "2023-12-05T23:54:44.113302Z"
    }
   },
   "id": "3513f7ed26cd5023"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "### split into training and testing\n",
    "# one week is used for testing\n",
    "# a number of weeks leading up to the test week are used to train the ranker\n",
    "test_week = 104\n",
    "num_training_weeks = 10\n",
    "testing_weeks = np.arange(test_week-num_training_weeks, test_week)\n",
    "train_data = transactions[transactions.week.isin(testing_weeks)].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T23:54:44.218001Z",
     "start_time": "2023-12-05T23:54:44.115811200Z"
    }
   },
   "id": "736503860d62eb20"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5490618/5490618 [01:09<00:00, 78701.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4835677681836136\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5490618/5490618 [01:10<00:00, 78055.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27997658968282624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "itemcf_transactions = train_data.copy().drop(['sales_channel_id', 'price', 'week'], axis=1)\n",
    "most_bought_articles = itemcf_transactions['article_id'].value_counts()[lambda x: x > 10].index\n",
    "itemcf_transactions = itemcf_transactions[itemcf_transactions['article_id'].isin(most_bought_articles)]\n",
    "itemcf_transactions['purchased'] = 1\n",
    "\n",
    "negative_samples = pd.DataFrame({\n",
    "    'article_id': np.random.choice(itemcf_transactions.article_id.unique(), itemcf_transactions.shape[0]),\n",
    "    'customer_id': np.random.choice(itemcf_transactions.customer_id.unique(), itemcf_transactions.shape[0]),\n",
    "    'purchased': np.zeros(itemcf_transactions.shape[0])\n",
    "})\n",
    "\n",
    "rec = ItemCF(itemcf_transactions, negative_samples, num_components=1000)\n",
    "rec.fit(n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T23:57:11.539558900Z",
     "start_time": "2023-12-05T23:54:44.219502500Z"
    }
   },
   "id": "263b3eb4529214dd"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 16s\n",
      "Wall time: 11min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ui_score = get_useritem_data(rec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:08:41.315427800Z",
     "start_time": "2023-12-05T23:57:11.538554400Z"
    }
   },
   "id": "58bf63bc3642b37b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.112795\n",
      "[LightGBM] [Info] Total Bins 1297\n",
      "[LightGBM] [Info] Number of data points in the train set: 11557594, number of used features: 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "                         score 0.99563\n",
      "                         price 0.00319\n",
      "                    article_id 0.00049\n",
      "              garment_group_no 0.00028\n",
      "                 department_no 0.00016\n",
      "               product_type_no 0.00015\n",
      "                    section_no 0.00006\n",
      "             colour_group_code 0.00004\n",
      "                index_group_no 0.00000\n",
      "                    index_code 0.00000\n",
      "    perceived_colour_master_id 0.00000\n",
      "     perceived_colour_value_id 0.00000\n",
      "       graphical_appearance_no 0.00000\n"
     ]
    }
   ],
   "source": [
    "### assemble training data (positive + negative examples)\n",
    "# each example has at least a customer_id, article_id and whether it was purchased or not (positive/negative)\n",
    "# add_features extracts and adds features to the examples\n",
    "train_examples = get_examples(train_data, test_week)\n",
    "X_train = add_features(train_examples)\n",
    "Y_train = train_examples['purchased']\n",
    "\n",
    "### fit ranker\n",
    "# training_groups tells LGBM that each (week, customer_id) combination is a seperate basket\n",
    "# !!! it is important that the training_examples are sorted according to week, customer_id for this to work\n",
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10\n",
    ")\n",
    "train_groups = train_examples.groupby(['week', 'customer_id'])['article_id'].count().values\n",
    "ranker.fit(X_train, Y_train, group=train_groups)\n",
    "print_importance(ranker, X_train.columns)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:15:39.907836500Z",
     "start_time": "2023-12-06T00:08:41.313429700Z"
    }
   },
   "id": "d7cef38fcd72844c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025965239693955543\n",
      "CPU times: total: 48.1 s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### test\n",
    "# candidates are generated similarly to the examples, only we don't know whether they are purchased\n",
    "# the same features are extracted and added\n",
    "# each candidate is scored by the ranker and predictions are generated using the highest scoring candidates\n",
    "test_candidates = get_candidates(train_data, test_week)\n",
    "X_test = add_features(test_candidates)\n",
    "predictions = get_predictions(test_candidates, X_test, ranker, 12)\n",
    "\n",
    "### evaluate\n",
    "if test_week < transactions.week.max() + 1:\n",
    "    # get ground truth data for test week\n",
    "    purchases = get_purchases(transactions[transactions.week == test_week])\n",
    "    \n",
    "    # fill missing prediction for customers in test set with popular items in last week\n",
    "    # only for customers in test set because only those are evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, purchases.customer_id, popular)\n",
    "    \n",
    "    # calculate score\n",
    "    score = mean_average_precision(predictions, purchases, 12)\n",
    "    print(score)\n",
    "\n",
    "### submit\n",
    "else:\n",
    "    # fill missing predictions for all customers with popular items in last week\n",
    "    # all customers because we don't know which ones will be evaluated\n",
    "    popular = transactions[transactions.week == test_week-1].article_id.value_counts().head(12).index.values\n",
    "    predictions = fill_missing_predictions(predictions, customers.customer_id, popular)\n",
    "\n",
    "    # write submission\n",
    "    sub = create_submission(predictions,sample_submission)\n",
    "    sub.to_csv('output/' + 'sub1.csv.gz', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:19:16.281290200Z",
     "start_time": "2023-12-06T00:15:39.907836500Z"
    }
   },
   "id": "adb56daea9ce7515"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 94  95  96  97  98  99 100 101 102 103]\n"
     ]
    }
   ],
   "source": [
    "print(testing_weeks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:19:16.288687700Z",
     "start_time": "2023-12-06T00:19:16.283179100Z"
    }
   },
   "id": "f7a132a3729c6fde"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def recall(predictions, purchases, k=12):\n",
    "    def calculate_recall(row):\n",
    "        intersect_count = len(set(row['prediction'][:k]).intersection(row['purchases']))\n",
    "        return intersect_count / min(len(row['purchases']), k) if len(row['purchases']) > 0 else 0\n",
    "\n",
    "    result = pd.merge(purchases, predictions, on=\"customer_id\", how=\"inner\")\n",
    "    result['recall'] = result.apply(calculate_recall, axis=1)\n",
    "\n",
    "    return result['recall'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:24:41.672646600Z",
     "start_time": "2023-12-06T00:24:41.660861800Z"
    }
   },
   "id": "6122f7445126bda8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.05234812642001266"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(predictions,purchases,12)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:24:43.597697100Z",
     "start_time": "2023-12-06T00:24:43.090920800Z"
    }
   },
   "id": "5e3d347040028189"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T00:19:16.785605Z",
     "start_time": "2023-12-06T00:19:16.781028400Z"
    }
   },
   "id": "785eb638c29d1b17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
