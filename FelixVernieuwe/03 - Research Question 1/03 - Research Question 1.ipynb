{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignments by Felix Vernieuwe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df4e549c1bebb94d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Assignment 3: Research Question 1\n",
    "#### **QUESTION:** *Quantify the importance of global, contextual and personalised candidates in evaluation and output score*\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e51aea5d4059b432"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from FelixVernieuwe.util import *\n",
    "from scorers import mean_average_precision\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "setup_seaborn()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:27.265898200Z",
     "start_time": "2023-11-07T23:37:25.985459100Z"
    }
   },
   "id": "45b18b37211c1233"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "DATA_PATH = \"../../data/\"\n",
    "OUTPUT_PATH = \"../../data/submissions/\"\n",
    "\n",
    "# Get OS profile root to get path to /.kaggle/kaggle.json\n",
    "PROFILE_ROOT = os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\", \"\"))\n",
    "KAGGLE_PATH = pathlib.Path(PROFILE_ROOT, \".kaggle/kaggle.json\").as_posix()\n",
    "initialise_kaggle(KAGGLE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:27.398490500Z",
     "start_time": "2023-11-07T23:37:27.259900800Z"
    }
   },
   "id": "1b4936d81241885f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Output description\n",
    "SUBMISSION_NAME = \"engineered_7_submission\"\n",
    "DESCRIPTION = \"Retesting baseline with rewrite\"\n",
    "UPLOAD_TO_KAGGLE = True\n",
    "INCLUDE_MISSING_CUSTOMERS = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:27.524025100Z",
     "start_time": "2023-11-07T23:37:27.398490500Z"
    }
   },
   "id": "9c56a3340cac0e0c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load in the datasets\n",
    "articles = pd.read_parquet(DATA_PATH + \"articles.parquet\")\n",
    "transactions = pd.read_parquet(DATA_PATH + \"transactions_train.parquet\")\n",
    "customers = pd.read_parquet(DATA_PATH + \"customers.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:28.644829500Z",
     "start_time": "2023-11-07T23:37:27.525026600Z"
    }
   },
   "id": "f83fe8a4f3990dc1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Features to train on\n",
    "base_features = [\n",
    "    'article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id',\n",
    "    'perceived_colour_master_id', 'department_no', 'index_code',\n",
    "    'index_group_no', 'section_no', 'garment_group_no', 'FN', 'Active',\n",
    "    'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', \n",
    "    \n",
    "]\n",
    "\n",
    "added_features = [\n",
    "    'has_promotion', 'bestseller_rank', 'all_time_rank', 'price_sensitivity'\n",
    "]\n",
    "\n",
    "all_features = base_features + added_features\n",
    "\n",
    "def join_all_features(transactions, articles, customers):\n",
    "    # Join all features together\n",
    "    transactions = transactions.merge(articles, on=\"article_id\", how=\"left\")\n",
    "    transactions = transactions.merge(customers, on=\"customer_id\", how=\"left\")\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "\n",
    "def filter_feature_data(data):\n",
    "    # Train only on available features in the data, and specified features above \n",
    "    available_features = [feature for feature in all_features if feature in data.columns]\n",
    "    \n",
    "    return data[available_features], available_features\n",
    "\n",
    "\n",
    "def clean_up_transaction_data(data):\n",
    "    # Clean up incoming data\n",
    "    data.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)\n",
    "    data.sort_values(['week', 'customer_id'], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if \"bought\" not in data:\n",
    "        data[\"bought\"] = 1\n",
    "        \n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:28.784564400Z",
     "start_time": "2023-11-07T23:37:28.647829500Z"
    }
   },
   "id": "863f556c93f6fd66"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Reference week (i.e. the last week of our training data, any week after this can be considered part of the test set)\n",
    "# In the current training set, the last week is 104, if we take this as reference, we will try to make predictions for week 105\n",
    "\n",
    "# REFERENCE_WEEK = transactions[\"week\"].max()\n",
    "REFERENCE_WEEK = transactions[\"week\"].max() - 1\n",
    "\n",
    "# How many weeks to train on\n",
    "TRAINING_INTERVAL = 10\n",
    "\n",
    "#Output additional graphs and information\n",
    "VERBOSE = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:28.909550Z",
     "start_time": "2023-11-07T23:37:28.784564400Z"
    }
   },
   "id": "ae1177052203ea9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Engineering\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7552cca43892cd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from features import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:29.037928400Z",
     "start_time": "2023-11-07T23:37:28.910550400Z"
    }
   },
   "id": "134a8eb66466499b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Filter out all transactions that are outside of the training interval\n",
    "transactions = transactions[transactions[\"week\"] >= REFERENCE_WEEK - TRAINING_INTERVAL]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:29.323444800Z",
     "start_time": "2023-11-07T23:37:29.037928400Z"
    }
   },
   "id": "d87c3ccb606786e1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "# DISCLAIMER: this concept was copied from Noah's ExperimentTemplate notebook (test_data is used for offline evaluation)\n",
    "train_data, test_data = split_dataframe(transactions, transactions[\"week\"] <= REFERENCE_WEEK)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:29.543461600Z",
     "start_time": "2023-11-07T23:37:29.324440100Z"
    }
   },
   "id": "52bf4931de9a69dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Global Candidates (apply to every person)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8b55d07aaa24a64"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Determine the bestselling candidates\n",
    "bestseller_candidates = bestseller_feature(train_data, REFERENCE_WEEK)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:38:44.735404200Z",
     "start_time": "2023-11-07T23:37:29.544462400Z"
    }
   },
   "id": "8d9930568904415c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine candidates that are on promotion (promotions per week)\n",
    "promoted_candidates = discount_feature(train_data, REFERENCE_WEEK)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9122d02f93a47e16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine candidates based on new arrivals in the catalogue\n",
    "new_arrival_candidates = new_arrival_feature(transactions, train_data, REFERENCE_WEEK)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46de7e520a101205"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Contextual Candidates (apply to groups of people)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efa9a3583cc9d9ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Personalised Candidates (apply to individuals)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e624bde9c1062c35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Model Training\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fce01ac06aa7334d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Data clean-up\n",
    "train_data = clean_up_transaction_data(all_data)\n",
    "\n",
    "# Split training data into two sets\n",
    "train_data, test_candidates = split_dataframe(train_data, train_data[\"week\"] < REFERENCE_WEEK)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:38:51.758960100Z",
     "start_time": "2023-11-07T23:38:44.694879100Z"
    }
   },
   "id": "21c9072a56bf255"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Join all features together\n",
    "train_data = join_all_features(train_data, articles, customers)\n",
    "\n",
    "# Gives amount of items purchased per customer per week (used for training the ranking)\n",
    "train_bins = train_data.groupby([\"week\", \"customer_id\"])[\"article_id\"].count().values\n",
    "\n",
    "# Train only on specified features\n",
    "train_X, available_features = filter_feature_data(train_data)\n",
    "train_y = train_data[\"bought\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:38:57.915767700Z",
     "start_time": "2023-11-07T23:38:51.759956100Z"
    }
   },
   "id": "fdde95a01fd543fe"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "ranker = LGBMRanker(\n",
    "    force_row_wise=True,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=1,\n",
    "    importance_type='gain',\n",
    "    verbose=10 if VERBOSE else 0,\n",
    ")\n",
    "\n",
    "ranker = ranker.fit(train_X, train_y, group=train_bins)\n",
    "\n",
    "# Get the relative importance of the feature according to the ranker\n",
    "feature_importance = feature_importance_df(ranker, available_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:00.492380600Z",
     "start_time": "2023-11-07T23:38:57.916766400Z"
    }
   },
   "id": "e6826505dc802c94"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Graph importances on bar chart\n",
    "if VERBOSE:\n",
    "    graph_feature_importance(feature_importance)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:00.648638900Z",
     "start_time": "2023-11-07T23:39:00.493380400Z"
    }
   },
   "id": "16c656a79d660e85"
  },
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a27279504ea99c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Prediction generation\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8fc354c95884c4e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "test_candidates = test_candidates.drop_duplicates([\"customer_id\", \"article_id\", \"sales_channel_id\"]).copy()\n",
    "test_candidates = join_all_features(test_candidates, articles, customers)\n",
    "test_X, available_features = filter_feature_data(test_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:05.660838600Z",
     "start_time": "2023-11-07T23:39:00.649641Z"
    }
   },
   "id": "5095b6fc7dc283e1"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def calculate_predictions(candidates, features, ranker, k=12):\n",
    "    \"\"\"\n",
    "    Retrieves dataframe for top 12 candidate articles per customer\n",
    "    NOTE: Partially adapted from Noah's ExperimentTemplate notebook\n",
    "    \n",
    "    :param candidates: Filtered dataframe combining only selected features\n",
    "    :param features: Filtered dataframe combining the features of transactions, articles and customers\n",
    "    :param ranker: Trained ranker model\n",
    "    :param k: Number of articles to predict per customer\n",
    "    :return: Dataframe containing the top k articles per customer\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict on test set\n",
    "    features = features.copy()\n",
    "    features[\"score\"] = ranker.predict(candidates)\n",
    "    features.sort_values([\"customer_id\", \"score\"], ascending=False, inplace=True)\n",
    "    features = features.groupby(\"customer_id\").head(k).reset_index(drop=True)\n",
    "    \n",
    "    return features[[\"article_id\", \"customer_id\", \"score\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:05.785930900Z",
     "start_time": "2023-11-07T23:39:05.661834700Z"
    }
   },
   "id": "6031a242446cf710"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "predicted_candidates = calculate_predictions(test_X, test_candidates, ranker)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:09.286951800Z",
     "start_time": "2023-11-07T23:39:05.786929300Z"
    }
   },
   "id": "41b515cbf29db3f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Model Evaluation / Submission\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a445360f6e3d25cd"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Products that also may prove to be generally interesting (excluding bestsellers)\n",
    "# interesting_products = []\n",
    "# if promoted_articles is not None:\n",
    "#     current_promos = promoted_articles[promoted_articles[\"week\"] == REFERENCE_WEEK - 1]\n",
    "#     bestsellers_df = pd.DataFrame({\"article_id\": bestsellers})\n",
    "#     interesting_products = bestsellers_df.merge(current_promos, on=\"article_id\", how=\"inner\")\n",
    "#     interesting_products = interesting_products[interesting_products[\"has_promotion\"] == True][\"article_id\"].tolist()\n",
    "\n",
    "# Bestselling products in the previous week\n",
    "bestsellers = transactions[transactions[\"week\"] == REFERENCE_WEEK - 1][\"article_id\"].value_counts().head(12).index.tolist()\n",
    "# interesting_products = set(interesting_products)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:09.426998500Z",
     "start_time": "2023-11-07T23:39:09.287953Z"
    }
   },
   "id": "53718059c6cd0ecc"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "predicted_candidates = predicted_candidates[[\"article_id\", \"customer_id\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:09.598522100Z",
     "start_time": "2023-11-07T23:39:09.426998500Z"
    }
   },
   "id": "a50e78e40d9573d0"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "if INCLUDE_MISSING_CUSTOMERS:\n",
    "    # Add extra articles to customers that do not have any predictions\n",
    "    missing_customers = pd.DataFrame({\"customer_id\": customers[\"customer_id\"].unique()})\n",
    "    missing_customers = missing_customers[~missing_customers[\"customer_id\"].isin(test_candidates[\"customer_id\"].unique())]\n",
    "    missing_customers = missing_customers.merge(pd.DataFrame({\"article_id\": bestsellers}), how=\"cross\")\n",
    "    \n",
    "    # Join the two dataframes together (predicted candidates and missing customers)\n",
    "    predicted_candidates = pd.concat([predicted_candidates, missing_customers], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:39:09.724679300Z",
     "start_time": "2023-11-07T23:39:09.598522100Z"
    }
   },
   "id": "63a9abe4eaa6b7c4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# DISCLAIMER: Evaluate test set/leaderboard concept copied from Noah's ExperimentTemplate notebook\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Evaluate on the test set (known data)\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtest_data\u001B[49m\u001B[38;5;241m.\u001B[39mempty \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# Filter test_data to only the reference week\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     ground_truth_purchases \u001B[38;5;241m=\u001B[39m test_data[test_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweek\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_WEEK]\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Get dataframe of customers and their bought articles, grouped by customer and as set, for convenient comparison operations\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# DISCLAIMER: Evaluate test set/leaderboard concept copied from Noah's ExperimentTemplate notebook\n",
    "\n",
    "# Evaluate on the test set (known data)\n",
    "if test_data.empty is False:\n",
    "    # Filter test_data to only the reference week\n",
    "    ground_truth_purchases = test_data[test_data[\"week\"] == REFERENCE_WEEK]\n",
    "    \n",
    "    # Get dataframe of customers and their bought articles, grouped by customer and as set, for convenient comparison operations\n",
    "    ground_truth_purchases = ground_truth_purchases.groupby(\"customer_id\")[\"article_id\"].apply(set).reset_index()\n",
    "    predicted_purchases = predicted_candidates.groupby(\"customer_id\")[\"article_id\"].apply(list).reset_index()\n",
    "\n",
    "    # Rename column to match the predicted candidates\n",
    "    ground_truth_purchases.rename(columns={\"article_id\": \"purchases\"}, inplace=True)\n",
    "    predicted_purchases.rename(columns={\"article_id\": \"predictions\"}, inplace=True)\n",
    "    \n",
    "    truth_prediction = mean_average_precision(predicted_purchases, ground_truth_purchases)\n",
    "    truth_prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T23:37:19.713682700Z",
     "start_time": "2023-11-07T23:37:19.573112800Z"
    }
   },
   "id": "f18d963e43199c74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate on leaderboard set (unknown data)\n",
    "if test_data.empty is False:  \n",
    "    # Read in sample submission (customer_id, prediction)\n",
    "    submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n",
    "    \n",
    "    # Convert to a dictionary and replaces the sample submission predictions with our own\n",
    "    predictions = generate_submission_df(submission, predicted_candidates)\n",
    "       \n",
    "    # Write submission to zipped csv\n",
    "    predictions.to_csv(OUTPUT_PATH + f\"{SUBMISSION_NAME}.csv.gz\", index=False)\n",
    "    \n",
    "    # Upload to kaggle\n",
    "    if UPLOAD_TO_KAGGLE:\n",
    "        result = upload_to_kaggle(OUTPUT_PATH + f\"{SUBMISSION_NAME}.csv.gz\", DESCRIPTION)\n",
    "        # Pretty print result\n",
    "        print(json.dumps(result, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65f6556427b5faa9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b55ce74f95df8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
